{"config":{"lang":["en","zh"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"],"fields":{"title":{"boost":1000.0},"text":{"boost":1.0},"tags":{"boost":1000000.0}}},"docs":[{"location":"blog/2023/02/20/a-role-for-all-your-ec2-instances.html","title":"A role for all your EC2 instances","text":"<p>tl;dr: You can now pass an IAM role to every EC2 instance in your account + region.</p> <p>On Feb 17<sup>th</sup> 2023, AWS Systems Manager released Default Host Management Configuration. This is a way to use Systems Manager on all your EC2 instances: they don't need  <code>ssm:*</code> permissions in their instance profiles, nor do they even need an instance profile at all.</p> <p>So how does it work?</p> <ol> <li>You create a role <code>MyCoolRole</code> assumable by <code>ssm.amazonaws.com</code></li> <li>You configure DHMC by calling the following:    <pre><code>aws ssm update-service-setting \\\n  --setting-id arn:aws:ssm:${AWS_REGION}:${AWS_ACCOUNT_ID}:servicesetting/ssm/managed-instance/default-ec2-instance-management-role \\\n  --setting-value MyCoolRole\n</code></pre></li> <li>The SSM agent on your EC2 instances can now retrieve credentials for <code>MyCoolRole</code>.</li> </ol> <p>What's interesting to note is that this role can have any permissions, it's not scoped down just to SSM agent-relevant permissions. I created a proof-of-concept <code>credential_process</code> credential provider called awsaccountcreds.</p> <p>To try it out, create an EC2 instance without an associated instance profile, install <code>awsaccountcreds</code> and write the following to <code>~/.aws/config</code>:</p> <pre><code>[default]\ncredential_process = /home/ec2-user/awsaccountcreds # or wherever you placed the executable\n</code></pre> <p>Now you can run something in the AWS CLI, like <code>aws sts get-caller-identity</code> or  (if you granted the DHMC role S3 access) <code>aws s3 ls</code>.</p>"},{"location":"blog/2023/02/20/a-role-for-all-your-ec2-instances.html#how-it-works-sequence-diagram-style","title":"How it works, sequence diagram-style","text":"<p>See the following sequence diagram. The CLI retrieves (from the instance metadata service) \"instance identity\" SigV4 credentials. Prior to DHMC, these were only used for EC2 Instance Connect (as far as I know). Calling <code>sts:GetCallerIdentity</code> with these credentials reveal they have an interesting ARN format: <code>arn:aws:sts::607481581596:assumed-role/aws:ec2-instance/i-0ab9ac31d8ff41296</code>.</p> <p>Next an RSA keypair is generated. The key pair's public key is sent to SSM using the undocumented <code>ssm:RegisterManagedInstance</code> API. This API call is signed using the above credentials (<code>C_0</code> in the diagram).</p> <p>Finally, <code>ssm:RequestManagedInstanceRoleToken</code> (also undocumented) is invoked. This is also signed using <code>C_0</code> credentials and has an additional <code>SSM-AsymmetricKeyAuthorization</code> request header. This request header is an RSA signature over the <code>Authorization</code> SigV4 header. This API returns credentials (<code>C_1</code> in the diagram) for a role  session for the DHMC role with the instance ID as the role session name.</p> <p>It seems only one RSA keypair can be registered for a given instance ID. This  keypair can then be used to retrieve credentials multiple times. I haven't yet  looked into how the RSA keypair gets refreshed, but it seems to be a thing (the API has a boolean <code>UpdateKeyPair</code> response field)</p> <p></p>"},{"location":"blog/2023/02/20/a-role-for-all-your-ec2-instances.html#risks","title":"Risks","text":"<p>I reported the potential for passing an over-privileged role to the AWS security team, and they described it as working as designed. Which is correct, and this probably falls on the customer side of the shared responsibility model. (Though I'm not sure why Systems Manager doesn't pass <code>PolicyArns=[arn:aws:iam::aws:policy/AmazonSSMManagedEC2InstanceDefaultPolicy]</code>  when calling <code>sts:AssumeRole</code>) But the risk remains: anyone with <code>ssm:UpdateServiceSetting</code>  and <code>iam:PassRole</code> can affect every EC2 instance in a single API call. And in  my experience, these permissions are typically granted to developers.</p> <p>The other risk is that even though this is described as a solution for managing instances that don't already have SSM privileges, it affects those instances too. Because the SSM agent tries the instance profile first (and only falls back to DHMC if the instance profile fails), it means that those instances remain \"unregistered\" and <code>ssm:RegisterManagedInstance</code> will succeed for a process running on the machine.</p>"},{"location":"blog/2023/02/20/a-role-for-all-your-ec2-instances.html#thanks","title":"Thanks","text":"<p>Thanks to Ian Mckay, Nick Frichette and Christophe Tafani-Dereeper for sanity-checking this. Also big thanks to Ben Bridts, who pointed out that Systems Manager couldn't just pass the <code>AmazonSSMManagedEC2InstanceDefaultPolicy</code> managed policy as a session policy, because that would preclude the S3 and KMS privileges needed for other parts of Systems Manager functionality. Also thanks to the Cloud Security Forum folks whose discussion of this functionality prompted me to do the research.</p>"},{"location":"blog/2023/12/19/an-aws-iam-identity-center-vulnerability.html","title":"An AWS IAM Identity Center vulnerability","text":"<p>The short version: AWS IAM Identity Center exchanges third-party OIDC tokens for Identity Center-issued tokens. Identity Center relies on the <code>jti</code> claim in the  third-party tokens to prevent replay attacks. Identity Center maintained a cache of  previously-seen <code>jti</code> values for a fixed period (24 hours) and didn't enforce that  the third-party tokens had expiry claims. This meant that a token with a <code>jti</code>  claim and without an <code>exp</code> claim could be replayed after &gt;24 hours had passed.  AWS now enforces that these third-party OIDC tokens include an <code>exp</code> claim.</p> <p>I feel this was a relatively minor issue, because a) few legitimate IdPs issue  tokens that never expire and b) the affected <code>CreateTokenWithIAM</code> API also  required AWS IAM authorization (unlike the public <code>CreateToken</code> API). </p>"},{"location":"blog/2023/12/19/an-aws-iam-identity-center-vulnerability.html#story","title":"Story","text":"<p>Around AWS re:Invent 2023, Amazon launched a handful of features relating to  making it easier for customers to access their cloud data in a scalable and  auditable way. These features span IAM Identity Center (formerly AWS SSO), S3,  Athena, Glue, Lake Formation and other services. </p> <p>The APIs, SDKs and documentation were released over a period of a couple of weeks. I dived in straight away, because I'm interested in any changes to authentication, authorisation or audit logs in AWS. Documentation was a little sparse in the  beginning, so I had to learn how to use these APIs through trial and error.  Interestingly,  had I waited a few days the documentation would have been more  complete so I  wouldn't have needed to experiment and likely wouldn't have found  this issue.</p> <p>Specifically, IAM Identity Center's trusted identity propagation docs at the time didn't yet specify that the <code>jti</code> claim in the OIDC token was required. I had created a hand-written OIDC IdP to trial the functionality - that felt  easier than trying to sign up and learn how to use Okta/Microsoft Entra/other IdPs. Through a process of trial and error<sup>1</sup> I learned that my tokens needed a <code>jti</code>  claim and it had to be unique each time. I also learned that they didn't need  <code>iat</code> (issued at time), <code>nbf</code> (not-before time) and <code>exp</code> (expiry time) claims. </p> <p>I got distracted by a different issue<sup>2</sup> (wherein the AWS docs provided an incorrect ARN) and chased down a suspicion that there was a security issue  with that. That theory went nowhere, so it took a few days (and chatting with  peers) to realise that the combination of relying on unique <code>jti</code> claims + no  expiry claims could only work if IAM Identity Center maintained a cache of  every <code>jti</code> it had ever seen, forever. So I created a token, exchanged it and  saved it to disk. Two days later I tried it again and it worked again. The docs  said there was replay prevention, so this should have failed: bug confirmed.</p> <p>If you like sequence diagrams (who doesn't?), here's one that represents the flow. The problem was that the OIDC JWT token named <code>j1</code> could be replayed after 24  hours:</p> <p></p>"},{"location":"blog/2023/12/19/an-aws-iam-identity-center-vulnerability.html#remediation","title":"Remediation","text":"<p>I emailed the AWS Security team with this finding on December 1<sup>st</sup>, while re:Invent was still on. They got back to me less than 12 hours later with an initial (human) confirmation of my email. I got a follow-up email on December 9<sup>th</sup> to confirm that they were still actively investigating the issue. On December 15<sup>th</sup>, I got an update to say that fixes for this issue had been deployed to all regions. That's a very quick turnaround for a small issue reported during re:Invent.</p>"},{"location":"blog/2023/12/19/an-aws-iam-identity-center-vulnerability.html#thanks","title":"Thanks","text":"<p>Thanks to Ian Mckay, Nathan Glover and Brandon Sherman for sanity-checking this blog post. It was even harder to read before their helpful advice.</p>"},{"location":"blog/2023/12/19/an-aws-iam-identity-center-vulnerability.html#footnotes","title":"Footnotes","text":"<ol> <li> <p>My tokens kept getting rejected, so I was randomly trying all kinds of  permutations: tokens with <code>aud</code> claims and without <code>sub</code> claims, tokens with  both <code>aud</code> and  <code>sub</code> claims, tokens with a <code>sub</code> claim and no <code>exp</code> claim, etc.  It was slow work. I eventually tried adding <code>jti</code> and <code>nbf</code> claims - and it worked!  I then tried without an <code>nbf</code> claim - it failed. It took a while before I realised  that failed because I had re-used <code>jti</code>, not because of the missing timestamp claims.\u00a0\u21a9</p> </li> <li> <p>I also solved this issue through trial and error by trying all plausible ARNs. I eventually stumbled on one that worked and tweeted about it. On November 29<sup>th</sup> I received an email from the AWS Security Outreach team to say that they'd seen my tweet, fixed all the docs and that the correct ARN was in fact something different. How many giant corporations can ever react that quickly to a  documentation issue, let alone during their annual conference? Super impressive.\u00a0\u21a9</p> </li> </ol>"},{"location":"blog/2021/09/06/api-gateway-http-apis-and-sqs-messageattributes.html","title":"API Gateway HTTP APIs and SQS MessageAttributes","text":"<p>A while ago I asked on Twitter if anyone knew how to knew how to set message attributes when using AWS API Gateway (HTTP API flavour)'s integration for <code>SQS-SendMessage</code>. I didn't get very far.</p> <p>But today that changed! Here is how you do it. Hopefully this helps someone else.</p> <pre><code>Resources:\n  ApiGateway:\n    Type: AWS::ApiGatewayV2::Api\n    Properties:\n      ProtocolType: HTTP\n      Name: sqsdemo\n\n  Stage:\n    Type: AWS::ApiGatewayV2::Stage\n    Properties:\n      ApiId: !Ref ApiGateway\n      StageName: $default\n      AutoDeploy: true\n\n  Queue:\n    Type: AWS::SQS::Queue\n\n  Role:\n    Type: AWS::IAM::Role\n    Properties:\n      AssumeRolePolicyDocument:\n        Version: 2012-10-17\n        Statement:\n          - Effect: Allow\n            Action: sts:AssumeRole\n            Principal:\n              Service: apigateway.amazonaws.com\n      Policies:\n        - PolicyName: sqssend\n          PolicyDocument:\n            Version: 2012-10-17\n            Statement:\n              - Effect: Allow\n                Action: sqs:SendMessage\n                Resource: !GetAtt Queue.Arn\n\n  Integration:\n    Type: AWS::ApiGatewayV2::Integration\n    Properties:\n      ApiId: !Ref ApiGateway\n      CredentialsArn: !GetAtt Role.Arn\n      PayloadFormatVersion: \"1.0\"\n      IntegrationType: AWS_PROXY\n      IntegrationSubtype: SQS-SendMessage\n      RequestParameters:\n        QueueUrl: !Ref Queue\n        MessageBody: $request.body\n        MessageAttributes: &gt;-\n          {\n            \"UserAgent\": {\n              \"DataType\": \"String\",\n              \"StringValue\": \"${request.header.user-agent}\"\n            }\n          }\n\n  Route:\n    Type: AWS::ApiGatewayV2::Route\n    Properties:\n      ApiId: !Ref ApiGateway\n      RouteKey: $default\n      Target: !Sub integrations/${Integration}\n</code></pre>"},{"location":"blog/2020/09/26/aws-access-key-format.html","title":"AWS Access Key ID formats","text":""},{"location":"blog/2020/09/26/aws-access-key-format.html#experimentation","title":"Experimentation","text":"<p>I was thinking about AWS access key IDs yesterday. Specifically, the one that's often in the <code>AWS_ACCESS_KEY_ID</code> environment variable, or <code>aws_access_key_id</code> in <code>~/.aws/credentials</code>. I was trawling through CloudTrail and the repetitive nature of them caught my eye.</p> <p>Here's an example key we'll refer to for this example: <code>ASIAY34FZKBOKMUTVV7A</code>. Firstly, the format of the first four characters is actually documented and reasonably well-known: <code>AKIA</code> is for long-lived access keys (i.e. those that are assigned to IAM users) and <code>ASIA</code> is for temporary access keys (i.e. those returned by <code>sts:AssumeRole</code> and so on.)</p> <p></p> <p>Beyond that, I'm not aware of any documentation on the format. A couple of years  ago, Scott Piper of Summit Route did some research and wrote down his findings. (I didn't find this blog post until after I did yesterday's research, would have  saved me a lot of time!) There's also the error message returned by <code>sts:GetAccessKeyInfo</code>:</p> <pre><code>$ aws sts get-access-key-info --access-key-id A\n\nParameter validation failed:\nInvalid length for parameter AccessKeyId, value: 1, valid range: 16-inf\n</code></pre> <p>So we know that it needs to be at least 16 characters - but I've personally only ever seen 20-character long key IDs.</p> <p>I noticed that the characters were always alphanumeric (A-Z0-9), but not the  entire 36 character set. <code>0</code>, <code>1</code>, <code>8</code> and <code>9</code> were absent. That leaves us with 32 characters - a nice even five bits per character. So the total set of valid characters is <code>ABCDEFGHIJKLMNOPQRSTUVWXYZ234567</code>.</p> <p>I noticed that <code>keyid[4:12]</code> (the eight characters after the AKIA/ASIA prefix) were almost always the same value for a given AWS account. On a whim, I decided to pass an almost-valid-but-last-character-changed to <code>sts:GetAccessKeyInfo</code> and it still returned the same account number! This means that there isn't some kind of checksum. Interesting. I incrementally worked out I could completely change <code>keyid[13:20]</code> (the last seven characters) and the returned account ID would be unaffected.</p> <p><code>keyid[12]</code> was interesting. It would add or subtract 1 from the numeric account ID depending on if it was before/after the letter Q in the alphabet.</p> <p>Lets recap so far: <code>keyid[4:13]</code> are somehow related to the account ID (with the last one being kinda weird) and <code>keyid[13:20]</code> appear to be completely random. I wanted to see if I could reverse-engineer the algorithm that <code>sts:GetAccessKeyInfo</code> is using.</p> <p>I ran the following to see what changing <code>keyid[11]</code> would do:</p> <pre><code>$ aws sts get-access-key-info --access-key-id ASIAY34FZKBOKMUTVV7A --query Account\n\"609629065308\"\n\n                                                         | `O` changed to `N`\n$ aws sts get-access-key-info --access-key-id ASIAY34FZKBNKMUTVV7A --query Account\n\"609629065306\"\n</code></pre> <p>So it was reduced by 2. Then I tried decrementing <code>keyid[10]</code> (<code>ASIAY34FZKAOKMUTVV7A</code>)  and got <code>609629065244</code> - a reduction of 64. Looks like this is big-endian. I found that a <code>keyid[4:12]</code> of <code>QAAAAAAA</code> would result in an account ID of  <code>000000000000</code> and <code>6RVFFB77</code> in <code>999999999998</code>. </p>"},{"location":"blog/2020/09/26/aws-access-key-format.html#the-resulting-code","title":"The resulting code","text":"<p>I used this knowledge to write following Go code to reproduce <code>sts:GetAccessKeyInfo</code>:</p> <pre><code>package main\n\nimport (\n    \"fmt\"\n    \"strconv\"\n    \"strings\"\n    \"github.com/kenshaw/baseconv\"\n)\n\n// technically this code works equally well for principal IDs (e.g. AROA or AIDA prefixes) but\n// i don't want to make the code sample any more complex\nvar ErrMissingPrefix = errors.New(\"only keys with AKIA or ASIA prefixes are supported\")\nvar ErrUnsupportedKey = errors.New(\"old-format keys (created before 29 march 2019) are unsupported\")\n\nfunc getAccessKeyInfo(accessKeyId string) (string, error) {\n    if strings.HasPrefix(accessKeyId, \"AKIA\") || strings.HasPrefix(accessKeyId, \"ASIA\") {\n        if accessKeyId[4] &lt; 'Q' {\n            return \"\", ErrUnsupportedKey\n        }\n    } else {\n        return \"\", ErrMissingPrefix\n    }\n\n    base10 := \"0123456789\"\n    base32AwsFlavour := \"ABCDEFGHIJKLMNOPQRSTUVWXYZ234567\"\n\n    offsetStr, _ := baseconv.Convert(\"QAAAAAAA\", base32AwsFlavour, base10)\n    offset, _ := strconv.Atoi(offsetStr)\n\n    offsetAccountIdStr, _ := baseconv.Convert(accessKeyId[4:12], base32AwsFlavour, base10)\n    offsetAccountId, _ := strconv.Atoi(offsetAccountIdStr)\n\n    accountId := 2 * (offsetAccountId - offset)\n\n    if strings.Index(base32AwsFlavour, accessKeyId[12:13]) &gt;= strings.Index(base32AwsFlavour, \"Q\") {\n        accountId++\n    }\n\n    return fmt.Sprintf(\"%012d\", accountId), nil\n}\n\nfunc main() {   \n    fmt.Println(getAccessKeyInfo(\"ASIAY34FZKBOKMUTVV7A\"))\n}\n</code></pre> <p>Playground link to try it yourself.</p>"},{"location":"blog/2020/09/26/aws-access-key-format.html#conclusion-or-so-i-thought","title":"Conclusion, or so I thought","text":"<p>I was ready to publish this blog post yesterday, but then I hit an issue. My  access key ID for my personal IAM user hasn't been rotated since March 2016 (I know, I know!) It begins <code>AKIAJ...</code> - what? Last I checked, J came before Q in the alphabet and so my code returned a negative account ID. But my account ID isn't negative. Well shit.</p> <p>These types of keys are different. I looked at the distribution of <code>keyid[4]</code>  in my CloudTrail logs. Most are in the range <code>Q-Z2-7</code>, but there are some that  are <code>I</code> or <code>J</code>. The newer keys (range <code>Q-Z2-7</code>) can have their last bytes changed and <code>sts:GetAccessKeyInfo</code> will still return correct results. The older keys  (<code>I</code> and <code>J</code>) will return <code>ValidationError</code> if the last bytes are changed. They also return the same error if the key is deleted (unlike new keys). This leads  me to believe that the older keys have no internal structure and the account ID  has to be looked up from a datastore. </p> <p>I asked for help on Twitter and got some great responses. Especially  helpful was @NYSharpie's tweet where he noticed keys created after  early 2019 are when it switched to &gt;= Q. Looking at resources in my own account, it looks like the switchover was sometime between the 27<sup>th</sup> of and 29<sup>th</sup> of March 2019.</p> <p>I've also learned that keys created before ~2010 don't even have the  <code>AKIA</code> prefix! E.g. the key ID <code>1YRA5YCR63BKA0BX35G2</code> was created in 2008 and the STS API will return the correct account ID for it. </p>"},{"location":"blog/2020/09/26/aws-access-key-format.html#questions","title":"Questions","text":"<ul> <li> <p>Anyone have any theories on what changed, and why? </p> </li> <li> <p>Do you think there's a pattern to the old-style key IDs, or they're completely random?</p> </li> <li> <p><code>sts:GetSessionToken</code> for my <code>AKIAJ</code> key returns <code>ASIAY</code> but I still see <code>ASIAJ</code>   and <code>ASIAI</code> for some roles - what's going on there?</p> </li> </ul> <p>Please join the conversation if you have any wild theories, I'm keen to explore this pointless space. Here's the tweet again if you want to respond.</p>"},{"location":"blog/2020/09/26/aws-access-key-format.html#updates","title":"Updates","text":"<p>25/10/2023: Tal Be'ery published a blog post wherein he used bit-shifting and masking to decode account IDs from access key IDs in Python.  This would be more efficient than my code sample which has a condition (checking if a character is greater than or equal to <code>Q</code>)</p> <p>07/01/2024: Clarified behaviour of old <code>AKIA</code> keys and even older <code>AKIA</code>-less  keys. Also updated code sample to explicitly fail for older keys (rather than returning negative account IDs)</p> <p>07/01/2024: TruffleHog now prints out account IDs based on Tal's  bit-shifting code. This knowledge is becoming useful!</p> <p>29/05/2024: I've narrowed down the switchover window to sometime between 27-29<sup>th</sup> of March 2019.</p>"},{"location":"blog/2021/09/15/aws-federation-comes-to-github-actions.html","title":"AWS federation comes to GitHub Actions","text":"<p>At the time of writing, this functionality exists but has yet to be announced or documented. It works, though!</p> <p>EDIT: Here is the functionality on the GitHub roadmap.</p> <p>GitHub Actions has new functionality that can vend OpenID Connect credentials to jobs running on the platform. This is very exciting for AWS account administrators as it means that CI/CD jobs no longer need any long-term secrets to be stored in GitHub. But enough of that, here's how it works:</p> <p>First, an AWS IAM OIDC identity provider and an AWS IAM role that GitHub Actions can assume. You can do that by deploying this CloudFormation template to your  account.</p> <pre><code>Parameters:\n  GithubOrg: # can also be a regular user\n    Type: String\n    Default: aidansteele\n  FullRepoName:\n    Type: String\n    Default: aidansteele/aws-federation-github-actions\n\nResources:\n  Role:\n    Type: AWS::IAM::Role\n    Properties:\n      RoleName: ExampleGithubRole\n      ManagedPolicyArns: [arn:aws:iam::aws:policy/ReadOnlyAccess]\n      AssumeRolePolicyDocument:\n        Statement:\n          - Effect: Allow\n            Action: sts:AssumeRoleWithWebIdentity\n            Principal:\n              Federated: !Ref GithubOidc\n            Condition:\n              StringLike:\n                token.actions.githubusercontent.com:sub: !Sub repo:${FullRepoName}:*\n\n  GithubOidc:\n    Type: AWS::IAM::OIDCProvider\n    Properties:\n      Url: https://token.actions.githubusercontent.com\n      ThumbprintList: [6938fd4d98bab03faadb97b34396831e3780aea1]\n      ClientIdList: \n        - !Sub https://github.com/${GithubOrg}\n\nOutputs:\n  Role:\n    Value: !GetAtt Role.Arn      \n</code></pre> <p>Ok, this new role can now be assumed by GitHub Actions, but crucially: only by jobs in my <code>aidansteele/aws-federation-github-actions</code> repo. Without that condition, any repo on GitHub could assume this role.</p> <p>Next, the GitHub workflow definition. Put this in a repo:</p> <pre><code># .github/workflows/example.yml\nname: Example\non:\n  push:\n\njobs:\n  build:\n    runs-on: ubuntu-latest\n    permissions:\n      id-token: write\n      contents: read\n    steps:\n      - run: sleep 5 # there's still a race condition for now\n\n      - name: Configure AWS\n        run: |\n          export AWS_ROLE_ARN=arn:aws:iam::0123456789012:role/ExampleGithubRole\n          export AWS_WEB_IDENTITY_TOKEN_FILE=/tmp/awscreds\n          export AWS_DEFAULT_REGION=us-east-1\n\n          echo AWS_WEB_IDENTITY_TOKEN_FILE=$AWS_WEB_IDENTITY_TOKEN_FILE &gt;&gt; $GITHUB_ENV\n          echo AWS_ROLE_ARN=$AWS_ROLE_ARN &gt;&gt; $GITHUB_ENV\n          echo AWS_DEFAULT_REGION=$AWS_DEFAULT_REGION &gt;&gt; $GITHUB_ENV\n\n          curl -H \"Authorization: bearer $ACTIONS_ID_TOKEN_REQUEST_TOKEN\" \"$ACTIONS_ID_TOKEN_REQUEST_URL\" | jq -r '.value' &gt; $AWS_WEB_IDENTITY_TOKEN_FILE\n\n      - run: aws sts get-caller-identity # just an example. why not deploy something?\n</code></pre> <p>Tada, you now have a GitHub Actions workflow that assumes your role. It works because the AWS SDKs (and AWS CLI) support using the <code>AWS_WEB_IDENTITY_TOKEN_FILE</code>  and <code>AWS_ROLE_ARN</code> environment variables since AWS EKS needed this.</p>"},{"location":"blog/2021/09/15/aws-federation-comes-to-github-actions.html#some-potential-trust-policies","title":"Some potential trust policies","text":"<p>Maybe you want an IAM role that can be assumed by any branch in any repo in your GitHub org, e.g. with relatively few permissions needed for PRs. You can do this:</p> <pre><code>Effect: Allow\nAction: sts:AssumeRoleWithWebIdentity\nPrincipal:\n  Federated: !Ref GithubOidc\nCondition:\n  StringLike:\n    token.actions.githubusercontent.com:sub: repo:your-github-org/*\n</code></pre> <p>Maybe you want an IAM role scoped only to workflows on the <code>main</code> branches, because this will be doing sensitive deployments. In that case, you can do:</p> <pre><code>Effect: Allow\nAction: sts:AssumeRoleWithWebIdentity\nPrincipal:\n  Federated: !Ref GithubOidc\nCondition:\n  StringLike:\n    token.actions.githubusercontent.com:sub: repo:your-github-org/*:ref:refs/heads/main\n</code></pre>"},{"location":"blog/2021/09/15/aws-federation-comes-to-github-actions.html#faq","title":"FAQ","text":""},{"location":"blog/2021/09/15/aws-federation-comes-to-github-actions.html#what-does-the-jwt-look-like","title":"What does the JWT look like?","text":"<pre><code>{\n  \"actor\": \"aidansteele\",\n  \"aud\": \"https://github.com/aidansteele\",\n  \"base_ref\": \"\",\n  \"event_name\": \"push\",\n  \"exp\": 1631672856,\n  \"head_ref\": \"\",\n  \"iat\": 1631672556,\n  \"iss\": \"https://token.actions.githubusercontent.com\",\n  \"job_workflow_ref\": \"aidansteele/aws-federation-github-actions/.github/workflows/test.yml@refs/heads/main\",\n  \"jti\": \"8ea8373e-0f9d-489d-a480-ac37deexample\",\n  \"nbf\": 1631671956,\n  \"ref\": \"refs/heads/main\",\n  \"ref_type\": \"branch\",\n  \"repository\": \"aidansteele/aws-federation-github-actions\",\n  \"repository_owner\": \"aidansteele\",\n  \"run_attempt\": \"1\",\n  \"run_id\": \"1235992580\",\n  \"run_number\": \"5\",\n  \"sha\": \"bf96275471e83ff04ce5c8eb515c04a75d43f854\",\n  \"sub\": \"repo:aidansteele/aws-federation-github-actions:ref:refs/heads/main\",\n  \"workflow\": \"CI\"\n}\n</code></pre>"},{"location":"blog/2021/09/15/aws-federation-comes-to-github-actions.html#and-the-cloudtrail-entry","title":"And the CloudTrail entry?","text":"<pre><code>{\n  \"awsRegion\": \"us-east-1\",\n  \"eventCategory\": \"Management\",\n  \"eventID\": \"096c33c2-7d1d-49c6-a87b-fb4bbb5d43d6\",\n  \"eventName\": \"AssumeRoleWithWebIdentity\",\n  \"eventSource\": \"sts.amazonaws.com\",\n  \"eventTime\": \"2021-09-15T03:00:36Z\",\n  \"eventType\": \"AwsApiCall\",\n  \"eventVersion\": \"1.08\",\n  \"managementEvent\": true,\n  \"readOnly\": true,\n  \"recipientAccountId\": \"0123456789012\",\n  \"requestID\": \"d62256aa-fe9b-4fe4-bd7b-8a3917e35d13\",\n  \"requestParameters\": {\n    \"roleArn\": \"arn:aws:iam::0123456789012:role/ExampleGithubRole\",\n    \"roleSessionName\": \"botocore-session-1631674835\"\n  },\n  \"resources\": [\n    {\n      \"ARN\": \"arn:aws:iam::0123456789012:role/ExampleGithubRole\",\n      \"accountId\": \"0123456789012\",\n      \"type\": \"AWS::IAM::Role\"\n    }\n  ],\n  \"responseElements\": {\n    \"assumedRoleUser\": {\n      \"arn\": \"arn:aws:sts::0123456789012:assumed-role/ExampleGithubRole/botocore-session-1631674835\",\n      \"assumedRoleId\": \"AROAY99999AOBPS6VNUFM:botocore-session-1631674835\"\n    },\n    \"audience\": \"https://github.com/aidansteele\",\n    \"credentials\": {\n      \"accessKeyId\": \"ASIAY29999OMG3MKNAG\",\n      \"expiration\": \"Sep 15, 2021 4:00:36 AM\",\n      \"sessionToken\": \"IQ[trimmed]lg==\"\n    },\n    \"provider\": \"arn:aws:iam::0123456789012:oidc-provider/token.actions.githubusercontent.com\",\n    \"subjectFromWebIdentityToken\": \"repo:aidansteele/aws-federation-github-actions:ref:refs/heads/main\"\n  },\n  \"sourceIPAddress\": \"104.211.45.236\",\n  \"tlsDetails\": {\n    \"cipherSuite\": \"ECDHE-RSA-AES128-GCM-SHA256\",\n    \"clientProvidedHostHeader\": \"sts.us-east-1.amazonaws.com\",\n    \"tlsVersion\": \"TLSv1.2\"\n  },\n  \"userAgent\": \"aws-cli/2.2.35 Python/3.8.8 Linux/5.8.0-1040-azure exe/x86_64.ubuntu.20 prompt/off command/sts.get-caller-identity\",\n  \"userIdentity\": {\n    \"identityProvider\": \"arn:aws:iam::0123456789012:oidc-provider/token.actions.githubusercontent.com\",\n    \"principalId\": \"arn:aws:iam::0123456789012:oidc-provider/token.actions.githubusercontent.com:https://github.com/aidansteele:repo:aidansteele/aws-federation-github-actions:ref:refs/heads/main\",\n    \"type\": \"WebIdentityUser\",\n    \"userName\": \"repo:aidansteele/aws-federation-github-actions:ref:refs/heads/main\"\n  }\n}\n</code></pre>"},{"location":"blog/2021/09/15/aws-federation-comes-to-github-actions.html#can-i-use-those-jwt-claims-as-role-session-tags","title":"Can I use those JWT claims as role session tags?","text":"<p>Not directly, unfortunately. AWS requires role session tags to follow a fairly  specific format - one that I doubt GitHub Actions will implement. But you could have a token vending machine\u2026 stay tuned.</p> <p>EDIT: I built an (still cooling down after coming out of the oven) example of how you could use all those JWT claims as role session tags. Take a look at glassechidna/ghaoidc and let me know your thoughts.</p>"},{"location":"blog/2022/01/20/aws-gwlb-deep-packet-manipulation.html","title":"AWS GWLB: Deep Packet Manipulation","text":"<p>AWS introduced Gateway Load Balancers back in November 2020. A reasonably accurate tl;dr would be that they are like having highly available,  auto-scaling NAT instances. That intro blog post will explain them better than I can.</p> <p>The blog post mentions a dozen AWS partners that implement various flavours of firewalls, deep packet inspection, DDoS protection, etc. It's all useful and serves a genuine need, but not very exciting from a developer perspective. </p> <p>There's been a real dearth of community blog posts about it in the subsequent  years. There's the GeneveProxy post by Luc van Donkersgoed, which does a really great job of explaining how the GWLB works and how to build a sample app for it, but that's about it. I suspect it's because going beyond a simple packet-by-packet model explodes in complexity. If you are more interested in the demos than why it's hard, you can skip to the bottom. Otherwise, I'll try explain:</p>"},{"location":"blog/2022/01/20/aws-gwlb-deep-packet-manipulation.html#the-complexity","title":"The complexity","text":"<p>GWLBs aren't hard themselves, look at this diagram (from Amazon's blog post):</p> <p></p> <p>It's inspecting and modifying network traffic in general that is extremely  difficult. Especially non-trivial modifications.  Take the following diagram as  an example. This is just one packet in a flow of packets between an EC2  instance and the Internet when <code>curl https://google.com</code> is run.</p> <p></p> <p>You can think of this packet as having many layers. Each layer \"wraps\" the layer below it. The bottom six layers were sent by the EC2 instance. The top three layers are GWLB-specific. They identify which VPC endpoint (e.g. customer) the packet came from and which \"flow\" of packets this particular packet belongs to.</p> <p>Say we want to change all web requests to google.com to have the <code>User-Agent</code> request header instead be lower-case, e.g. <code>user-agent</code>. This would require us to parse the formats for:</p> <ul> <li>The inner IPv4 layer, to identify is this a TCP packet</li> <li>The TCP layer, to identify if the destination port is 80 or 443</li> <li>The TLS layer, to (magically, for now) decrypt the payload</li> <li>The HTTP/2 layer, to inspect the multiplexed streams within</li> <li>The frames in each HTTP/2 stream, to identify if they are a <code>HEADERS</code> frame.</li> <li>The headers in the HTTP/2 frame, to see if the <code>User-Agent</code> header is present.</li> </ul> <p>Finally we would have to edit the packet in memory at the right offset to change <code>U</code> to <code>u</code> and <code>A</code> to <code>a</code>, correct the checksums at every layer of the packet and re-encrypt the TLS payload. That's a lot of work.</p> <p>And that's a trivial change: the packet length hasn't changed. Imagine if wanted to insert a few additional headers in that request. Maybe that would push the packet length over the typical 1500 byte limit for packets on the Internet. That increases the amount of work needed by orders of magnitude: now we need to reimplement the TCP state machine, because we'll now need two packets. And those packets each need sequence numbers. But the original EC2 instance will get a response from Google for sequence numbers it didn't expect, so the connection will fail. So what we need to do is instead terminate the TCP connection at the GWLB appliance and open a new connection to Google from the GWLB appliance. The app will need to juggle these two TCP connections and pass the underlying data to and from Google and the EC2 instance, all while keeping the two connection's different states in sync.</p> <p>That's so much work that it's no wonder that even after more than a year, only  massive well-funded vendors have  implemented this capability. And even then, it  looks like they're limited to either read-only inspection or dropping suspicious  packets.</p>"},{"location":"blog/2022/01/20/aws-gwlb-deep-packet-manipulation.html#a-solution","title":"A solution","text":"<p>I've come up with a handful of demo applications, only made possible thanks to some fantastic open source software. Specifically:</p> <ul> <li> <p><code>inet.af/netstack</code>: a reimplementation   of the entire Linux TCP/IP stack in Go, extracted from the gVisor project.</p> </li> <li> <p><code>github.com/google/gopacket</code>   to extract and parse the Geneve, IP, TCP, UDP, etc layers from the raw packets   delivered by the GWLB.</p> </li> <li> <p><code>httputil</code> in the Go stdlib, to   reverse-proxy HTTP and HTTPS traffic and  parse flows into individual request   and response objects.</p> </li> <li> <p><code>github.com/aws/aws-sdk-go</code> to   use AWS KMS asymmetric keys for the root certificate authority that can be   installed on EC2 instances for transparent TLS decryption - without having to   manage a highly-sensitive private key.</p> </li> <li> <p><code>rogchap.com/v8go</code> to embed the V8   JavaScript engine into Go, so that we can write scripts to modify traffic   in JS, which is more familiar than Go to many developers.</p> </li> </ul>"},{"location":"blog/2022/01/20/aws-gwlb-deep-packet-manipulation.html#example-use-cases","title":"Example use cases","text":"<p>These are really just intended to demonstrate that anything is possible in the world of software-defined networking. Please ping me on Twitter with any cool ideas you have. Or any enhancements to the following ideas.</p> <ul> <li><code>lambda_acceptor/lambda_acceptor.go</code>   takes the idea of AWS API Gateway Lambda authorizers and applies   it to VPC flows. At the start of every new connection, a Lambda function is   invoked and returns a decision about whether to allow or drop the connection.   It's like security groups 2.0. Input/output looks like this:</li> </ul> <p></p> <ul> <li><code>flowdogshark/flowdogshark.go</code> is an   <code>extcap</code> plugin for Wireshark that allows you to live-attach    Wireshark to flowdog and capture traffic flowing through your VPC. Given that   flowdog does TLS interception (see later section in README), it can even use    Wireshark's support for decoding TLS. Here's an example of intercepting the   Amazon SSM agent:</li> </ul> <p></p> <ul> <li> <p><code>account_id_emf/account_id_emf.go</code>   is an example of scanning all AWS API calls made within the VPC for SigV4 auth   headers, extracting the AWS account ID and emitting it to   CloudWatch via specially-formatted logs that are turned into metrics. This could   be used to alert on newly-seen account IDs: a potential indicator of a compromised   instance.</p> </li> <li> <p><code>upsidedown/upsidedown.go</code> is an    implementation of the classic Upside-Down-Ternet. It blurs and    rotates every image 180\u00ba when browsing the net.</p> </li> </ul> <p></p> <ul> <li><code>sts_rickroll/sts_rickroll.go</code> is   another silly example. Here we are modifying the response of the AWS API call   for <code>aws sts get-caller-identity</code> to return something unexpected. You could   equally use the same logic to return your favourite video on every seventh   object downloaded through an S3 VPC gateway. </li> </ul> <p></p> <ul> <li> <p><code>gwlb/websocket.go</code> is not an example, but I got lazy.   Nick Frichette had the great suggestion of intercepting the SSM agent   for shenanigans. This code will detect websockets and parse messages, but right   now only passes them back and forth. Soon\u2122.</p> </li> <li> <p><code>cloudfront_functions/rick.js</code> is   an example of how the CloudFront Functions event model can be   applied to rewriting HTTP(S) requests inside a VPC. In this particular example,   we're ensuring that any AWS Workspaces users visiting YouTube   can only watch one particular video. Code:</p> </li> </ul> <pre><code>function onRequest(event) {\n    const r = event.request;\n    if (r.headers.host.value !== \"www.youtube.com\") {\n        return r;\n    }\n\n    const onlyVideoOnYoutube = \"https://www.youtube.com/watch?v=dQw4w9WgXcQ\";\n    const referer = r.headers.referer;\n    if (referer &amp;&amp; referer.value === onlyVideoOnYoutube) {\n        return r;\n    }\n\n    if (r.uri === \"/watch\" &amp;&amp; r.querystring.v.value === \"dQw4w9WgXcQ\") {\n        return r;\n    }\n\n    return {\n        statusCode: 302,\n        statusDescription: 'Found',\n        headers: {\n            location: { value: onlyVideoOnYoutube }\n        }\n    };\n}\n</code></pre>"},{"location":"blog/2022/01/20/aws-gwlb-deep-packet-manipulation.html#bonus-tls","title":"Bonus: TLS","text":"<p>We haven't broken TLS. For this app, we create a custom root certificate authority  and add it to the trust store on our EC2 instances. Rather than deal in sensitive  private key material, we use  AWS KMS' support for asymmetric keys for our  private key. <code>generate.go</code> creates a certificate  using that key. That certificate is then stored and trusted on the OS (e.g. in  Amazon Linux 2 you would run <code>cat $CERT &gt;&gt; /usr/share/pki/ca-trust-source/anchors/lol.pem &amp;&amp; update-ca-trust</code>)</p> <p>Rather than invoking KMS on every TLS connection, on launch this app creates an ephemeral key pair and certificate in memory, asks KMS to sign it and then uses that as an intermediate certificate authority. This means we can have fast TLS de/re-encryption with no stored secrets.</p> <p>When Wireshark is attached, flowdog can stream TLS key logs in NSS Key Log Format. This allows the Wireshark user to view all decrypted TLS traffic without giving away either the KMS private key (impossible) or intermediate CA private key (very unwise).</p>"},{"location":"blog/2020/09/29/aws-iam-needs-resourceorgid.html","title":"AWS IAM needs aws:ResourceOrgID","text":""},{"location":"blog/2020/09/29/aws-iam-needs-resourceorgid.html#update","title":"Update","text":"<p>In April 2022, AWS released <code>aws:ResourceOrgID</code> (and a couple of other related condition keys). This blog post does a great job of describing  how it works. I'll leave the rest of this blog post in place for historical interest, but it's now outdated.</p>"},{"location":"blog/2020/09/29/aws-iam-needs-resourceorgid.html#background","title":"Background","text":"<p>In May 2018, AWS released a new IAM condition key, <code>aws:PrincipalOrgID</code>. This was a game-changer for improving security posture as it made the dream of extremely granular AWS accounts that much more achievable. I won't go into detail on why it was so useful as I'm sure others have written much better explanations than I could.</p> <p>The existence of <code>aws:PrincipalOrgID</code> made writing some resource-based policies much easier. In the same theme, a hypothetical <code>aws:ResourceOrgID</code> condition key would be extremely useful for identity-based policies, service control policies and VPC endpoint policies. I'm not alone in thinking this. For example, in Square's blog post on adoption of VPC endpoints they wrote this:</p> <p>There is a limit of 20,480 characters on VPC Endpoint Policies. While this may  suffice for most use-cases, at Square we currently have close to 200 AWS accounts  and are expected to add hundreds more AWS accounts as more teams build in the  cloud. We calculated that with our policy specification, we cannot list more  than approximately 800 AWS accounts. In order to monitor this we have a graph  of the VPC Endpoint Policy text sizes (in number of characters) and have an  alarm set if that reaches into the ten thousand characters.</p> <p>If AWS offers a <code>aws:ResourceOrgID</code> IAM conditional context key, similar to  the <code>aws:PrincipalOrgID</code> conditional context key, we would not have to manually  list AWS accounts in the resources section.</p> <p>Additionally, this is a frequent topic of complaining discussion in the Cloud Security Forum:</p> <p></p>"},{"location":"blog/2020/09/29/aws-iam-needs-resourceorgid.html#arguments-for-awsresourceorgid","title":"Arguments for aws:ResourceOrgID","text":"<p>Something to keep in mind is that these scenarios may seem trivial to fix: it's a matter of \"duh, don't do that thing\" and that is indeed often true of the demographics that might read this blog. But there are large orgs that have hundreds to thousands of developers of significantly differing levels of AWS proficiency. That's where an SCP like the following can be exceedingly useful:</p> <pre><code>{\n    \"Effect\": \"Deny\",\n    \"Action\": \"kms:Decrypt\",\n    \"Condition\": {\n        \"StringNotEquals\": {\n            \"aws:ResourceOrgID\": \"o-my-org-id\"\n        }\n    }\n}\n</code></pre> <p>Scenario one: Imagine a scenario wherein an attacker has gained control of an  EC2 instance with an IAM policy that has <code>s3:PutObject</code> permissions on <code>*</code>  resources. The attacker could exfiltrate data to a bucket that they own by  running <code>aws s3 cp sensitive-file s3://attacker-bucket/</code>. This usually should  be prevented by being explicit about buckets that it has write access to. But for argument's sake, it needs unbounded access. This could be made safer by adding a condition that the instance role can only write to buckets owned by the org.</p> <p>Scenario two: An attacker can trivially circumvent the protection in scenario one by using their own access key ID instead of the instance's to perform the  file upload. This is where the condition key would be useful in a VPC endpoint: any same-region traffic to S3 could be restricted to buckets owned by the org by including an <code>aws:ResourceOrgID</code> condition key.</p> <p>Scenario three: You are running a centrally-administered EKS/ECS cluster. Images are stored in ECR repositories across the org. You want to prevent attackers from specifying that an image hosted in an attacker-owned account should be run - the condition key helps here too. Likewise with ECR VPC endpoint policies.</p> <p>Scenario four: An attacker convinces your application to call <code>kms:Decrypt</code> on an attacker-provided ciphertext that has been encrypted with a KMS CMK owned by the attacker. The attacker's CMK has a key policy that grants <code>kms:Decrypt</code> to <code>*</code> principals and your application has an IAM policy that grants <code>kms:Decrypt</code> on <code>*</code> keys.</p>"},{"location":"blog/2020/09/29/aws-iam-needs-resourceorgid.html#the-problem-with-kms-keys","title":"The problem with KMS keys","text":"<p>I think scenario four warrants the most thought. In my (limited, anecdotal)  experience AWS KMS is the service that suffers in terms of the ratio of being poorly understood relative to how critical it is. There's also some unfortunate gotchas in the user experience that make security issues more likely.</p> <p>The first issue concerns the relationship between KMS key IDs, key aliases and IAM principal policies. When you use a KMS key, you're usually calling one of the KMS APIs (like <code>Decrypt</code>, <code>Encrypt</code>, etc) with a key alias, e.g.:</p> <pre><code>    kmsClient.encrypt({ \n        Plaintext = someSensitiveUserData, \n        KeyId = \"alias/MyEncryptionKey\" \n    })\n</code></pre> <p>This is a much more pleasant dev experience than passing in a key ID, which is an opaque GUID. This is doubly the case if you are using multiple AWS regions and/or multiple AWS accounts: it's very tempting to use the same alias in each environment so you don't need to make your code configurable.</p> <p>Where this experience falls down is in assigning permissions. The first thing a developer will try is the following IAM policy statement:</p> <pre><code>- Effect: Allow\n  Action: [kms:Encrypt, kms:Decrypt]\n  Resource: !Sub arn:aws:kms:${AWS::Region}:${AWS::AccountId}:alias/MyEncryptionKey\n</code></pre> <p>This will fail. The resource must be the key ID. So what are they going to  do? They have a few lousy options:</p> <ul> <li>Make the key ID a template parameter. This now makes instantiating the template   more annoying.</li> <li>Add all the key IDs to a <code>Mappings</code> section. This now makes the <code>Resource</code> field   more effort and uglier. You might call this petty, but would you say developers   aren't petty?</li> <li>Change the statement to be <code>Resource: \"*\"</code>. And why wouldn't they? It's substantially   easier, guaranteed to work and the attack scenarios aren't immediately apparent.    How insecure could it be when the key alias is hardcoded in the app?</li> </ul> <p>The issue only arises when the app later calls <code>kms:Decrypt</code> if the ciphertext is provided by the user. The issue is that an attacker can provide a ciphertext encrypted with their key. <code>kms:Decrypt</code> supports an optional <code>KeyId</code>  parameter, but 99% of the time I see the following usage:</p> <pre><code>    const resp = await kmsClient.decrypt({ CiphertextBlob = someCiphertextString });\n    const decrypted = resp.Plaintext;\n    // some code that uses the decrypted value\n</code></pre> <p>This works because the key ID is encoded into the ciphertext blob returned by <code>kms:Encrypt</code>. It's very handy because you don't need to record the key ID alongside the encrypted data! And it's typically pretty secure, because if an attacker  tries to modify the ciphertext, it becomes invalid and returns and error as the cipher is authenticated. This in combination with the hardcoded encryption key will lead most developers to feeling confident in the security of the system as-is.</p> <p>But that authentication counts for nothing if you don't validate that the  <code>resp.KeyId</code> matches the <code>KeyId</code> that you passed to the <code>kms:Encrypt</code> API call.</p> <p>I suspect that this problem is more likely to occur than you might first guess. Think of scenarios like a Dropbox app, where the backend encrypts users' files that are then stored on users' machines. The authenticated nature of the ciphertext would lead most reasonable developers to believe the data is tamper-proof.</p> <p>It's for this reason that I think an <code>aws:ResourceOrgID</code> condition key would be extremely useful for AWS Organization SCPs: to deny the very uncommon scenario of cross-org encryption/decryption. In the unlikely scenario that is part of your workflow, it can always be narrowed down to not deny usage of cross-org keys by suitably tagged IAM principals.</p>"},{"location":"blog/2020/09/29/aws-iam-needs-resourceorgid.html#what-does-aws-have-to-say-about-all-this","title":"What does AWS have to say about all this?","text":"<p>The <code>kms:Decrypt</code> do actually have a bit to say on this, but it's not spelled out as clearly as might be necessary for someone without sufficient pre-existing background. The relevant excerpts:</p> <p>[s]pecifying the CMK is always recommended as a best practice. When you use  the KeyId parameter to specify a CMK, AWS KMS only uses the CMK you specify.  If the ciphertext was encrypted under a different CMK, the Decrypt operation  fails. This practice ensures that you use the CMK that you intend.</p> <p>Whenever possible, use key policies to give users permission to call the Decrypt  operation on a particular CMK, instead of using IAM policies. Otherwise, you might create an IAM user policy that gives the user Decrypt permission on all  CMKs. This user could decrypt ciphertext that was encrypted by CMKs in other  accounts if the key policy for the cross-account CMK permits it. If you  must use an IAM policy for Decrypt permissions, limit the user to particular  CMKs or particular trusted accounts. </p> <p>[...]</p> <p><code>KeyId</code> [...] If you used a symmetric CMK, AWS KMS can get the CMK from metadata that  it adds to the symmetric ciphertext blob. However, it is always recommended  as a best practice. This practice ensures that you use the CMK that you intend.</p>"},{"location":"blog/2020/09/29/aws-iam-needs-resourceorgid.html#youre-wrong","title":"You're wrong","text":"<p>I probably am wrong. Or maybe I've misestimated the likelihood of something. Or very likely: I've not considered another great reason why this would be a wonderful new condition key. Please reach out to me on Twitter and share your thoughts.</p>"},{"location":"blog/2021/10/12/aws-iam-oidc-idps-need-more-controls.html","title":"AWS IAM OIDC IDPs need more controls","text":""},{"location":"blog/2021/10/12/aws-iam-oidc-idps-need-more-controls.html#background-primer","title":"Background primer","text":"<p>In my post AWS federation comes to GitHub Actions I wrote about  GitHub Actions' new ability to federate access into AWS (and other clouds) via OpenID Connect. This is really great, much better than the prior state of affairs, but needs improvement on the AWS side.</p> <p>Today, AWS IAM OIDC identity providers have a few configuration options:</p> <ul> <li> <p>URL: this is specified at OIDC IDP creation time, immutable and must be unique   on a per-AWS account basis. Corresponds to the <code>iss</code> claim in OIDC tokens.</p> </li> <li> <p>Client ID list: this is specified at creation time, but also mutable and can   added to or removed from after the fact. Corresponds to the <code>aud</code> claim in   OIDC tokens.</p> </li> <li> <p>Thumbprint list: this is specified at creation time and can be updated after    the fact. This is a security measure to ensure that MITM attacks between AWS   and the IDP cannot occur, nor can an attacker register an expired IDP domain   and gain access to your account.</p> </li> <li> <p>Tags: mutable, not super interesting as the functionality is free and there   are only very rarely multiple OIDC IDPs in any given account.</p> </li> </ul> <p>The above is the full extent of what exists today. I think it's insufficient, especially in light of the wide interest in GitHub Actions' support and EKS' (AWS hosted Kubernetes service) dependence on OIDC.</p>"},{"location":"blog/2021/10/12/aws-iam-oidc-idps-need-more-controls.html#my-requests","title":"My requests","text":""},{"location":"blog/2021/10/12/aws-iam-oidc-idps-need-more-controls.html#centralised-control-and-guardrails","title":"Centralised control and guardrails","text":"<p>In the previous blog post, I shared an example CloudFormation template for using the GHA OIDC feature. It looks like this:</p> <p></p> <p>I have highlighted the most important part of the template. When this blog post was first published, that condition was technically optional (in the sense that  everything will work without it) but devastating if it was omitted. It is the  only thing that controls which repositories on GitHub are authorised to assume  this role. (GitHub allows configuration of the <code>aud</code> claim, so the <code>ClientIdList</code>  property of the OIDC  IDP is not a security control.) Since then, AWS has made the condition mandatory. Scott Piper over at Wiz has a great write-up on the happy ending.</p> <p>Many AWS customers now allow developers to freely create IAM roles, thanks to permission boundaries. There is no analogue to permission boundaries for role trust policies - so once the GitHub IDP exists in an account,  administrators have no way to enforce that it can only be used by particular GitHub orgs. </p> <p>The same problem exists for Kubernetes: once the IDP exists in an account, there is no way for administrators to enforce that roles in that account can only be used by particular Kubernetes namespaces.</p> <p>Request: something equivalent to permission boundaries for role trust policies. Really, anything. It could be \"trust policy boundary\" policies that are attached to individual roles with conditions mandating attachment in <code>iam:CreateRole</code> and <code>iam:UpdateAssumeRolePolicy</code>. Or it could be policies that are attached to the OIDC IDP IAM object itself. I haven't thought about which is preferable, but something is needed.</p>"},{"location":"blog/2021/10/12/aws-iam-oidc-idps-need-more-controls.html#claim-to-tag-mappings","title":"Claim-to-tag mappings","text":"<p>Update: This doesn't exist in AWS IAM yet, but it can be achieved via AWS Cognito. I wrote a follow-up blog post that explains how to achieve it.</p> <p>AWS IAM has supported role session tags for a while now. Have you ever looked at how they work for roles bootstraped from OIDC IDPs? Here's what needs to go in the JWT issued by the IDP:</p> <p></p> <p>Do you ever see GitHub Actions supporting this? Or any social network? Or GitLab? Not even AWS EKS vends ID tokens with this claim namespace. I can't think of any use case for this design. If you have that much control over the format of the JWTs, you're probably issuing them yourself. In which case, why not assume a role directly? Anyway, I digress.</p> <p>Request: Give me a way to map arbitrary claims from the OIDC token to role session tags in AWS. The tokens issued by GitHub have some extremely useful information and I want to use it in my policies. I also want those values to appear in CloudTrail. Hell, they should be usable in the above request for trust policy boundaries: maybe roles should only be assumable for  workflows initiated by <code>aidansteele</code>. </p> <p>Again, these could be configured on either a per-role basis or as a property of the OIDC IDP object. I'm erring towards the latter in this case. It's the kind of thing you want consistency in. And again, this would be useful for EKS.</p>"},{"location":"blog/2021/10/12/aws-iam-oidc-idps-need-more-controls.html#wrap-up","title":"Wrap up","text":"<p>AWS is customer-obsessed, but not silly. They're not going to build this functionality just because I asked for it. But they use demand to prioritise work. I would encourage you to file a feature request ticket with the IAM team adding your +1 to the ask. Here's an example that you definitely shouldn't use.</p> <p></p>"},{"location":"blog/2020/12/24/aws-lambda-latest-is-dangerous.html","title":"AWS Lambda $LATEST is dangerous","text":"<p>AWS Lambda has supported function versions since October 2015, only a couple of months after the service itself was publicly launched. Versions are an optional feature - you can develop and use Lambda functions without versioning, in which case you are working with the implicit <code>$LATEST</code> version. </p> <p>It has been my experience that most developers don't use function versions. Some \"evidence\" of this is:</p> <ul> <li>The AWS CDK has a bug that can make working with versions a hassle.</li> <li>The AWS SAM transform has had similar bugs in the past.</li> <li>CloudFormation's <code>AWS::Lambda::Version</code> is too difficult to use on its own   without relying on one of the previous two tools to do it automatically for you.</li> </ul>"},{"location":"blog/2020/12/24/aws-lambda-latest-is-dangerous.html#but-is-not-using-function-versions-a-problem-i-think-so","title":"But is not using function versions a problem? I think so.","text":"<p>Let me explain the problem by way of an example that quite a few developers will find surprising. Look at the following diff of a simple change to fix a typo in an environment variable's name:</p> <pre><code>--- a/cfn.yml\n+++ b/cfn.yml\n@@ -1,20 +1,20 @@\n Transform: AWS::Serverless-2016-10-31\n Resources:\n   Function:\n     Type: AWS::Serverless::Function\n     Properties:\n       Runtime: python3.8\n       Handler: index.handler\n       CodeUri: ./\n       Environment:\n         Variables:\n-          TABLE_NAM: example\n+          TABLE_NAME: example\n       Policies:\n         DynamoDBReadPolicy:\n           TableName: example\n       Events:\n         Api:\n           Type: HttpApi\n Outputs:\n   ApiUrl:\n     Value: !Sub https://${ServerlessHttpApi}.execute-api.${AWS::Region}.amazonaws.com\ndiff --git a/index.py b/index.py\nindex 7f95be8..617824a 100644\n--- a/index.py\n+++ b/index.py\n@@ -1,8 +1,8 @@\n import os\n import boto3\n\n dynamo = boto3.client('dynamodb')\n\n def handler(event, context):\n-    table_name = os.environ[\"TABLE_NAM\"]\n+    table_name = os.environ[\"TABLE_NAME\"]\n     return dynamo.get_item(TableName=table_name, Key={\"PK\": {\"S\": \"key\"}})\n</code></pre> <p>Super simple, right? If I deploy this change on a busy API, would you expect any problems? Remember: the app was working before and it will continue working after the deployment is complete. But could there be any temporary issues?</p> <p>(some time to think)</p> <p>If you anticipated problems, you were right. (Though I suppose this would be a very anti-climactic blog had there been none). Here are the results of a load test (courtesy of the pretty great free tier on loader.io):</p> <p></p> <p>There were 164 errors returned - all during the stack update! (The timeouts were unrelated, because I had set them way too low during this example.) And what was the cause of those errors? Here's what the logs say:</p> <p></p> <p>There were 164 times where the Python code failed to find the <code>TABLE_NAM</code>  environment variable. The only way this could happen is if the function's env vars changed before the code did - and that's exactly what happened.</p> <p>Why does CloudFormation update the env vars and the code separately? Because it doesn't have a choice. The Lambda service exposes two APIs:  <code>UpdateFunctionCode</code> and <code>UpdateFunctionConfiguration</code>. CloudFormation needs to call both these APIs in order to apply the changes we've requested. And no matter which order it calls them in, there's going to be some small window of time when one has taken effect and the other has yet to.</p> <p>This problem is intractable<sup>1</sup> if you are not using function versions.</p>"},{"location":"blog/2020/12/24/aws-lambda-latest-is-dangerous.html#so-how-do-versions-help","title":"So how do versions help?","text":"<p>Function versions are an immutable \"snapshot\" of a function's code and configuration. Creating a version by itself doesn't help, you also need to use those versions. In other words, <code>$LATEST</code> is best always avoided. </p> <p>In the earlier example, using versions is as simple as adding <code>AutoPublishAlias: live</code> to the function resource. This will automatically create a new version whenever the function's code or configuration changes. What's more, the auto-generated API Gateway will automatically use those automatically created versions. (We'll  get to aliases in a moment.)</p> <p>For the AWS CDK, it would instead look a bit like:</p> <pre><code>const fn = new lambda.Function(this, 'Function', {\n    runtime: lambda.Runtime.PYTHON_3_8,\n    handler: 'index.handler',\n    code: lambda.AssetCode.fromAsset('./')\n});\n\n// instead of this:\n// const integration = new int.LambdaProxyIntegration({handler: fn});\n\n// do this:\nconst integration = new int.LambdaProxyIntegration({handler: fn.currentVersion});\n\nconst api = new apigw.HttpApi(this, 'Api', {});\napi.addRoutes({path: '/', integration: integration});\n</code></pre> <p>This will configure the API Gateway to call <code>lambda:Invoke</code> with an ARN of  <code>arn:aws:lambda:&lt;region&gt;:&lt;account id&gt;:function:FunctionName:3</code> instead of one without the <code>:3</code> prefix. That <code>:3</code> auto-increments whenever the function's code or configuration changes. Now you will never see those 5xx errors again!</p> <p>Back to aliases. While the above works, it's a little lame because every code change means updating the API Gateway too - which means waiting an extra second or so in the stack update. Instead we can use aliases. An alias is a way of creating a name that points to a version - and the version it points to can change. Think of the relationship between aliases and versions as similar to the  relationship between branches and commits in Git. If our code instead had these lines:</p> <pre><code>const alias = fn.currentVersion.addAlias('live');\nconst integration = new int.LambdaProxyIntegration({handler: alias});\n</code></pre> <p>then API gateway would invoke <code>arn:aws:lambda:&lt;region&gt;:&lt;account id&gt;:function:FunctionName:live</code>. Now the API Gateway doesn't need to be updated on every code change, plus a few other benefits.</p>"},{"location":"blog/2020/12/24/aws-lambda-latest-is-dangerous.html#summary","title":"Summary","text":"<p>You should always use function versioning. You should almost always<sup>2</sup> use function aliases.  Aliases have a handful of benefits involving metrics in CloudWatch, IAM  permissions, traffic-shifting, etc. that are too big a topic for this post. </p> <p><sup>1</sup> Yes, you could write your code to look for either environment variable in this contrived example - but it can apply to any configuration and code changes. E.g. EFS mounts changing, layers changing, runtime changing, etc. I would argue that coding defensively to prevent this scenario instead of just using versions is an ineffective use of time.</p> <p><sup>2</sup> An example scenario of where you might want to use function versions and not aliases (and not <code>$LATEST</code> either) is in Step Function definitions. If you have version N of a step function mid-execution when you do a deployment, you might not want version N+1 of the Lambda functions in the subsequent states to receive outputs from version N functions. This way you can ensure that the entire step function definition (state machine, Lambdas and all) is immutable for all given executions.</p>"},{"location":"blog/2023/10/25/aws-role-session-tags-for-github-actions.html","title":"AWS role session tags for GitHub Actions","text":"<p>Back in 2021, I requested that AWS add some kind of \"claim-to-tag mapping\" functionality to OIDC IDPs, so that we could have role session tags based on  claims in OIDC tokens issued by GitHub Actions. That hasn't happened yet, but today I learned (thanks to this comment and associated blog post by Daniel Jons\u00e9n) that the same outcome can be achieved by using AWS Cognito  identity pools as an intermediary.</p> <p>Cognito identity pools has functionality that allows claims in an OIDC token to be mapped to role session tags. On a simple level: once you've  configured a dictionary of claim-&gt;tag mappings, you can give Cognito a GitHub  OIDC token and it will return to you a Cognito-issued OIDC token with session  tags. That Cognito OIDC token can then be used with <code>AssumeRoleWithWebIdentity</code>.</p> <p>Here's how it works:</p> <pre><code># note that this cloudformation template assumes you have already created the github actions OIDC IdP in your account\n\nResources:\n  IdentityPool:\n    Type: AWS::Cognito::IdentityPool\n    Properties:\n      IdentityPoolName: gha-tags-example\n      AllowClassicFlow: true # this is needed to allow direct AssumeRoleWithWebIdentity calls\n      AllowUnauthenticatedIdentities: false\n      OpenIdConnectProviderARNs:\n        - !Sub arn:aws:iam::${AWS::AccountId}:oidc-provider/token.actions.githubusercontent.com\n\n  TagMapping:\n    Type: AWS::Cognito::IdentityPoolPrincipalTag\n    Properties:\n      IdentityPoolId: !Ref IdentityPool\n      IdentityProviderName: !Sub arn:aws:iam::${AWS::AccountId}:oidc-provider/token.actions.githubusercontent.com\n      UseDefaults: false\n      PrincipalTags:\n        actor: actor\n        sha: sha\n        run_id: run_id\n        event: event_name # your tag can have a different name than the OIDC claim\n        ref: ref\n        repository: repository\n\n  Role:\n    Type: AWS::IAM::Role\n    Properties:\n      AssumeRolePolicyDocument:\n        # this `Version` line is very important. conditions like the\n        # role session name one below will fail without it\n        Version: \"2012-10-17\"\n        Statement:\n          - Effect: Allow\n            Action: sts:AssumeRoleWithWebIdentity\n            Principal:\n              Federated: cognito-identity.amazonaws.com # note that this is *NOT* the GHA url\n            Condition:\n              StringEquals:\n                # this next condition is what stops cognito in *other* aws accounts from crafting\n                # OIDC tokens for *your* account\n                cognito-identity.amazonaws.com:aud: !Ref IdentityPool\n                # this next condition is just an example of what's now possible. you don't \n                # actually need it, but it's handy for cloudtrail!\n                sts:RoleSessionName: ${aws:RequestTag/run_id}@${aws:RequestTag/sha}\n          - Effect: Allow\n            Action: sts:TagSession\n            Principal:\n              Federated: cognito-identity.amazonaws.com\n            Condition:\n              StringEquals:\n                cognito-identity.amazonaws.com:aud: !Ref IdentityPool\n\nOutputs:\n  IdentityPool:\n    Value: !Ref IdentityPool\n  Role:\n    Value: !GetAtt Role.Arn\n</code></pre> <p>The process is now:</p> <ol> <li>GHA workflow requests an OIDC token from GitHub Actions</li> <li>GHA workflow calls <code>cognito-identity:GetId</code> with original OIDC token and     is returned a Cognito \"identity ID\"</li> <li>GHA workflow calls <code>cognito-identity:GetOpenIdToken</code> with     original OIDC token and is returned a Cognito-issued OIDC token</li> <li>GHA workflow calls <code>sts:AssumeRoleWithWebIdentity</code> with Cognito-issued    OIDC token and IAM role name ARN and is returrned temporary AWS credentials.</li> </ol> <p>Steps 2 and 3 are new compared to the \"standard process\" and step 4 uses the  Cognito-issued OIDC token instead of the GHA-issued OIDC token. Here's what the  entry in CloudTrail looks like:</p> <pre><code>{\n  \"eventVersion\": \"1.08\",\n  \"userIdentity\": {\n    \"type\": \"WebIdentityUser\",\n    \"principalId\": \"cognito-identity.amazonaws.com:ap-southeast-2:14deebd0-19f8-4295-a55e-8b36e60b4926:ap-southeast-2:f33550b0-d103-4ff1-9319-1745fea988da\",\n    \"userName\": \"ap-southeast-2:f33550b0-d103-4ff1-9319-1745fea988da\",\n    \"identityProvider\": \"cognito-identity.amazonaws.com\"\n  },\n  \"eventTime\": \"2023-10-25T01:21:21Z\",\n  \"eventSource\": \"sts.amazonaws.com\",\n  \"eventName\": \"AssumeRoleWithWebIdentity\",\n  \"awsRegion\": \"ap-southeast-2\",\n  \"sourceIPAddress\": \"121.221.159.246\",\n  \"userAgent\": \"aws-cli/2.13.28 Python/3.11.6 Darwin/23.0.0 source/arm64 prompt/off command/sts.assume-role-with-web-identity\",\n  \"requestParameters\": {\n    \"principalTags\": {\n      \"actor\": \"aidansteele\",\n      \"ref\": \"refs/heads/main\",\n      \"run_id\": \"6634485805\",\n      \"event\": \"workflow_dispatch\",\n      \"repository\": \"ak2-au/oidc-token-fetcher\",\n      \"sha\": \"65e4b17a18e0f86c7b608703ec4a8340c3461d01\"\n    },\n    \"roleArn\": \"arn:aws:iam::607481581596:role/gha-tags-test-Role-ZmTOykdCAhxs\",\n    \"roleSessionName\": \"6634485805@65e4b17a18e0f86c7b608703ec4a8340c3461d01\"\n  },\n  \"responseElements\": {\n    \"credentials\": {\n      \"accessKeyId\": \"ASIAY24FZKAOEK7KVHCV\",\n      \"sessionToken\": \"IQo&lt;truncated&gt;f+J+wQ=\",\n      \"expiration\": \"Oct 25, 2023, 2:21:21 AM\"\n    },\n    \"subjectFromWebIdentityToken\": \"ap-southeast-2:f33550b0-d103-4ff1-9319-1745fea988da\",\n    \"assumedRoleUser\": {\n      \"assumedRoleId\": \"AROAY24FZKAOKXOMX4HDD:6634485805@65e4b17a18e0f86c7b608703ec4a8340c3461d01\",\n      \"arn\": \"arn:aws:sts::607481581596:assumed-role/gha-tags-test-Role-ZmTOykdCAhxs/6634485805@65e4b17a18e0f86c7b608703ec4a8340c3461d01\"\n    },\n    \"packedPolicySize\": 43,\n    \"provider\": \"cognito-identity.amazonaws.com\",\n    \"audience\": \"ap-southeast-2:14deebd0-19f8-4295-a55e-8b36e60b4926\"\n  },\n  \"requestID\": \"a282953d-2752-442d-96b3-8d8bff4a73f3\",\n  \"eventID\": \"824092dd-18ba-4a2f-8f94-8ada97a08dd4\",\n  \"readOnly\": true,\n  \"resources\": [\n    {\n      \"accountId\": \"607481581596\",\n      \"type\": \"AWS::IAM::Role\",\n      \"ARN\": \"arn:aws:iam::607481581596:role/gha-tags-test-Role-ZmTOykdCAhxs\"\n    }\n  ],\n  \"eventType\": \"AwsApiCall\",\n  \"managementEvent\": true,\n  \"recipientAccountId\": \"607481581596\",\n  \"eventCategory\": \"Management\",\n  \"tlsDetails\": {\n    \"tlsVersion\": \"TLSv1.2\",\n    \"cipherSuite\": \"ECDHE-RSA-AES128-GCM-SHA256\",\n    \"clientProvidedHostHeader\": \"sts.ap-southeast-2.amazonaws.com\"\n  }\n}\n</code></pre> <p>Daniel has written a GHA action that implements the aforementioned process of requesting and exchanging OIDC tokens. </p>"},{"location":"blog/2023/10/25/aws-role-session-tags-for-github-actions.html#bonus-thoughts","title":"Bonus thoughts","text":"<p>Cognito identity pools has two different authentication \"flows\" that  are relevant to us. I used the \"basic (classic) flow\" above because it means we call <code>AssumeRoleWithWebIdentity</code> directly (rather than Cognito doing it for us in the  \"enhanced\" flow), which allows us to specify a role session name - this is useful for CloudTrail attribution. It also allows us to specify the role ARN in the GHA workflow itself, rather than in the Cognito configuration. This feels like it  will be more familiar to administrators than the enhanced flow.</p>"},{"location":"blog/2021/10/24/aws-sigv4-caching.html","title":"AWS SigV4 caching","text":"<p>Say you find yourself doing silly things with AWS APIs on a lazy Sunday afternoon. And you are getting the following inexplicable error when using perfectly valid credentials:</p> <pre><code>&lt;Error&gt;\n    &lt;Type&gt;Sender&lt;/Type&gt;\n    &lt;Code&gt;SignatureDoesNotMatch&lt;/Code&gt;\n    &lt;Message&gt;The request signature we calculated does not match the signature \n    you provided. Check your AWS Secret Access Key and signing method. Consult \n    the service documentation for details.&lt;/Message&gt;\n  &lt;/Error&gt;\n</code></pre> <p>The solution might be <code>sleep()</code>. Ideally for yourself (the sun is shining and it is Sunday afternoon), but in your code is also acceptable. Or hang up and reconnect.</p>"},{"location":"blog/2021/10/24/aws-sigv4-caching.html#why","title":"Why?","text":"<p>There appears to be a credential cache on the AWS services. Specifically, it looks like:</p> <ul> <li>The cache timeout is 5 seconds</li> <li>It is keyed by access key ID<sup>1</sup> (i.e. <code>AKIA...</code> or <code>ASIA...</code>)</li> <li>Only invalid credentials are cached</li> </ul> <p>So you'll only be hit by this issue if you try a [valid key ID, invalid secret key] pair followed (within 5 seconds) by [same valid key ID, valid secret key].</p> <p>I suppose it's fair enough, because it doesn't affect legitimate usage and it's  a cheap way for AWS to avoid spending too much time processing invalid  credentials - can you imagine all the infinite loops of bad credentials trying  to hammer their APIs all the time?</p> <p><sup>1</sup>: Maybe it's keyed by the entire <code>Credential=AKIA0123456../20211024/us-east-1/sts/aws4_request</code> string, but I'm not going to wait until the stroke of midnight to find out.</p>"},{"location":"blog/2022/02/03/aws-vpc-data-exfiltration-using-codebuild.html","title":"AWS VPC data exfiltration using CodeBuild","text":"<p>UPDATE: This issue has been fixed. See the end of the article for timeline and details.</p> <p>In September 2020, I published a guest blog post on Ian Mckay's blog. The tl;dr is that \"escaping\" a privileged container running in an Amazon-managed AWS account isn't a security concern for Amazon, thanks to defence-in-depth on both an EC2 and IAM level. Here's a few paragraphs I left out of that blog post at the time.</p>"},{"location":"blog/2022/02/03/aws-vpc-data-exfiltration-using-codebuild.html#but-wait-theres-more","title":"But wait, there's more","text":"<p>As Corey said in the tweet at the start of that story, you can grant CodeBuild access to resources in your own VPC. CodeBuild is much like AWS Lambda in that  regard: by default it has public Internet connectivity or you can allow it to attach to your VPC to access internal resources. </p> <p>A tweet by @amcabee13 on her favourite topic of IP addresses for VPC-unattached Lambdas got me thinking: some people attach Lambdas to VPCs  because then it allows more observability and control over data egress, i.e.  connections out to the Internet. The same goes for CodeBuild jobs: traffic will now reach the Internet through whatever egress path you have and the associated  security controls (firewalls, etc). But what about the EC2 instance the CodeBuild container is running on - it's still in the AWS-managed VPC?</p> <p>So naturally, I went back and checked. Running commands in the container had  connectivity to the VPC. And running them on the host had connectivity directly  to the public Internet, unassociated with the customer's (my) VPC. Could this  make for very convenient data exfiltration? The answer is: yes, quite easy! </p> <p>First, run commands like these on the CodeBuild host:</p> <pre><code>ssh-keygen\ncat ~/.ssh/id_rsa.pub &gt; /home/ec2-user/.ssh/authorized_keys\nssh -f -N -D 169.254.170.8:1080 ec2-user@localhost\n</code></pre> <p>Now in the CodeBuild container you can run:</p> <pre><code># this is routed through the customer VPC\ncurl http://10.1.2.3/internal \n\n# this is routed through the AWS VPC, bypassing your VPC egress routing tables\ncurl --socks4a 169.254.170.8:1080 https://google.com  \n</code></pre> <p>Pay careful attention to the <code>a</code> at the end of that <code>--socks4a</code> flag. If you  instead pass <code>--socks4</code> everything will work, but the DNS resolution of <code>google.com</code>  will be done by the container and your malicious DNS queries could be recorded  by Route 53 Resolver Query Logs in the customer VPC, not the AWS VPC.</p>"},{"location":"blog/2022/02/03/aws-vpc-data-exfiltration-using-codebuild.html#what-would-be-better","title":"What would be better?","text":"<p>The CodeBuild host EC2 instance seems to only need Internet connectivity in  order to connect to <code>codebuild.{region}.amazonaws.com</code> and associated services. Instead of using a NAT gateway, perhaps the AWS-managed VPC could use interface VPC endpoints. That would eliminate the issue entirely.</p> <p>That said, do I consider this a huge issue? Not really. CodeBuild builds are logged in CloudTrail and CodeBuild itself, so forensics are easy enough. It's unlikely that this is going to be a problem in reality. But I found it fun and worth sharing.</p>"},{"location":"blog/2022/02/03/aws-vpc-data-exfiltration-using-codebuild.html#update","title":"Update","text":"<p>I reported this to the AWS Security team on February 4<sup>th</sup>. My initial email was  acknowledged by a human within 6 hours. On the 24<sup>th</sup> of February I got an update  with this (emphasis my own):</p> <p>CodeBuild executes customer builds in containers which run on single-tenanted  Amazon EC2 instances within a service-managed VPC.  When customers configure  a CodeBuild project with their VPC, CodeBuild's build container will apply  the same network routing rules as defined in the customer's VPC Security Group.  We have updated the CodeBuild service to block all outbound network access  for newly created CodeBuild projects, which contain a customer-defined VPC  configuration. </p> <p>CodeBuild's security isolation is enforced at the AWS account level and EC2  instances are never shared across AWS accounts. CodeBuild also logs all build  requests in CloudTrail for auditing usage within an AWS account.</p> <p>I verified that the issue in this article is no longer applicable for new CodeBuild projects. <code>codebuild.{region}.amazonaws.com</code> and other host-level dependencies use VPC endpoints and there is no connectivity to the public Internet for VPC-attached projects.</p> <p>Honestly, I'm delighted with the outcome. Making this kind of change to a service as well-established as AWS CodeBuild likely isn't straight forward, but the AWS security team and the CodeBuild team managed to identify, develop and roll out  the ideal fix in fewer than three weeks. Kudos to them.</p> <p>That said, I did notice this new caveat appear in the CodeBuild VPC docs and I feel bad for anyone whose builds are slower now! Hopefully it's only a minor increase.</p> <p></p>"},{"location":"blog/2022/12/16/centralised-logging-from-cloudwatch-to-kinesis-firehose.html","title":"Centralised logging: from CloudWatch to Kinesis Firehose","text":"<p>AWS CloudWatch Logs supports automatic forwarding of logs to AWS Kinesis Data Streams and AWS Kinesis Data Firehose. These destinations are can even be in a different AWS account and region. This is very handy for aggregating logs from thousands of log groups and forwarding them to a single place, like Axiom, Datadog, Splunk, etc.</p> <p>The only problem is that the docs are (to me, anyway) very confusing. They're also very long. Look at this!</p> <p></p> <p>So this post is an attempt to explain it in my own words for a) me in the future  when I forget all this and b) others who might stumble across it. I'm going to focus on Kinesis Data Firehose delivery streams as that is more interesting to me, but the same applies to Kinesis Data Streams.</p>"},{"location":"blog/2022/12/16/centralised-logging-from-cloudwatch-to-kinesis-firehose.html#centralised-logging","title":"Centralised logging","text":"<p>Here is a diagram of the relevant parts. I've made it multiple region and multiple account to demonstrate the most complex configuration. It also works in single region configurations too.</p> <p></p> <p>First, we'll start with the straightforward part: a template that deploys the delivery stream, a role for the delivery stream and a backup bucket. This only needs to be deployed to a single region in the logs destination account.</p> <pre><code>Parameters:\n  DeliveryStreamName:\n    Type: String\n    Default: centralised-logs\n  APIKey:\n    Type: String\n    NoEcho: true\n\nResources:\n  Firehose:\n    Type: AWS::KinesisFirehose::DeliveryStream\n    Properties:\n      DeliveryStreamName: !Ref DeliveryStreamName\n      DeliveryStreamType: DirectPut\n      HttpEndpointDestinationConfiguration:\n        RoleARN: !GetAtt FirehoseRole.Arn\n        BufferingHints:\n          IntervalInSeconds: 60\n          SizeInMBs: 4\n        EndpointConfiguration:\n          Name: example\n          Url: https://example.com\n          AccessKey: !Ref APIKey\n        RequestConfiguration:\n          ContentEncoding: GZIP\n        RetryOptions:\n          DurationInSeconds: 60\n        S3BackupMode: AllData\n        S3Configuration:\n          BucketARN: !Sub arn:aws:s3:::${Bucket}\n          RoleARN: !GetAtt FirehoseRole.Arn\n          Prefix: logs/!{timestamp:yyyy/MM/dd}/\n          ErrorOutputPrefix: errors/!{firehose:error-output-type}/!{timestamp:yyyy/MM/dd}/\n          CompressionFormat: GZIP\n          BufferingHints:\n            IntervalInSeconds: 60\n            SizeInMBs: 128\n\n  FirehoseRole:\n    Type: AWS::IAM::Role\n    Properties:\n      AssumeRolePolicyDocument:\n        Version: \"2012-10-17\"\n        Statement:\n          - Effect: Allow\n            Principal:\n              Service: firehose.amazonaws.com\n            Action: sts:AssumeRole\n            Condition:\n              StringEquals:\n                sts:ExternalId: !Ref AWS::AccountId\n      Policies:\n        - PolicyName: Firehose\n          PolicyDocument:\n            Version: \"2012-10-17\"\n            Statement:\n              - Effect: Allow\n                Action:\n                  - s3:AbortMultipartUpload\n                  - s3:GetBucketLocation\n                  - s3:GetObject\n                  - s3:ListBucket\n                  - s3:ListBucketMultipartUploads\n                  - s3:PutObject\n                Resource:\n                  - !Sub arn:aws:s3:::${Bucket}\n                  - !Sub arn:aws:s3:::${Bucket}/*\n\n  Bucket:\n    DeletionPolicy: Retain\n    Type: AWS::S3::Bucket\n\nOutputs:\n  FirehoseArn:\n    Value: !GetAtt Firehose.Arn        \n</code></pre> <p>Delivery streams do not have resource-based policies, so by themselves they have no way of permitting cross-account publishing. To resolve this, CloudWatch introduces the concept of \"logical destinations\". These are abstractions that  wrap a delivery stream (or Kinesis data stream) and add a) a role that the destination itself assumes when publishing to the delivery stream and b) a resource-based destination policy that specifies who is permitted to forward logs to this destination. </p> <p>CWL log groups can only forward logs to a CWL destination in the same region, but a CWL destination can publish those logs to a delivery stream in a different region. Therefore, my preferred pattern is to deploy this CloudFormation template to the \"logs destination\" account in every region my org uses.</p> <pre><code># this line lets us use `Fn::ToJsonString` below\nTransform: AWS::LanguageExtensions\n\nParameters:\n  FirehoseArn:\n    Type: String\n    Default: arn:aws:firehose:us-east-1:0123456789012:deliverystream/centralised-logs\n  OrgId:\n    Type: String\n    Default: o-abc123\n\nResources:\n  Destination:\n    Type: AWS::Logs::Destination\n    Properties:\n      DestinationName: CentralisedLogs\n      RoleArn: !GetAtt DestinationRole.Arn\n      TargetArn: !Ref FirehoseArn\n      DestinationPolicy:\n        Fn::ToJsonString:\n          Version: '2012-10-17'\n          Statement:\n            - Sid: AllowMyOrg\n              Effect: Allow\n              Principal: \"*\"\n              Action: logs:PutSubscriptionFilter\n              Resource: !Sub arn:aws:logs:${AWS::Region}:${AWS::AccountId}:destination:CentralisedLogs\n              Condition:\n                StringEquals:\n                  aws:PrincipalOrgID: !Ref OrgId\n\n  DestinationRole:\n    Type: AWS::IAM::Role\n    Properties:\n      AssumeRolePolicyDocument:\n        Version: \"2012-10-17\"\n        Statement:\n          - Effect: Allow\n            Principal:\n              Service: logs.amazonaws.com\n            Action: sts:AssumeRole\n      Policies:\n        - PolicyName: Firehose\n          PolicyDocument:\n            Version: \"2012-10-17\"\n            Statement:\n              - Effect: Allow\n                Resource: !Ref FirehoseArn\n                Action: firehose:PutRecord*\n                # the AWS docs say to grant \"firehose:*\" -- how weird is that?\n\nOutputs:\n  DestinationArn:\n    Value: !GetAtt Destination.Arn\n</code></pre> <p>With that deployed, our logs destination account is completely setup. We can move  onto the source accounts. In those accounts we deploy this template:</p> <pre><code>Parameters:\n  LogPusherRoleName:\n    Type: String\n    Default: CentralisedLogsPusher\n\nResources:\n  LogPusherRole:\n    Type: AWS::IAM::Role\n    Properties:\n      RoleName: !Ref LogPusherRoleName\n      AssumeRolePolicyDocument:\n        Version: \"2012-10-17\"\n        Statement:\n          - Effect: Allow\n            Action: sts:AssumeRole\n            Principal:\n              Service: logs.amazonaws.com\n      Policies:\n        - PolicyName: PutLogEvents\n          PolicyDocument:\n            Version: \"2012-10-17\"\n            Statement:\n              - Effect: Allow\n                Action: logs:PutLogEvents\n                Resource: \"*\"\n                # why does it need this permission? not sure. it's what the docs \n                # say and i haven't yet tested if it works without it. \n                # https://docs.aws.amazon.com/AmazonCloudWatch/latest/logs/CreateSubscriptionFilter-IAMrole.html\n\nOutputs:\n  LogPusherRoleArn:\n    Value: !GetAtt LogPusherRole.Arn                \n</code></pre> <p>Now you can finally create a subscription filter that forwards logs for a given log group to that centralised delivery stream (via a CWL destination). Here's how to do that:</p> <pre><code>aws logs put-subscription-filter \\\n  --log-group-name /aws/lambda/example-log-group \\\n  --filter-name CentralisedLogging \\\n  --filter-pattern '' \\\n  --destination-arn arn:aws:logs:ap-southeast-2:0123456789012:destination:CentralisedLogs \\\n  --role-arn arn:aws:iam::987654321012:role/CentralisedLogsPusher\n</code></pre> <p>Note that the principal calling <code>logs:PutSubscriptionFilter</code> will need permission to call that action on the given log group and <code>iam:PassRole</code> on the log pusher role ARN.</p>"},{"location":"blog/2021/10/17/cgo-for-arm64-lambda-functions.html","title":"cgo for ARM64 Lambda Functions","text":"<p>In my post Graviton2: ARM comes to Lambda I showed that it is very easy to cross-compile Go code to run on ARM64 AWS Lambda functions. That's true as long as your code is 100% Go - as soon as there's any C code involved it feels impossible. I mean, who wants to deal with this error message?</p> <p></p> <p>It turns out there's a solution: to use Zig to compile and link your C code. Zig is an extremely interesting new programming language that seeks to replace C. It also happens to be able to compile C via <code>zig cc</code>. It can also cross-compile C. Check out this amazing blog post by its creator. Anyway, here's how to do it:</p>"},{"location":"blog/2021/10/17/cgo-for-arm64-lambda-functions.html#sqlite-and-go-on-arm64-lambda-functions-via-zig","title":"SQLite and Go on ARM64 Lambda functions via Zig","text":"<p>Here's a useless Lambda function:</p> <pre><code>package main\n\nimport (\n    \"context\"\n    \"crawshaw.io/sqlite/sqlitex\"\n    \"encoding/json\"\n    \"github.com/aws/aws-lambda-go/lambda\"\n)\n\nvar dbpool *sqlitex.Pool\n\nfunc main() {\n    dbpool, _ = sqlitex.Open(\"file:memory:?mode=memory\", 0, 1)\n    lambda.Start(handle)\n}\n\nfunc handle(ctx context.Context, input json.RawMessage) (string, error) {\n    conn := dbpool.Get(ctx)\n    defer dbpool.Put(conn)\n\n    // please excuse my complete lack of error handling\n    stmt, _, _ := conn.PrepareTransient(\"SELECT 123\")\n    defer stmt.Finalize()\n\n    stmt.Step()\n    return stmt.ColumnText(0), nil\n}\n</code></pre> <p>And here's how you can compile it for ARM64 (and x86_64) from your laptop:</p> <pre><code># build.sh\nset -eux\nexport GOOS=linux\nexport CGO_ENABLED=1\nexport CC=$(pwd)/zcc.sh\nexport CXX=$(pwd)/zxx.sh\n\nGOARCH=arm64 \\\nZTARGET=aarch64-linux-musl \\\ngo build -ldflags=\"-linkmode external\" -o arm64/bootstrap\n\nGOARCH=amd64 \\\nZTARGET=x86_64-linux-musl \\\ngo build -ldflags=\"-linkmode external\" -o amd64/bootstrap\n</code></pre> <p>Here's what those <code>zcc.sh</code> and <code>zxx.sh</code> files should look like:</p> <pre><code># zcc.sh\n#!/bin/sh\nset -eu\nexport ZIG_SYSTEM_LINKER_HACK=1\nexport ZIG_LOCAL_CACHE_DIR=\"$HOME/.zigcache/\"\nzig cc -target $ZTARGET \"$@\"\n\n# zxx.sh\n#!/bin/sh\nset -eu\nexport ZIG_SYSTEM_LINKER_HACK=1\nexport ZIG_LOCAL_CACHE_DIR=\"$HOME/.zigcache/\"\nzig c++ -target $ZTARGET \"$@\"\n</code></pre> <p>Finally, for completeness, a potential CloudFormation template for your functions:</p> <pre><code>Transform: AWS::Serverless-2016-10-31\n\nResources:\n  Arm64:\n    Type: AWS::Serverless::Function\n    Properties:\n      Handler: hello\n      CodeUri: ./arm64/bootstrap\n      Runtime: provided.al2\n      Architectures: [arm64]\n\n  Amd64:\n    Type: AWS::Serverless::Function\n    Properties:\n      Handler: hello\n      CodeUri: ./amd64/bootstrap\n      Runtime: provided.al2\n      Architectures: [x86_64]\n</code></pre>"},{"location":"blog/2022/10/15/cheap-serverless-containers-using-api-gateway.html","title":"Cheap serverless containers using API Gateway","text":"<p>Sometimes I need to run a long-lived app. In those cases I reach for AWS ECS Fargate instead of AWS Lambda. You can run a container on Fargate for as little  as $9/month, or $2.70/month if you're happy to roll the dice with Fargate Spot (I usually do!)</p> <p>If you have a web app, you almost certainly use a load balancer in front of your containers. And this is where the cost goes from \"fun side project\" to \"oh, I'm not sure I'm willing to spend that much money on this.\" The load balancer by  itself is at least $16.40/month - you could run six containers for that price!</p>"},{"location":"blog/2022/10/15/cheap-serverless-containers-using-api-gateway.html#no-need-for-load-balancers","title":"No need for load balancers","text":"<p>You can forego the ALB entirely - and still get TLS termination and balancing load over multiple containers. You just need to use API Gateway HTTP API's support for private integrations. These allow you to specify the origin behind the API Gateway as a HTTP endpoint inside a VPC, rather than the typical Lambda function ARN.</p> <p>Instead of $16.40+/month you pay only $1 per million requests. For the traffic volumes that my hobby projects receive, that's a huge saving. </p>"},{"location":"blog/2022/10/15/cheap-serverless-containers-using-api-gateway.html#deployable-example","title":"Deployable example","text":"<p>Here's a complete deployable example. There are two templates.</p> <p>The first template is the base infrastructure. You would deploy this once into your account, and it can be shared across the many web apps you will deploy at example.com. It contains:</p> <ul> <li>A VPC and its associated subnets, route tables, etc.</li> <li>A Route 53 hosted zone for your DNS records</li> <li>An ACM-managed TLS certificate (used by API Gateway later)</li> <li>An API Gateway VPC Link and its security group. This is how API GW \"reaches in\"   to the container running in your VPC.</li> <li>A Cloud Map namespace. </li> </ul> <p>The second template contains everything specific to a single application hosted on example.com. You would deploy multiple stacks from this template, one for each serverless app you have developed. It contains:</p> <ul> <li>An ECS task definition</li> <li>IAM roles for your ECS task definition</li> <li>A CloudMap service. This holds the IP addresses of your running containers</li> <li>An ECS service. This runs one copy of your task and registers/deregisters   Fargate IPs with the CloudMap service when tasks start and stop.</li> <li>A security group for your ECS service that only allows the VPC link to make   requests to it.</li> <li>An API gateway that forwards all requests to the CloudMap service via the   VPC link.</li> <li>An API Gateway API mapping and Route 53 record to make your API accessible   at my-app.example.com.</li> </ul> <p>Note that the ECS task definition contains a health check. This is because API Gateway itself doesn't perform health checks like an ALB would - it's up to you to tell ECS how it should check the health of your container. Here I have chosen to have ECS run <code>curl</code> inside the container.</p> <pre><code># vpc-infra.yml\nResources:\n  HostedZone:\n    Type: AWS::Route53::HostedZone\n    Properties:\n      Name: example.com\n\n  Certificate:\n    Type: AWS::CertificateManager::Certificate\n    Properties:\n      DomainName: example.com\n      ValidationMethod: DNS\n      SubjectAlternativeNames:\n        - \"*.example.com\"\n      DomainValidationOptions:\n        - DomainName: example.com\n          HostedZoneId: !Ref HostedZone\n\n  CloudMapNamespace:\n    Type: AWS::ServiceDiscovery::PrivateDnsNamespace\n    Properties:\n      Vpc: !Ref Vpc\n      Name: example\n\n  VpcLink:\n    Type: AWS::ApiGatewayV2::VpcLink\n    Properties:\n      Name: vpclink\n      SecurityGroupIds:\n        - !Ref VpcLinkSecurityGroup\n      SubnetIds:\n        - !Ref SubnetA\n        - !Ref SubnetB\n\n  VpcLinkSecurityGroup:\n    Type: AWS::EC2::SecurityGroup\n    Properties:\n      GroupDescription: vpc link\n      VpcId: !Ref VpcId\n      SecurityGroupIngress: []\n\n  Vpc:\n    Type: AWS::EC2::VPC\n    Properties:\n      EnableDnsHostnames: true\n      EnableDnsSupport: true\n      CidrBlock: 10.1.0.0/16\n\n  SubnetA:\n    Type: AWS::EC2::Subnet\n    Properties:\n      VpcId: !Ref Vpc\n      CidrBlock: 10.1.1.0/24\n      AvailabilityZone: !Sub ${AWS::Region}a\n\n  SubnetB:\n    Type: AWS::EC2::Subnet\n    Properties:\n      VpcId: !Ref Vpc\n      CidrBlock: 10.1.2.0/24\n      AvailabilityZone: !Sub ${AWS::Region}b\n\n  InternetGateway:\n    Type: AWS::EC2::InternetGateway\n\n  GatewayAttachment:\n    Type: AWS::EC2::VPCGatewayAttachment\n    Properties:\n      VpcId: !Ref Vpc\n      InternetGatewayId: !Ref InternetGateway\n\n  RouteTable:\n    Type: AWS::EC2::RouteTable\n    Properties:\n      VpcId: !Ref Vpc\n\n  InternetRoute:\n    Type: AWS::EC2::Route\n    DependsOn: GatewayAttachment\n    Properties:\n      GatewayId: !Ref InternetGateway\n      RouteTableId: !Ref RouteTable\n      DestinationCidrBlock: 0.0.0.0/0\n\n  RouteTableAssociationSubnetA:\n    Type: AWS::EC2::SubnetRouteTableAssociation\n    Properties:\n      SubnetId: !Ref SubnetA\n      RouteTableId: !Ref RouteTable\n\n  RouteTableAssociationSubnetB:\n    Type: AWS::EC2::SubnetRouteTableAssociation\n    Properties:\n      SubnetId: !Ref SubnetB\n      RouteTableId: !Ref RouteTable\n\nOutputs:\n  HostedZone:\n    Value: !Ref HostedZone\n    Export:\n      Name: HostedZoneId\n  Certificate:\n    Value: !Ref Certificate\n    Export:\n      Name: Certificate\n  CloudMapNamespace:\n    Value: !Ref CloudMapNamespace\n    Export:\n      Name: CloudMapNamespace      \n  VpcLink:\n    Value: !Ref VpcLink\n    Export:\n      Name: VpcLink\n  VpcLinkSecurityGroup:\n    Value: !Ref VpcLinkSecurityGroup\n    Export:\n      Name: VpcLinkSecurityGroup\n  Vpc:\n    Value: !Ref Vpc\n    Export:\n      Name: VpcId\n  SubnetA:\n    Value: !Ref SubnetA\n    Export:\n      Name: SubnetA\n  SubnetB:\n    Value: !Ref SubnetB\n    Export:\n      Name: SubnetB  \n</code></pre> <pre><code># cheap-container-app.yml\nParameters:\n  Image:\n    Type: String\n    Default: nginx\n\nResources:\n  Service:\n    Type: AWS::ECS::Service\n    Properties:\n      ServiceName: my-app\n      TaskDefinition: !Ref TaskDefinition\n      DesiredCount: 1\n      ServiceRegistries:\n        - RegistryArn: !GetAtt CloudMapService.Arn\n          Port: 80\n      NetworkConfiguration:\n        AwsvpcConfiguration:\n          AssignPublicIp: ENABLED\n          Subnets:\n            - !ImportValue SubnetA\n            - !ImportValue SubnetB\n          SecurityGroups:\n            - !Ref FargateSecurityGroup\n\n  FargateSecurityGroup:\n    Type: AWS::EC2::SecurityGroup\n    Properties:\n      GroupDescription: my app\n      VpcId: !ImportValue VpcId\n      SecurityGroupIngress:\n        - SourceSecurityGroupId: !ImportValue VpcLinkSecurityGroup\n          FromPort: 80\n          ToPort: 80\n          IpProtocol: tcp\n\n  TaskDefinition:\n    Type: AWS::ECS::TaskDefinition\n    Properties:\n      Family: my-app\n      Volumes: []\n      Cpu: 256\n      Memory: 512\n      NetworkMode: awsvpc\n      TaskRoleArn: !Ref TaskRole\n      ExecutionRoleArn: !Ref ExecutionRole\n      ContainerDefinitions:\n        - Name: main\n          Image: !Ref Image\n          HealthCheck:\n            Command:\n              - CMD-SHELL\n              - curl --fail http://127.0.0.1\n          PortMappings:\n            - ContainerPort: 80\n              Protocol: tcp\n\n  CloudMapService:\n    Type: AWS::ServiceDiscovery::Service\n    Properties:\n      NamespaceId: !ImportValue CloudMapNamespace\n      Name: my-app.example\n      DnsConfig:\n        DnsRecords:\n          - Type: SRV\n            TTL: 60\n      HealthCheckCustomConfig:\n        FailureThreshold: 1\n\n  TaskRole:\n    Type: AWS::IAM::Role\n    Properties:\n      AssumeRolePolicyDocument:\n        Statement:\n          - Effect: Allow\n            Principal:\n              Service: [ecs-tasks.amazonaws.com]\n            Action: sts:AssumeRole\n\n  ExecutionRole:\n    Type: AWS::IAM::Role\n    Properties:\n      AssumeRolePolicyDocument:\n        Statement:\n          - Effect: Allow\n            Action: sts:AssumeRole\n            Principal:\n              Service: [ecs-tasks.amazonaws.com]\n      ManagedPolicyArns:\n        - arn:aws:iam::aws:policy/service-role/AmazonECSTaskExecutionRolePolicy\n\n  ApiGateway:\n    Type: AWS::ApiGatewayV2::Api\n    Properties:\n      ProtocolType: HTTP\n      Name: my-app\n\n  Integration:\n    Type: AWS::ApiGatewayV2::Integration\n    Properties:\n      ApiId: !Ref ApiGateway\n      ConnectionId: !ImportValue VpcLink\n      ConnectionType: VPC_LINK\n      IntegrationMethod: ANY\n      IntegrationType: HTTP_PROXY\n      IntegrationUri: !GetAtt CloudMapService.Arn\n      PayloadFormatVersion: 1.0\n\n  Stage:\n    Type: AWS::ApiGatewayV2::Stage\n    Properties:\n      ApiId: !Ref ApiGateway\n      StageName: $default\n      AutoDeploy: true\n\n  Route:\n    Type: AWS::ApiGatewayV2::Route\n    Properties:\n      ApiId: !Ref ApiGateway\n      RouteKey: $default\n      Target: !Sub integrations/${Integration}\n\n  GatewayDomain:\n    Type: AWS::ApiGatewayV2::DomainName\n    Properties:\n      DomainName: my-app.example.com\n      DomainNameConfigurations:\n        - EndpointType: REGIONAL\n          CertificateArn: !ImportValue Certificate\n\n  GatewayMapping:\n    Type: AWS::ApiGatewayV2::ApiMapping\n    Properties:\n      ApiId: !Ref ApiGateway\n      DomainName: !Ref GatewayDomain\n      Stage: !Ref Stage\n\n  Record:\n    Type: AWS::Route53::RecordSet\n    Properties:\n      HostedZoneId: !ImportValue HostedZoneId\n      Name: my-app.example.com\n      Type: A\n      AliasTarget:\n        DNSName: !GetAtt GatewayDomain.RegionalDomainName\n        HostedZoneId: !GetAtt GatewayDomain.RegionalHostedZoneId\n</code></pre>"},{"location":"blog/2022/10/15/cheap-serverless-containers-using-api-gateway.html#thoughts","title":"Thoughts","text":"<p>After writing out those templates, I get the feeling that this could be the kind of thing that would be a useful AWS CDK module, for the CDK-inclined. If anyone wants to build it, I'd be happy to update this post with a link to it.</p>"},{"location":"blog/2022/10/16/cloudfront-and-lambda-function-urls.html","title":"CloudFront and Lambda function URLs","text":"<p>In April 2022, AWS Lambda announced the launch of function URLs - a way to invoke websites powered by Lambda functions without needing API Gateway. A common complaint was the lack of support for custom domains: it only supported the URLs it would generate that look like <code>lprqaxgvt4f6ab3dbj3ixftr640uzgie.lambda-url.ap-southeast-2.on.aws</code>.</p> <p>But that's where CloudFront comes in useful. Not only can it provide us with  custom domain functionality, but we get caching, WAF support, etc as well. Here's a template I've been using for my apps:</p> <pre><code>Transform: AWS::Serverless-2016-10-31\n\nParameters:\n  CertificateArn:\n    Type: String\n  HostedZoneId:\n    Type: String\n  Domain:\n    Type: String\n\nResources:\n  Function:\n    Type: AWS::Serverless::Function\n    Properties:\n      CodeUri: ./webapp\n      AutoPublishAlias: live\n      MemorySize: 1024\n      Timeout: 30\n      Architectures: [arm64]\n      Runtime: provided.al2\n      Handler: unused\n      FunctionUrlConfig:\n        AuthType: AWS_IAM\n\n  CloudFront:\n    Type: AWS::CloudFront::Distribution\n    Properties:\n      DistributionConfig:\n        Enabled: true\n        HttpVersion: http2and3\n        Aliases:\n          - !Ref Domain\n        ViewerCertificate:\n          AcmCertificateArn: !Ref CertificateArn\n          MinimumProtocolVersion: TLSv1.2_2021\n          SslSupportMethod: sni-only\n        DefaultCacheBehavior:\n          ViewerProtocolPolicy: redirect-to-https\n          Compress: true\n          CachePolicyId: 658327ea-f89d-4fab-a63d-7e88639e58f6 # Managed-CachingOptimized\n          OriginRequestPolicyId: 59781a5b-3903-41f3-afcb-af62929ccde1 # Managed-CORS-CustomOrigin\n          TargetOriginId: web\n        Origins:\n          - Id: web\n            DomainName: !Select [2, !Split [\"/\", !GetAtt FunctionUrl.FunctionUrl]]\n            OriginAccessControlId: !Ref OAC\n            CustomOriginConfig:\n              OriginProtocolPolicy: https-only\n              OriginSSLProtocols: [TLSv1.2]\n            OriginShield:\n              Enabled: true\n              OriginShieldRegion: !Ref AWS::Region\n\n  Permission:\n    Type: AWS::Lambda::Permission\n    Properties:\n      Action: lambda:InvokeFunctionUrl\n      FunctionName: !Ref Function.Alias\n      FunctionUrlAuthType: AWS_IAM\n      Principal: cloudfront.amazonaws.com\n      SourceArn: !Sub arn:aws:cloudfront::${AWS::AccountId}:distribution/${CloudFront}\n\n  OAC:\n    Type: AWS::CloudFront::OriginAccessControl\n    Properties:\n      OriginAccessControlConfig:\n        Name: !Ref AWS::StackName\n        OriginAccessControlOriginType: lambda\n        SigningBehavior: always\n        SigningProtocol: sigv4\n\n  Record:\n    Type: AWS::Route53::RecordSet\n    Properties:\n      HostedZoneId: !Ref HostedZoneId\n      Name: !Ref Domain\n      Type: A\n      AliasTarget:\n        DNSName: !GetAtt CloudFront.DomainName\n        HostedZoneId: Z2FDTNDATAQYW2 # this is documented as the cloudfront hosted zone id\n</code></pre> <p>Update 24/07/2024: CloudFront added support for AWS IAM authentication when connecting to Lambda function URLs. I have updated the template to use that. This means that even if an attacker discovers your Lambda function URL, they can't bypass CloudFront - this is useful if you rely on WAF protections.</p>"},{"location":"blog/2025/02/10/cloudfront-triggered-s3-data-event-formats.html","title":"CloudFront-triggered S3 data event formats","text":"<p>There are several ways that CloudFront can be configured with an S3 origin. There are functionality differences between them, but the focus in this blog post is how activity is represented in CloudTrail, specifically the differences in S3 data-level events for each CloudFront option. Those options in CloudFront are  (in decreasing order of desirability):</p> <ol> <li>S3 origin with an Origin Access Control (OAC) configuration. OACs were launched    in 2022 and support all AWS regions, SSE-KMS-encrypted objects and PUT/DELETE    HTTP verbs. </li> <li>S3 origin with an Origin Access Identity (OAI) configuration. OAIs were     launched in 2009<sup>1</sup> and prior to OACs were the only way to serve public from    an S3 bucket without also making the S3 bucket directly accessible to the     world.</li> <li>S3 origin with public access. This means making your bucket accessible to the    world and hoping no one finds your bucket name and bypasses your CDN/firewall.</li> <li>Custom origin pointed at an S3 bucket with static website hosting enabled.</li> </ol> <p>The format of <code>s3:GetObject</code> events in CloudTrail will depend on which one of these options you choose. I couldn't find any examples of these events online, so I tried out each option and published them in this gist. I  \"normalised\" each event to reduce spurious differences - the structure is what  I care about.</p> <p>A summary of the differences that I found interesting:</p> <p>a) When using an OAC, the <code>sourceIPAddress</code> and <code>userAgent</code> are both rewritten to <code>cloudfront.amazonaws.com</code>. In all other scenarios the source IP address is a random CloudFront public IP and the user-agent is <code>[Amazon CloudFront]</code>.</p> <p>b) The <code>userIdentity</code> field looks like this for OAC vs OAI vs public access:</p> <pre><code>// OAC\n{\n  \"invokedBy\": \"cloudfront.amazonaws.com\",\n  \"type\": \"AWSService\"\n}\n\n// OAI\n{\n  \"accountId\": \"CloudFront\",\n  \"principalId\": \"AIDAIWBU3NBABM5FLVT5E\",\n  \"type\": \"AWSAccount\"\n}\n\n// Public\n{\n  \"accountId\": \"anonymous\",\n  \"principalId\": \"\",\n  \"type\": \"AWSAccount\"\n}\n</code></pre> <p>The format of the event makes most sense when using OAC, but it feels like a slight regression compared to using an OAI: there is no way to determine from CloudTrail which OAC was used to access the bucket<sup>2</sup>. Feature request for Amazon: it would be great if the ARN of the OAC appeared here, ideally in the new-ish <code>userIdentity.inScopeOf.sourceArn</code> field. </p> <p>c) When using an OAC, the <code>tlsDetails</code> field is omitted entirely. We probably don't need this level of implementation detail anyway, but we can still confirm that HTTPS is used thanks to the <code>additionalEventData.CipherSuite</code> field.</p> <ol> <li> <p>I learned this thanks to this very detailed timeline.\u00a0\u21a9</p> </li> <li> <p>You can determine which OAI was used by pasting the <code>userIdentity.principalId</code> field (i.e. the one beginning <code>AIDA...</code>) into my website.\u00a0\u21a9</p> </li> </ol>"},{"location":"blog/2025/05/07/cloudtrail-wish-almost-granted.html","title":"CloudTrail wish: almost granted","text":"<p>Back in November last year, I wished for the ability to filter  CloudTrail data events by the requesting principal's ARN. Two days later, my wish was almost granted: CloudTrail launched the ability to filter on <code>userIdentity.arn</code> for CloudTrail lakes but not trails. And now it seems my wish has almost completely come true: the functionality has rolled out to trails as well.</p> <p>I say \"almost\" because it comes with a few caveats. I wished for a way to filter on the principal's ARN, i.e. the field at <code>userIdentity.sessionContext.sessionIssuer.arn</code>. We can only filter on <code>userIdentity.arn</code>. These are mostly similar, but the latter drops the path (if any exists) on the IAM user/role. That's a shame if you've neatly categorised all your human-assumable IAM roles under a <code>/human/</code> path and hoped to only log their access to DynamoDB, S3, etc. Instead you have to hope that you are also using a pattern on the role name and match based on that.</p> <p>That leads me to the second caveat: you can match on string prefixes and/or  suffixes and use negation, but there is no pattern-matching. If you are using an organization-level trail and want to log all DynamoDB writes by a role called <code>admin</code>, I can't see how to do that. You can't match on <code>arn:aws:sts::*:assumed-role/admin</code>. You can't use a suffix match on <code>assumed-role/admin</code> because the <code>userIdentity.arn</code> field also includes the role session name (which is often an email address, a timestamp or a random string). That makes it signficantly less useful if you have a lot of AWS accounts.</p> <p>The CloudTrail console has an example Exclude AWS service-initiated events  event selector, and it generates the following JSON. Notably the negated prefix match has a role session ARN without an account ID. I tried following this pattern and it didn't work. An event selector with an account ID did match.</p> <p></p> <p>I hope I'm missing something here, because this limits the utility of the new functionality in my eyes. Please reach out if so!</p>"},{"location":"blog/2024/11/09/cloudtrail-wishlist.html","title":"CloudTrail wishlist: filtering by principal ARN","text":"<p>UPDATE: My dream came true - almost. See follow-up post.</p> <p>AWS re:Invent 2024 is fast approaching and there's usually a flurry of exciting new services and features for existing services launched around this time each year. I'll be there in person this year - come say hello if you are too!</p> <p>AWS CloudTrail is one of the services I rely on most. I probably run SQL queries against CloudTrail events at least ten times a week. It's useful for so much  more than just auditing: when a developer writes on Slack \"my app can't access an object in bucket xyz\", I usually jump into CloudTrail to find out what AWS account they're using, which IAM role in that account, what API are they using, etc. It's great.</p> <p>There's only one problem with CloudTrail: it can get expensive when you turn on all the functionality. Data events (i.e. things like reading objects in S3, DynamoDB, etc) cost $1 per million events - that's (significantly) more expensive than  the actual API call itself. This makes it hard to justify enabling data-level events when 99% of events are \"boring\" (i.e. applications doing what they're designed to do). That's where my wishlist for a re:Invent launch comes in:</p> <p>I wish I could specify an advanced event selector filter on my trails to  the effect of \"only log (S3|DynamoDB|Kinesis|etc) data-level events if the principal  ARN does not match <code>arn:aws:iam::*:role/service-role/*</code>\". This would allow me  to have an audit trail of atypical access (e.g. a developer downloading an object  as part of debugging), while cutting out 99% of the noise - and therefore cost.</p>"},{"location":"blog/2021/11/22/cloudwatch-emf-in-honeycomb.html","title":"CloudWatch EMF in Honeycomb","text":"<p>The CloudWatch embedded metric format (EMF) is a convenient way to publish metrics to CloudWatch from your apps running in AWS - especially Lambda functions. You just need to emit logs in the right JSON format and CloudWatch Logs will automatically publish them to CloudWatch Metrics. </p> <p>This is okay for basic use cases where you have zero or few dimensions on your metrics and those dimensions are low cardinality. If they're high cardinality, Amazon advises that you instead do not create dimensions and instead treat the metrics as logs and query them using CloudWatch Log Insights. So you have to make  a trade-off between something useful, something affordable and something easy to  use. </p> <p>Or you could use Honeycomb! Honeycomb doesn't make you decide in advance which properties (or combination of properties) would be useful dimensions. You can instead mix and match at query-time. It's the best of both worlds (and a whole lot more, but that's another whole post).</p>"},{"location":"blog/2021/11/22/cloudwatch-emf-in-honeycomb.html#but-i-dont-want-to-rewrite-my-stack-just-to-trial-honeycomb","title":"But I don't want to rewrite my stack just to trial Honeycomb","text":"<p>It's tricky, isn't it? It's hard to evaluate an observability platform without using your own data, but it's also quite a commitment to get data in there.</p> <p>That's why I made a tool that can forward all CloudWatch EMF logs across an AWS org to Honeycomb. Once deployed, all log groups in all accounts in all  regions  will have EMF log entries (and only those entries) forwarded to Honeycomb.  Then you can make a direct comparison and see if Honeycomb is useful to you.</p> <p>Here's a diagram of how the pieces fit together:</p> <p></p> <p>And here's a screenshot of something that took a couple seconds of clicking around in Honeycomb, but would be difficult to impossible in CloudWatch. It's all Lambda functions in my personal org that have used more than 200 MB-seconds over the last 2 hours, broken down by account ID and function name. (The  Lambda Insights layer emits these metrics in EMF format.)</p> <p></p>"},{"location":"blog/2022/10/19/configuration-in-the-cloud.html","title":"Configuration in the cloud","text":"<p>A couple of days ago, Announcing AWS Parameters and Secrets Lambda Extension appeared on the AWS What's New site. The general thrust of the motivation behind  this (providing an easier way to vend credentials securely to AWS Lambda functions) is something that's absolutely needed, but I'm not sure that this approach will move the needle - and it confuses me.</p> <p>My confusion stems from how different an approach this is to how AWS ECS does it. In June 2019, AWS ECS released support for specifying secret values  to be provided to ECS tasks at runtime. These secret values are fetched at  container launch and provided to the process running inside the container as regular environment variables. This is great because:</p> <ul> <li>12 factor apps are encouraged to store configuration in environment variables</li> <li>Almost all application frameworks support it out of the box, integrating    environment values into their own configuration management systems, e.g.   how ASP.Net Core does it.</li> <li>Apps not using frameworks can access environment variables in a single line   of code without requiring any dependencies.</li> <li>It mirrors how configuration can be injected by IDEs or Docker containers   during local development.</li> </ul> <p>I've been complaining about this for a while on Twitter:</p> <p></p> <p>In my imagined ideal state this configuration of secrets would become part of the function configuration just like regular plain environment variables. So the input to <code>UpdateFunctionCode</code> would look something like this:</p> <p></p> <p>This would bring parity with AWS ECS to AWS Lambda and make porting apps between the two a breeze, with no configuration code changes required. In an even more perfect world, I would be able to define those secrets in AWS SAM (or CDK) and have SAM automatically add the least-permissioned policies to the IAM policy for the function, the same as it does for event sources, etc.</p>"},{"location":"blog/2022/10/19/configuration-in-the-cloud.html#less-complaining-more-demos","title":"Less complaining, more demos","text":"<p>Check out <code>cloudenv</code>. It turns YAML ideas into usable demos:</p> <p></p> <p>I just built it as a proof of concept to show that what I want is technically  possible and demonstrates a better (IMO) dev experience. I measure better as:</p> <ul> <li>Fewer Lambda-specific things to learn (and get wrong)</li> <li>Easy enough that people will use it</li> </ul> <p>Keen to hear from folks about why I'm wrong, what might work better, etc.</p>"},{"location":"blog/2020/11/29/cursory-kms-research.html","title":"Cursory AWS KMS research","text":"<p>A couple of months ago, Thai Duong wrote an interesting post about problems with the AWS Encryption SDK. Most of it goes way over my head, but my curiosity was piqued by the mention of reverse-engineering the format of the \"ciphertext\" returned by KMS <code>Encrypt</code>. There wasn't much detail on that (it wasn't the primary topic) so I thought I'd do some digging into it.</p> <p>I haven't dug into this too much, but I thought I'd share what I have so far as I've yet to find any other resources on this. My hope is that someone else is able to build upon this and publish something much more useful and interesting.</p> <p></p> <p>Some points worth calling out, in no particular order:</p> <ul> <li> <p>I was surprised by how much \"overhead\" there is, i.e. how many bytes there   are that I can't seem to change. There's at least 52 bytes that don't change,   regardless of plaintext, key ID, region or encryption context.</p> </li> <li> <p>The bytes that change with the key ID are 33 bytes long. They appear to be   entirely random. A key ID is 16 bytes. An account ID is 12 decimal digits. I'm   not yet sure if this is decipherable.</p> </li> <li> <p>Of the fields that change on every response, there are three. One is 16 bytes,   one is 12 bytes and the other is the length of the plaintext + 28 bytes. The 12   byte field is likely the 96-bit IV used for the AES-GCM algorithm that KMS   performs. I think that part of the 28 bytes is the 16 byte authentication tag   emitted by AES-GCM. And the 16 byte field is the nonce used to derive the data   encryption key using SP800-108 as per the KMS docs. But I really   don't know what I'm talking about.</p> </li> </ul>"},{"location":"blog/2024/01/11/deep-dive-into-aws-cloudshell.html","title":"Deep dive into AWS CloudShell","text":"<p>AWS CloudShell got a new capability in January 2024: running Docker  containers. This piqued my curiosity because Docker-in-Docker usually implies privileged containers, and I have previously used that to escape CodeBuild onto the parent EC2 instance. I wanted to know if the same could be done in  CloudShell - and how its AWS credential system worked (the environment inherits the user's credentials, unlike CodeBuild). The short answer is \"it can be done\",  and this post goes into a) how to do it and b) documenting what I could learn  about the inner workings of CloudShell.</p>"},{"location":"blog/2024/01/11/deep-dive-into-aws-cloudshell.html#container-escape","title":"Container escape","text":"<p>The CloudShell environment itself runs on a container on an EC2 instance. \"Escaping\"  the container is relatively easy because it is a privileged container. These are  the steps I took:</p> <ol> <li><code>sudo -i</code> to start a shell as <code>root</code>.</li> <li><code>mkdir /host &amp;&amp; mount /dev/nvme0n1p1 /host</code>. This mounts the host EC2     filesystem into the <code>/host</code> directory of the container. </li> <li><code>ssh-keygen</code> to generate an SSH keypair.</li> <li><code>cat /root/.ssh/id_rsa.pub &gt;&gt; /host/root/.ssh/authorized_keys</code></li> <li><code>ssh $(uname -n)</code> to SSH into the host EC2 instance.</li> </ol>"},{"location":"blog/2024/01/11/deep-dive-into-aws-cloudshell.html#looking-around","title":"Looking around","text":"<p>Once on the EC2 instance, I was interested to see how the credentials were provided to the CloudShell environment. The CloudShell environment has the following two environment variables that determine credential retrieval:</p> <ul> <li><code>AWS_CONTAINER_CREDENTIALS_FULL_URI=http://localhost:1338/latest/meta-data/container/security-credentials</code></li> <li><code>AWS_CONTAINER_AUTHORIZATION_TOKEN=T0yMmQ5cHLbmd15fL45al1bK1Aw/Ty6rNtQ8V4Q/3DM=</code></li> </ul> <p>The first env var is the URL that the AWS CLI and SDKs will send a GET request  to get credentials. The second env var contains the value sent in the <code>Authorization</code> request header during that request. Based on this, we now know there's a program listening on local port 1338 and vending credentials - we'll use that in our  search.</p>"},{"location":"blog/2024/01/11/deep-dive-into-aws-cloudshell.html#containers","title":"Containers","text":"<p>On the EC2 host, I ran <code>docker ps</code> and saw a few containers running. Running  <code>docker inspect $cid</code> on them reveals that they are managed by AWS ECS. The image  itself is <code>507722522093.dkr.ecr.ap-southeast-2.amazonaws.com/mde-images:live_base_100</code>. I have copied this to public.ecr.aws/aidansteele/cloudshell:live_base_100 if you want to take a look. </p> <p>There are two relevant containers, both running the above image. The first is  the \"controller\" container, running <code>/usr/bin/controller</code> in a few different \"modes\". The <code>/usr/bin/controller -mode credentials</code> process is the interesting one that is serving port 1338.</p> <p>The second container is the \"base\" container. It is running the same binary, but in <code>base-container</code> mode. This process is parent to <code>dockerd</code>. It uses this Docker-in-Docker to run a different image: <code>117854687782.dkr.ecr.ap-southeast-2.amazonaws.com/scallop-customer-image:latest-patched</code> -  this image is copied to public.ecr.aws/aidansteele/cloudshell:scallop-customer-image-latest-patched.  This is where the CloudShell environment exists, so it's actually  Docker-in-Docker-in-Docker. Each CloudShell session runs inside a <code>tmux</code> session in this inner container. </p> <p></p>"},{"location":"blog/2024/01/11/deep-dive-into-aws-cloudshell.html#iam-roles","title":"IAM roles","text":"<p>There are a number of roles involved here:</p> <p>The EC2 instance role has a role session ARN that looks like this: <code>arn:aws:sts::624018990330:assumed-role/moontide-ecs-ec2-cluster-moontidedevstandard1micr-1A1CJ1ZR4CHF9/i-01a473779ad5984cc</code>.  This role seems to have the <code>AmazonEC2ContainerServiceforEC2Role</code> managed policy and some explicit denies to further narrow it down.</p> <p>The ECS-managed containers all use the same \"ECS task IAM role\". Its ARN looks like: <code>arn:aws:sts::624018990330:assumed-role/moontide-task-role-control-plane/i-02d8913df95941b0a22cd3726ead75d0</code>.</p> <p>There's the ECS task execution IAM role, used for pulling ECR images and pushing logs to CloudWatch. Its ARN looks like: <code>arn:aws:sts::624018990330:assumed-role/moontide-task-execution-role/i-02d8913df95941b0a22cd3726ead75d0</code>.</p> <p>The ECS task IAM role is used by the credential server process to retrieve  credentials from the private control plane API and return them to the CloudShell environment on-demand. That API is powered by API Gateway and uses SigV4 (IAM) authentication. If you install <code>awscurl</code> into the \"controller\" container, you  can replicate the credential-fetching functionality by running this:</p> <pre><code>awscurl \\\n  --service execute-api \\\n  --region ${AWS_REGION} \\\n  ${CALLBACK_SERVICE_ENDPOINT_DNS}/${INSTANCE_ID}/credentials/role\n</code></pre>"},{"location":"blog/2024/01/11/deep-dive-into-aws-cloudshell.html#assorted-thoughts","title":"Assorted thoughts","text":"<ul> <li>There seem to be multiple ECS clusters per CloudShell AWS account. There are   also multiple CloudShell AWS accounts per region. Two principals in my own account   got mapped to entirely different CloudShell AWS accounts. I wonder what the   mapping algorithm is.</li> <li>The role session name for the \"control plane\" role looks like a very long EC2   instance ID. What is it?</li> <li>In my (limited) testing, the EC2 instance is always a t3.medium</li> <li>The EC2 instance has a public IP address. I guess this saves money on a NAT   gateway. It has a security group that allows no inbound traffic, though.</li> <li>This \"moontide\" binary makes a few references to Cloud9, so I think maybe   there is shared heritage there. I haven't looked into Cloud9 to see how   closely CloudShell reflects it.</li> <li>There are references to interesting objects in S3. I can't figure out how to   access them. I don't know if that's because they're only useful for Cloud9,   or there is extra functionality I couldn't reverse-engineer.</li> <li>AWS does a good job of defence-in-depth. Even though I \"escaped\" the container,   their internal API Gateway is locked down to only allow access in legitimate   scenarios. Many other places wouldn't do that and have a \"soft\" interior.</li> <li>There is a line in the EC2 userdata that looks like it intends to block access   to the EC2 IMDS from the containers. It doesn't work, but I'm also so weak at   <code>iptables</code> that I can't even suggest a fix.</li> <li>Thanks go to Christophe Tafani-Dereeper and Nick Frichette    for their collaboration on poking at this. It helped a lot bouncing ideas around    with them. </li> </ul>"},{"location":"blog/2024/01/11/deep-dive-into-aws-cloudshell.html#update","title":"Update","text":"<p>I just learned about this write-up of CloudShell published last year. What stood out to me is that CloudShell previously used Firecracker microVMs, but no longer does. I wonder why the migration to \"regular\" EC2 instances occurred. Maybe it was to support this new functionality (runner Docker containers)?</p>"},{"location":"blog/2025/07/27/federating-into-azure-gcp-and-aws-with-oidc.html","title":"Federating into Azure, GCP and AWS with OIDC","text":"<p>Lately, I've been interested in how third party vendors can best authenticate into their customers' cloud accounts. The status quo in AWS is usually role assumption from the vendor's account to the customers', but what about GCP and Azure? Can OIDC be used to authenticate into all three clouds in approximately the same way? I think the answer is yes, and this blog post aims to show how to do so.</p> <p>I've been learning more about GCP and Azure recently. Specifically, whether it is  possible to de-duplicate federation logic between the clouds using OIDC. My partner  was a big help here. They did a lot of research and experimentation and got all the code working. I terraformed the setup and wrote this blog post. </p> <p>I wanted to write this because I genuinely couldn't find code-level examples for  GCP or Azure elsewhere. I found plenty of blogs and docs about how to use OIDC  to enable secretless federation between GitHub, Azure DevOps, etc - but none on  how a vendor should do it themselves! So this blog post aims to serve as an example of how I'd love to see vendors federate into my cloud accounts.</p>"},{"location":"blog/2025/07/27/federating-into-azure-gcp-and-aws-with-oidc.html#the-first-step-deploy-everything","title":"The first step: deploy everything","text":"<p>I've created a GitHub repo named <code>cloudfed</code>. It is a Terraform project that consists of four modules:</p> <p><code>idp</code>: This module implements the publicly-accessible part of an OIDC identity provider. Specifically, it provisions a KMS key (hosted in AWS KMS), a Lambda  function and a URL for that function. The function serves two paths: <code>/{tenant}/.well-known/openid-configuration</code> and <code>{tenant}/.well-known/jwks</code> as per the OIDC spec.</p> <p><code>azure</code>, <code>gcp</code> and <code>aws</code>: These modules implement the resources required to  federate into their corresponding clouds using an OIDC token. More on these in each of the following sections.</p> <p>In order to deploy these resources, you will need an Azure tenant, a GCP organization and an AWS account. The free tier is fine for all of these. Clone the repo, edit the values in <code>vars.auto.tfvars</code> and run <code>make deploy</code>.</p>"},{"location":"blog/2025/07/27/federating-into-azure-gcp-and-aws-with-oidc.html#the-second-step","title":"The second step","text":"<p>You can now run <code>make run</code>. That will in turn run each of the sample apps to federate into AWS, GCP and Azure. The code isn't perfect, but it should be a reasonable starting point (e.g. it handles refreshing credentials when they expire, etc.)</p> <p>Each Go package does the moral equivalent of enumerating storage buckets in an  account/project/subscription, so if you want to see some output you'll need to create some (free) buckets. Empty buckets are fine.</p>"},{"location":"blog/2025/07/27/federating-into-azure-gcp-and-aws-with-oidc.html#azure-specifics","title":"Azure specifics","text":"<p>I'm still wrapping my head around Azure. There appear to be a couple of different ways to do OIDC federation. One is directly configured on a \"user-assigned managed identity\", which seems like the equivalent of an IAM role? The other is to create an application in Entra ID (which itself seems more of a sibling to Azure than an Azure service itself), configure OIDC on that application, and then assign that application roles in Azure subscriptions. </p> <p>It's worth noting (especially if you're mostly an AWS person, like me) that these \"roles\" are like permissions in AWS, and they can be assigned at relatively  arbitrary scopes. You can assign them at the subscription level (mostly equivalent to AWS accounts), or you can assign them at a \"management group\" level, which seem equivalent-ish to AWS organization units. Technically, you can also assign them at lower resource-based levels, too.</p> <p>Note</p> <p>The role assignment resource is <code>azurerm_role_assignment</code> and the <code>azurerm</code> provider needs to be configured with a subscription, but if you specify a management group scope, does the resource really belong to a subscription? Stay tuned while I figure that out (or get in touch and explain it to me, please)</p>"},{"location":"blog/2025/07/27/federating-into-azure-gcp-and-aws-with-oidc.html#gcp-specifics","title":"GCP specifics","text":"<p>GCP is more similar to AWS than Azure in that the resource type (<code>google_iam_workload_identity_pool_provider</code>) that creates an OIDC IdP belongs to a project, but it's more similar to Azure  than AWS in that IAM permissions can be assigned to principals in the workload  identity pool at the organisation level. My example Terraform module does exactly this: grants the <code>viewer</code> role across the whole organization. Is that a bad idea? I guess I'll find out soon if a reader contacts me and lets me know!</p>"},{"location":"blog/2025/07/27/federating-into-azure-gcp-and-aws-with-oidc.html#a-note-on-multi-tenancy","title":"A note on multi-tenancy","text":"<p>Amazon have docs where they strongly advise tenant-specific OIDC issuer URLs to avoid confused deputy attacks. I won't bother to repeat their explanation here, but I'll add that I incorporated their advice into the sample code in the GitHub repo.</p>"},{"location":"blog/2024/06/05/gotcha-alway-use-arns-for-s3-sse-kms.html","title":"Gotcha: always use ARNs for S3 SSE-KMS","text":"<p>Imagine you have a scenario represented in the following diagram: </p> <p></p> <p>When role 2 in account 2 calls <code>s3:PutObject</code> to store an object in bucket 1, which KMS key do you think is used to encrypt the object? </p> <p>If you guessed key 2 (i.e. the one in the same account as the role) you would  be right - and you would have known more than I did this morning. I assumed it would be key 1, i.e. the one living in the same account as the bucket. I would wager a guess that most people would think the same as I did (i.e. that  unqualified key IDs and aliases are resolved relative to the bucket's account,  not the caller's account) when they are configuring their bucket's encryption  configuration.</p>"},{"location":"blog/2024/06/05/gotcha-alway-use-arns-for-s3-sse-kms.html#why-is-this-a-problem","title":"Why is this a problem?","text":"<p>This behaviour can cause problems in two ways. </p> <p>The first is the more likely scenario: key 2 doesn't exist. Then role 2's  attempt to write objects will fail with an error message about a KMS key with  ARN <code>arn:aws:kms:region:22222222:alias/my-key</code> not existing. At least that \"fails fast\" and can be resolved.</p> <p>The second is described at the start of this post: the object will be successfully stored and encrypted with key 2. Now if any principals in account 1 try to read the object they will fail with KMS decryption errors, even if they have permission to decrypt using key 1 -- that will cause a lot of confusion down the track,  especially if roles from both accounts write to the bucket: some objects will be  encrypted with key 1 and some encrypted with key 2. The only way to fix those is to re-upload the objects.</p>"},{"location":"blog/2024/06/05/gotcha-alway-use-arns-for-s3-sse-kms.html#what-can-be-done-to-prevent-it","title":"What can be done to prevent it?","text":"<p>Always use a fully-qualified ARN (either a key ID or key alias is fine) and you won't have this problem. AWS already tries to nudge users towards this safe  default: the web console will only allow you to specify an ARN. I suspect they won't change the API because of their commendable commitment to backwards compatibility.</p>"},{"location":"blog/2024/06/05/gotcha-alway-use-arns-for-s3-sse-kms.html#do-other-aws-services-have-this-issue","title":"Do other AWS services have this issue?","text":"<p>There are a lot of services that allow a) cross-account access and b) encryption using customer-managed KMS keys. I don't have the time to check them all, but I did check DynamoDB and Secrets Manager.</p> <p>DynamoDB doesn't have this problem because when a value like <code>alias/my-key</code> is provided to the <code>UpdateTable</code> API, it is resolved to a fully-qualified key ARN and stored as such. </p> <p>Secrets Manager likewise doesn't have this problem. It doesn't take the DynamoDB approach of resolving to a fully-qualified ARN, but it will fail fast-ish.  Cross-account reads will get the error message <code>Access to KMS is not allowed</code>. Cross-account writes will get <code>You must specify a KMS key ARN to update the secret from a different AWS account.</code> It would be nice if the first error message was as helpful as the second error message, but now I'm just being picky.</p>"},{"location":"blog/2024/06/05/gotcha-alway-use-arns-for-s3-sse-kms.html#is-this-a-security-issue","title":"Is this a security issue?","text":"<p>I don't think so. The closest I can come is this really contrived scenario:</p> <ul> <li>Key 2's key policy is changed to allow any AWS account to call <code>kms:Decrypt</code>.</li> <li>Role 2 downloads and re-uploads every object in bucket 1, encrypted with key 2.</li> <li>Now the administrator of account 2 gets a sneaky audit trail of everyone who   reads objects from bucket 1. The administrator of account 1 might not like that.</li> </ul>"},{"location":"blog/2021/09/29/graviton2-arm-comes-to-lambda.html","title":"Graviton2: ARM comes to Lambda","text":"<p>Today, Amazon Web Services has unveiled AWS Lambda Functions Powered By AWS Graviton2 Processors,  offering a 20% reduction in the GB-second price compared to their x86-powered cousins. But first, some short history.</p>"},{"location":"blog/2021/09/29/graviton2-arm-comes-to-lambda.html#history","title":"History","text":"<p>In 2015, AWS acquired Annapurna Labs. They were first tasked with building the hardware that now powers the AWS Nitro system. But soon thereafter, they turned their focus to compute and launched the first generation of Graviton-powered EC2 instances in the form of the <code>a1</code> instance family.</p> <p>The next year, they launched Graviton2 with significant performance  improvements over the first generation. This made them not only competitive with  Intel and AMD's chips, but sometimes up to 40% better on a cost-performance basis compared to current generation x86-based instances.</p> <p>And that brings us to today. Graviton2, which has rolled out to Amazon EC2, Amazon RDS and Amazon ElastiCache has made its way to AWS Lambda.</p>"},{"location":"blog/2021/09/29/graviton2-arm-comes-to-lambda.html#tldr","title":"tl;dr","text":"<p>It's almost always a no-brainer to switch to the new architecture. Here's how you can do that for an example app written in Go:</p> <pre><code>--- a/cfn.yml\n+++ b/cfn.yml\n@@ -1,20 +1,20 @@\n Transform: AWS::Serverless-2016-10-31\n Resources:\n   Function:\n     Type: AWS::Serverless::Function\n     Properties:\n       Runtime: provided.al2\n       Handler: my-app\n       CodeUri: ./bootstrap\n+      Architectures: [arm64]\n       Policies:\n         DynamoDBReadPolicy:\n           TableName: example\n       Events:\n         Api:\n           Type: HttpApi\ndiff --git a/build.sh b/build.sh\nindex 7f95be8..617824a 100755\n--- a/build.sh\n+++ b/build.sh\n@@ -1,8 +1,8 @@\n  #!/bin/sh\n\n  export CGO_ENABLED=0\n  export GOOS=linux\n- export GOARCH=amd64\n+ export GOARCH=arm64\n\n  go build -o bootstrap\n  sam deploy ...\n</code></pre> <p>Pretty easy, right? There's no need to install new tools, or mess around with Docker, or rewrite your code. Just change (or set, if you didn't have it in the first place) that <code>GOARCH</code> environment variable and you're good to go.</p>"},{"location":"blog/2021/09/29/graviton2-arm-comes-to-lambda.html#performance","title":"Performance","text":"<p>Of course I was excited to see how the new architecture performed. Some folks had  reported 28-65% performance gains for their workloads on EC2 instances! But that is extremely dependent on your workload. In my case, my app is fairly CRUDdy. There are parts that wait on the  network, parts that read some JSON and write some slightly different JSON and some business logic squeezed in between. So what performance gains did I see?</p> <p>None. I made over 100,000 invocations of my functions in both <code>x86_64</code> and <code>arm64</code> architecture options and it was impossible to tell them apart. Sometimes <code>arm64</code> won, sometimes <code>x86_64</code> won. All in all, the difference was well within the margin of noise.</p> <p>So is it a disappointment? Hardly! I just saved 20% on my Lambda bill by changing two lines of code. That's a great day in anyone's books. Or maybe I'll increase the RAM allocated to my functions by 25% (= 1/.8) and have a faster execution  time for the same price I was paying before. I guess I'll check the  Lambda Power Tuning tool to see if that's worthwhile.</p>"},{"location":"blog/2021/09/29/graviton2-arm-comes-to-lambda.html#still-need-convincing","title":"Still need convincing?","text":"<p>Maybe your business depends on your Lambda functions being rock solid and you don't want to risk trying something new. That's fair enough. Lambda supports  alias routing configurations: a feature where you can send x% of a function's traffic to one version of the function and the remaining 100-x% to a different version of the same function. So you can have two different versions of the same function - one with <code>x86_64</code> and one with  <code>arm64</code> - and incrementally shift traffic as slowly as it takes to build your confidence. </p> <p>tl;dr: AWS have made it very easy to try out the new architecture. You might as well give it a shot. They've measured up to 34% price-performance improvement over x86-based Lambda functions. </p>"},{"location":"blog/2023/12/29/how-ima-ge-cx-works.html","title":"How ima.ge.cx works","text":"<p>This article has been in my drafts for 380 days. It's probably time I published it, before I forget even more details about how it works. ~A few~ 380+ days ago, I  published ima.ge.cx. It's a web frontend for browsing the contents  of Docker images. It's like a less powerful version of <code>dive</code> that  doesn't require you to pull images locally. It's also worth noting that there's a much more feature-rich (and likely less buggy) site that does a similar thing: Registry Explorer.</p> <p>I planned on cleaning up the code significantly before publishing (I can't have anyone see how the sausage is made), but in the year since writing it I have lost all context and motivation to do so. In an act of personal growth, I have pushed  the code to GitHub in its incomplete state. There are deep link references to specific functions below. But first, here's a rough rundown of how it works. First, an architecture diagram:</p> <p></p> <p>And here's a screenshot of the workflow diagram from the Step Functions console:</p> <p></p>"},{"location":"blog/2023/12/29/how-ima-ge-cx-works.html#image-indexing","title":"Image indexing","text":"<p>When an image hasn't yet been indexed, a Step Function execution is started. The input to that execution is:</p> <pre><code>{\n  \"Repo\": \"mcr.microsoft.com/dotnet/sdk\",\n  \"Digest\": \"sha256:d775c8f5267a84304e601702c37b0c416ba9813e72d16d0ee3e70e96323ee624\"\n}\n</code></pre> <p>The first task (Get Image Layers) contacts the registry and returns a list of digests for all the layers in the image.</p> <p>The second task is a Map state over those layers. The <code>Index layer tar.gz</code> task is invoked concurrently for each layer in the image. The input to that task is:</p> <pre><code>{\n  \"Repo\": \"mcr.microsoft.com/dotnet/sdk\",\n  \"Digest\": \"sha256:d775c8f5267a84304e601702c37b0c416ba9813e72d16d0ee3e70e96323ee624\",\n  \"Layer\": \"sha256:2f2ed1ba8f71764331c67cd10a338c6ed607e5c173b056ec69fbe592389fc33a\"\n}\n</code></pre> <p>That Lambda function does the following:</p> <ul> <li>It downloads the layer tar.gz blob from the registry</li> <li>Streams the tar.gz stream to <code>gztool</code> via stdin</li> <li>Reads the tar archive from <code>gztool</code>'s stdout</li> <li>Records the metadata and offset for each file in the tar stream</li> <li>Uploads the gz index generated by <code>gztool</code> to S3 at <code>s3://bucket/sha256:$layer/index.gzi</code></li> <li>Uploads the tar index as a newline-delimited JSON doc to S3 at    <code>s3://bucket/sha256:$layer/files.json.gz</code></li> </ul> <p>The next task in the workflow is <code>Concatenate indexes</code>, which combines all the <code>files.json.gz</code> layer-specific indexes into a single file. We do this so we only have one file to query at read-time. Here's an excerpt of a few lines from that concatenated file (pretty-printed for readability):</p> <pre><code>{\n  \"Offset\": 100097024,\n  \"Spans\": [\n    10\n  ],\n  \"Hdr\": {\n    \"Typeflag\": 48,\n    \"Name\": \"opt/teamcity/webapps/ROOT/WEB-INF/lib/spring-security.jar\",\n    \"Linkname\": \"\",\n    \"Size\": 2545358,\n    \"Mode\": 33188,\n    \"Uid\": 1000,\n    \"Gid\": 1000,\n    \"Uname\": \"\",\n    \"Gname\": \"\",\n    \"ModTime\": \"2022-10-27T09:46:58Z\",\n    \"AccessTime\": \"0001-01-01T00:00:00Z\",\n    \"ChangeTime\": \"0001-01-01T00:00:00Z\",\n    \"Devmajor\": 0,\n    \"Devminor\": 0,\n    \"Xattrs\": null,\n    \"PAXRecords\": null,\n    \"Format\": 2\n  },\n  \"Parent\": \"opt/teamcity/webapps/ROOT/WEB-INF/lib/\",\n  \"Layer\": \"sha256:30175d9c0a53916cd46e92a1f59c48406e6fa7690889342829a7636745756f23\"\n}\n{\n  \"Offset\": 102643200,\n  \"Spans\": [\n    10,\n    11\n  ],\n  \"Hdr\": {\n    \"Typeflag\": 48,\n    \"Name\": \"opt/teamcity/webapps/ROOT/WEB-INF/lib/spring-webmvc.jar\",\n    \"Linkname\": \"\",\n    \"Size\": 2661280,\n    \"Mode\": 33188,\n    \"Uid\": 1000,\n    \"Gid\": 1000,\n    \"Uname\": \"\",\n    \"Gname\": \"\",\n    \"ModTime\": \"2022-10-27T09:46:58Z\",\n    \"AccessTime\": \"0001-01-01T00:00:00Z\",\n    \"ChangeTime\": \"0001-01-01T00:00:00Z\",\n    \"Devmajor\": 0,\n    \"Devminor\": 0,\n    \"Xattrs\": null,\n    \"PAXRecords\": null,\n    \"Format\": 2\n  },\n  \"Parent\": \"opt/teamcity/webapps/ROOT/WEB-INF/lib/\",\n  \"Layer\": \"sha256:30175d9c0a53916cd46e92a1f59c48406e6fa7690889342829a7636745756f23\"\n}\n{\n  \"Offset\": 105305088,\n  \"Spans\": [\n    11\n  ],\n  \"Hdr\": {\n    \"Typeflag\": 48,\n    \"Name\": \"opt/teamcity/webapps/ROOT/WEB-INF/lib/spring.jar\",\n    \"Linkname\": \"\",\n    \"Size\": 5050699,\n    \"Mode\": 33188,\n    \"Uid\": 1000,\n    \"Gid\": 1000,\n    \"Uname\": \"\",\n    \"Gname\": \"\",\n    \"ModTime\": \"2022-10-27T09:46:58Z\",\n    \"AccessTime\": \"0001-01-01T00:00:00Z\",\n    \"ChangeTime\": \"0001-01-01T00:00:00Z\",\n    \"Devmajor\": 0,\n    \"Devminor\": 0,\n    \"Xattrs\": null,\n    \"PAXRecords\": null,\n    \"Format\": 2\n  },\n  \"Parent\": \"opt/teamcity/webapps/ROOT/WEB-INF/lib/\",\n  \"Layer\": \"sha256:30175d9c0a53916cd46e92a1f59c48406e6fa7690889342829a7636745756f23\"\n}\n</code></pre>"},{"location":"blog/2023/12/29/how-ima-ge-cx-works.html#image-reading","title":"Image reading","text":""},{"location":"blog/2023/12/29/how-ima-ge-cx-works.html#directory-contents","title":"Directory contents","text":"<p>GitHub link. When enumerating the filesystem of an image, the  <code>/api/dir</code> function uses S3 Select to return only parts of the file.  The SQL query is <code>SELECT * FROM s3object s WHERE s.Parent = '%s'</code>, which causes  S3 to only return the JSON lines for files in that particular directory. Using S3 Select instead of downloading the whole file into memory means that the Lambda  function won't slow down or crash when processing the contents of giant Docker images.</p>"},{"location":"blog/2023/12/29/how-ima-ge-cx-works.html#file-contents","title":"File contents","text":"<p>GitHub link. When returning the contents of a specific file, the <code>/api/file</code> function uses S3 Select again. This time the query is <code>SELECT * FROM s3object s WHERE s.Hdr.Name = '%s'</code>. This determines which layer gzindex needs to be downloaded and fed back into <code>gztool</code>. <code>gztool</code> then tells us the range of the compressed bytes that we need to download from the Docker image registry (via a HTTP range request). After downloading that byte range, we feed the compressed bytes back into <code>gztool</code> and it returns the uncompressed file contents - which get returned back to the client.</p>"},{"location":"blog/2023/12/29/how-ima-ge-cx-works.html#parting-thoughts","title":"Parting thoughts","text":"<ul> <li>I should have finished this blog post while it was still fresh. It's hard to   blog about code you don't remember writing.</li> <li>Frontend UIs are so hard. Being unable to build a nice frontend for this   project drained almost all my enthusiasm to see it through to the end.</li> <li>Serverless is really nice. Step Functions and Lambda will scale out as much   as needed when an image needs to be processed and scale back to zero when the   site isn't being used - which is most of the time. If I had to pay a few dollars   a month to run a VPS for this project, I would have turned it off long ago. I   don't think this project has even cracked the $1 threshold in the last year it's   been online.</li> <li>S3 Select is a great service, but I've never heard of anyone using it. One    surprising gotcha was that it returns results in chunks, but they're not   necessarily aligned with the ends of lines - so you need to buffer the results   and then parse those line-by-line. Here's a function I wrote    for that.</li> <li>The absolute highlight of working on this is that it lead to getting in touch   with Jon Johnson. He's a very smart guy and I am honoured to have    played my part in his enthusiasm for gzip. </li> </ul>"},{"location":"blog/2023/01/11/improve-github-actions-oidc-security-posture-with-custom-issuer.html","title":"Improve GitHub Actions OIDC security posture with custom issuer","text":"<p>GitHub Actions has supported using OIDC tokens for about 15 months now. It is a much better way of providing AWS credentials to workflows than creating IAM users and storing long-lived access keys in GitHub Actions secrets.</p> <p>One issue holding back larger organisations from adopting this solution is the lack of useful granular controls. I touched on this in an earlier article AWS IAM OIDC IDPs need more controls.</p> <p>I've since seen a new section pop up in the GitHub docs: Switching to a unique token URL. It was actually announced back in August 2022 but I either didn't  see it or missed the significance.</p> <p>GitHub Enterprise Cloud customers can now get OIDC tokens issued that look like  this (trimmed for brevity):</p> <pre><code>{\n  \"iss\": \"https://token.actions.githubusercontent.com/octocat-inc\",\n  \"jti\": \"6f4762ed-0758-4ccb-808d-ee3af5d723a8\",\n  \"sub\": \"repo:octocat-inc/private-server:ref:refs/heads/main\",\n  \"aud\": \"http://octocat-inc.example/octocat-inc\",\n  \"enterprise\": \"octocat-inc\",\n  \"bf\": 1755350653,\n  \"exp\": 1755351553,\n  \"iat\": 1755351253\n}\n</code></pre> <p>The relevant field is the <code>iss</code> (issuer) on the first line. If your enterprise  slug is <code>octocat-inc</code> like in this example, you can now create an IAM OIDC IdP with the following CloudFormation:</p> <pre><code>  GithubOidc:\n    Type: AWS::IAM::OIDCProvider\n    Properties:\n      Url: https://token.actions.githubusercontent.com/octocat-inc\n      ThumbprintList: [6938fd4d98bab03faadb97b34396831e3780aea1]\n      ClientIdList: [sts.amazonaws.com]\n</code></pre> <p>and you create a role for GitHub Actions to assume with this:</p> <pre><code>  Role:\n    Type: AWS::IAM::Role\n    Properties:\n      RoleName: ExampleGithubRole\n      ManagedPolicyArns: [arn:aws:iam::aws:policy/ReadOnlyAccess]\n      AssumeRolePolicyDocument:\n        Statement:\n          - Effect: Allow\n            Action: sts:AssumeRoleWithWebIdentity\n            Principal:\n              # this line horizontally scrolls. \n              # the important part is on the end!\n              Federated: !Sub arn:aws:iam::${AWS::AccountId}:oidc-provider/token.actions.githubusercontent.com/octocat-inc\n            Condition:\n              StringLike:\n                token.actions.githubusercontent.com:sub: !Sub repo:your-org/your-repo:*\n</code></pre>"},{"location":"blog/2023/01/11/improve-github-actions-oidc-security-posture-with-custom-issuer.html#significance","title":"Significance","text":"<p>Note that the ARN of the federated IAM IdP includes the enterprise name. You  should still create roles with conditions on the <code>sub</code>, but it is not  catastrophic if you don't. You can grant developers permission to invoke  <code>iam:CreateRole</code> without worrying that an errant role trust policy has opened  up access to the entirety of Github.com (a missing condition \"only\" opens up access to any repository in your enterprise)</p>"},{"location":"blog/2023/01/11/improve-github-actions-oidc-security-posture-with-custom-issuer.html#conclusion","title":"Conclusion","text":"<p>I'd still like to see the AWS IAM controls mentioned in the earlier blog post and I'd still like a way to see the full JWT claims in CloudTrail, but this is a really nice improvement that will enable usage of this functionality for more conservative organisations.</p>"},{"location":"blog/2022/01/06/ipv6-and-totp.html","title":"IPv6 and TOTP","text":"<p>What's the silliest use for 281 trillion IP addresses that you can think of? That's the question I asked myself when AWS launched support for assigning  IPv6 prefixes to EC2 instances. </p> <p>The IPv6 prefixes are <code>/80</code>, which gives your EC2 instance 281,474,976,710,656  IP addresses to play with. You could use the feature to run 281 trillion containers  with their own IPs (which I assume is what AWS intended for the feature), but I  wanted to find a more fun use.</p> <p>After noodling on it for a bit, I had an idea: SSH doesn't support TOTP  (those six digit codes that change every 30 seconds) out of the box. Neither  does Telnet, plain old HTTP or any number of protocols. So I thought it would  be fun to add TOTP support to every protocol by embedding the six digit code  inside the IP address.</p> <p>The result is <code>ipv6-ghost-ship</code>.</p>"},{"location":"blog/2023/03/21/lambda-cloudtrail-data-events.html","title":"Lambda CloudTrail data events","text":"<p>Today I was experimenting with CloudTrail data events for Lambda invocations, because I learned that as of 2021, these data events log the ENI ID used by a Lambda function invocation. For completeness, the event looks like this:</p> <pre><code>{\n  \"eventVersion\": \"1.08\",\n  \"userIdentity\": {\n    \"invokedBy\": \"states.amazonaws.com\",\n    \"type\": \"AssumedRole\",\n    \"principalId\": \"AROAEXAMPLEZGVVXB2VC:wozynIhxRGfTvfRlXhHKONdyrIaAmRyN\",\n    \"arn\": \"arn:aws:sts::012345679012:assumed-role/My-RoleName-1746R4C1N6UFS/wozynIhxRGfTvfRlXhHKONdyrIaAmRyN\",\n    \"accountId\": \"012345679012\",\n    \"accessKeyId\": \"ASIAEXAMPLEQMIZPFY5\",\n    \"sessionContext\": {\n      \"sessionIssuer\": {\n        \"type\": \"Role\",\n        \"principalId\": \"AROAEXAMPLEZGVVXB2VC\",\n        \"arn\": \"arn:aws:iam::012345679012:role/My-RoleName-1746R4C1N6UFS\",\n        \"accountId\": \"012345679012\",\n        \"userName\": \"My-RoleName-1746R4C1N6UFS\"\n      },\n      \"attributes\": {\n        \"creationDate\": \"2023-03-21T05:32:03Z\",\n        \"mfaAuthenticated\": \"false\"\n      }\n    }\n  },\n  \"eventTime\": \"2023-03-21T05:32:03Z\",\n  \"eventSource\": \"lambda.amazonaws.com\",\n  \"eventName\": \"Invoke\",\n  \"awsRegion\": \"us-east-1\",\n  \"sourceIPAddress\": \"states.amazonaws.com\",\n  \"userAgent\": \"states.amazonaws.com\",\n  \"requestParameters\": null,\n  \"responseElements\": null,\n  \"additionalEventData\": {\n    \"customerEniId\": \"eni-02258670f86ec5c51\",\n    \"functionVersion\": \"arn:aws:lambda:us-east-1:012345679012:function:my-function-name:23\"\n  },\n  \"requestID\": \"006ee61e-4b61-4de5-b3b8-99b4d72ca7e7\",\n  \"eventID\": \"b1e55492-d8be-4b9a-b794-a8a0a28162f0\",\n  \"readOnly\": false,\n  \"resources\": [\n    {\n      \"accountId\": \"012345679012\",\n      \"type\": \"AWS::Lambda::Function\",\n      \"ARN\": \"arn:aws:lambda:us-east-1:012345679012:function:my-function-name\"\n    }\n  ],\n  \"eventType\": \"AwsApiCall\",\n  \"managementEvent\": false,\n  \"recipientAccountId\": \"012345679012\",\n  \"eventCategory\": \"Data\"\n}\n</code></pre> <p>The ENI ID is at <code>$.additionalEventData.customerEniId</code>. It's also worth noting that the executed function version is logged, but not the alias used for the <code>lambda.Invoke()</code> API call.</p>"},{"location":"blog/2023/03/21/lambda-cloudtrail-data-events.html#bonus","title":"Bonus","text":"<p>A pair of CloudTrail events caught my eye. There was a record with <code>\"eventName\": \"InvokeExecution\"</code>. I tried googling for \"InvokeExecution\" and got very few results. I figured fellow CloudTrail nerds might have mentioned it, so I tried searching the Cloud Security  Forum slack and found this:</p> <p></p> <p>...turns out that I had asked the exact same question almost exactly a year ago and had forgotten all about it. So I thought I'd blog about it solely so Google indexes this and I can find it next time I forget about this and look it up again.</p> <p>For the record: <code>InvokeExecution</code> events happen when a Lambda function is invoked asynchronously, i.e. with an <code>InvocationType: \"Event\"</code> parameter. They appear in pairs (or more, if the invocation fails and is retried): there is an <code>Invoke</code> event and one or more <code>InvokeExecution</code> events. They can be correlated by the CloudTrail record's <code>requestID</code> attribute - which also matches the request ID in the function's CloudWatch Logs output. Interestingly, the <code>Invoke</code> record does include the invoked alias (unlike the synchronous execution in the previous example). The executed version and ENI ID are in the <code>InvokeExecution</code> record. Here are examples:</p>"},{"location":"blog/2023/03/21/lambda-cloudtrail-data-events.html#invoke","title":"Invoke","text":"<pre><code>{\n  \"eventVersion\": \"1.08\",\n  \"userIdentity\": {\n    \"type\": \"AWSService\",\n    \"invokedBy\": \"events.amazonaws.com\"\n  },\n  \"eventTime\": \"2023-03-21T05:32:03Z\",\n  \"eventSource\": \"lambda.amazonaws.com\",\n  \"eventName\": \"Invoke\",\n  \"awsRegion\": \"us-east-1\",\n  \"sourceIPAddress\": \"events.amazonaws.com\",\n  \"userAgent\": \"events.amazonaws.com\",\n  \"requestParameters\": {\n    \"functionName\": \"arn:aws:lambda:us-east-1:012345679012:function:my-function-name:live\",\n    \"invocationType\": \"Event\",\n    \"sourceArn\": \"arn:aws:events:us-east-1:012345679012:rule/my-rule-name-VMA1FQWKHEIL\",\n    \"sourceAccount\": \"012345679012\"\n  },\n  \"responseElements\": null,\n  \"additionalEventData\": {\n    \"functionVersion\": \"arn:aws:lambda:us-east-1:012345679012:function:my-function-name:14\"\n  },\n  \"requestID\": \"6baa1b22-d95f-4550-8528-2d0e0ea5b845\",\n  \"eventID\": \"32321ee4-ffd9-4fae-9207-a9bb12efc18c\",\n  \"readOnly\": false,\n  \"resources\": [\n    {\n      \"accountId\": \"012345679012\",\n      \"type\": \"AWS::Lambda::Function\",\n      \"ARN\": \"arn:aws:lambda:us-east-1:012345679012:function:my-function-name\"\n    }\n  ],\n  \"eventType\": \"AwsApiCall\",\n  \"managementEvent\": false,\n  \"recipientAccountId\": \"012345679012\",\n  \"sharedEventID\": \"675fc334-334b-47a2-b2e0-f831cc51196c\",\n  \"eventCategory\": \"Data\"\n}\n</code></pre>"},{"location":"blog/2023/03/21/lambda-cloudtrail-data-events.html#invokeexecution","title":"InvokeExecution","text":"<pre><code>{\n  \"eventVersion\": \"1.08\",\n  \"userIdentity\": {\n    \"type\": \"AWSService\",\n    \"invokedBy\": \"lambda.amazonaws.com\"\n  },\n  \"eventTime\": \"2023-03-21T05:32:04Z\",\n  \"eventSource\": \"lambda.amazonaws.com\",\n  \"eventName\": \"InvokeExecution\",\n  \"awsRegion\": \"us-east-1\",\n  \"sourceIPAddress\": \"lambda.amazonaws.com\",\n  \"userAgent\": \"lambda.amazonaws.com\",\n  \"requestParameters\": null,\n  \"responseElements\": null,\n  \"additionalEventData\": {\n    \"customerEniId\": \"eni-0fe91c93d7934f8a0\",\n    \"functionVersion\": \"arn:aws:lambda:us-east-1:012345679012:function:my-function-name:14\"\n  },\n  \"requestID\": \"6baa1b22-d95f-4550-8528-2d0e0ea5b845\",\n  \"eventID\": \"904d57aa-6ff8-43e0-9a78-a4ad67c32595\",\n  \"readOnly\": false,\n  \"resources\": [\n    {\n      \"accountId\": \"012345679012\",\n      \"type\": \"AWS::Lambda::Function\",\n      \"ARN\": \"arn:aws:lambda:us-east-1:012345679012:function:my-function-name\"\n    }\n  ],\n  \"eventType\": \"AwsApiCall\",\n  \"managementEvent\": false,\n  \"recipientAccountId\": \"012345679012\",\n  \"sharedEventID\": \"f7d6a59b-1dbe-4114-94db-a8f10ed0a000\",\n  \"eventCategory\": \"Data\"\n}\n</code></pre>"},{"location":"blog/2022/12/15/lambda-extension-environment-variables.html","title":"Lambda extension environment variables","text":"<p>This is really just some context for myself so I don't have to write code to sanity-check myself each time. Here are the environment variables available to an AWS Lambda extension in the <code>provided.al2</code> runtime:</p> Name Value AWS_ACCESS_KEY_ID ASIAY24FZKAOHEXAMPLE AWS_DEFAULT_REGION ap-southeast-2 AWS_LAMBDA_FUNCTION_MEMORY_SIZE 1769 AWS_LAMBDA_FUNCTION_NAME test-Function-Oir9IIuvmE3E AWS_LAMBDA_FUNCTION_VERSION $LATEST AWS_LAMBDA_INITIALIZATION_TYPE on-demand AWS_LAMBDA_RUNTIME_API 127.0.0.1:9001 AWS_REGION ap-southeast-2 AWS_SECRET_ACCESS_KEY Y8fuc8UvsbAO/JEXAMPLE+qEO2lasMzB AWS_SESSION_TOKEN IQoJb3JpZ2lumFwLX...EXAMPLE LANG en_US.UTF-8 LD_LIBRARY_PATH /lib64:/usr/lib64:/var/runtime:/var/runtime/lib:/var/task:/var/task/lib:/opt/lib PATH /usr/local/bin:/usr/bin/:/bin:/opt/bin TZ :UTC <p>In addition to the above, here are the environment variables that the function runtime has:</p> Name Value AWS_LAMBDA_LOG_GROUP_NAME /aws/lambda/test-Function-Oir9IIuvmE3E AWS_LAMBDA_LOG_STREAM_NAME 2022/12/15/[$LATEST]534601edddcb4ad8a23042e2d6042f68 AWS_XRAY_CONTEXT_MISSING LOG_ERROR AWS_XRAY_DAEMON_ADDRESS 169.254.79.129:2000 LAMBDA_RUNTIME_DIR /var/runtime LAMBDA_TASK_ROOT /var/task _AWS_XRAY_DAEMON_ADDRESS 169.254.79.129 _AWS_XRAY_DAEMON_PORT 2000 _HANDLER myhandler"},{"location":"blog/2026/02/21/locking-down-aws-principal-tags-with-rcps-and-scps.html","title":"Locking down AWS principal tags with RCPs and SCPs","text":"<p>AWS principal tags are useful for fine-grained access control. As an organisation administrator, you can craft service control policies (SCPs) that only allow  tagged roles to call sensitive APIs. The problem then becomes: how do you guarantee  that the tags are legitimate? This is where resource control policies (RCPs) come  in handy - I provide a demonstration of them in this blog post, and an example of  what you can achieve with the trustworthy tags in place.</p>"},{"location":"blog/2026/02/21/locking-down-aws-principal-tags-with-rcps-and-scps.html#the-problem","title":"The problem","text":"<p>I'll lay out a scenario. You run a large AWS organisation. You give your  development teams a lot of autonomy: each service gets its own AWS accounts, and the development team for each service effectively has admin-level access in  their respective accounts. This allows them to ship quickly, and not get slowed down by a central IAM team that has to approve (or worse yet: deploy themselves) all IAM changes. </p> <p>This works well, but in practice not every developer on every team is going to  be an AWS expert with years of experience avoiding gotchas. Or they're under time pressure and take shortcuts. Or maybe they just have a lazy AI agent who wants to get the job done quickly and go back to sleep. So you need to lock some things down. Like long-lived credentials.</p> <p>Long-lived credentials are a common audit finding. They're hard to keep track of, but difficult to eliminate entirely. So you want to allow some teams in your  organisation to create access keys, but not all. You might decide to use tags  to do that. Your SCP will look something like this:</p> <pre><code>- Sid: DenyLongTermCredentialCreation\n  Effect: Deny\n  Action:\n    - iam:CreateAccessKey\n    - iam:UpdateAccessKey\n    - iam:CreateLoginProfile\n    - iam:UpdateLoginProfile\n    - iam:CreateServiceSpecificCredential\n  Resource: \"*\"\n  Condition:\n    StringNotEquals:\n      aws:PrincipalTag/scp-exempt-access-keys: \"true\"\n</code></pre> <p>Most principals won't have this tag, so they'll be denied. But out of the box, any of your development teams can add those tags to a role (because you've given them admin access!)</p>"},{"location":"blog/2026/02/21/locking-down-aws-principal-tags-with-rcps-and-scps.html#a-partial-solution","title":"A partial solution","text":"<p>So you decide that only a <code>tagger</code> role can apply those tags. So you roll out a  <code>tagger</code> role everywhere (presumably using CloudFormation service-managed stack  sets), and follow it up with this SCP statement:</p> <pre><code>- Sid: DenyIAMPrincipalTagging\n  Effect: Deny\n  Action:\n    - iam:TagRole\n    - iam:TagUser\n    - iam:UntagRole\n    - iam:UntagUser\n    - iam:CreateRole\n    - iam:CreateUser\n  Resource: \"*\"\n  Condition:\n    ForAnyValue:StringLike:\n      aws:TagKeys: \"scp-*\"\n    StringNotLike:\n      aws:PrincipalArn: arn:aws:iam::*:role/tagger\n</code></pre> <p>Great, now we're a bit closer. Only the organisation administrator can assume the <code>tagger</code> role and hand out <code>scp-exempt-access-keys=true</code> tags to <code>admin</code> roles in member AWS accounts.</p>"},{"location":"blog/2026/02/21/locking-down-aws-principal-tags-with-rcps-and-scps.html#session-tags","title":"Session tags","text":"<p>...but resource tags on roles and users aren't the only place that principal tags can come from. Principal tags are the union of the principal resource tags and  session tags, so you need to lock down session tags too. This is where RCPs  come in. </p> <p>Why RCPs? Because SCPs only apply to principals in your organisation - and session tags can come from outside your organisation. Think cross-account role assumption from SaaS vendors, think OIDC and SAML identity providers. RCPs launched  in November 2024 and solved some of the problems I asked for in a previous  blog post.</p> <p>RCPs provide a way to guarantee that <code>scp-*</code> session tags can only be applied to principals in our organisation by the <code>tagger</code> role. Here's how:</p> <pre><code>- Sid: DenySCPTagsNonTaggerRole\n  Effect: Deny\n  Principal: \"*\"\n  Action: sts:TagSession\n  Resource: \"*\"\n  Condition:\n    ForAnyValue:StringLike:\n      aws:TagKeys: \"scp-*\"\n    StringNotLike:\n      aws:PrincipalArn: arn:aws:iam::*:role/tagger\n- Sid: DenySCPTagsOutsideOrg\n  Effect: Deny\n  Principal: \"*\"\n  Action: sts:TagSession\n  Resource: \"*\"\n  Condition:\n    ForAnyValue:StringLike:\n      aws:TagKeys: \"scp-*\"\n    StringNotEquals:\n      aws:ResourceOrgID: \"${aws:PrincipalOrgID}\"\n</code></pre> <p>The RCP needs two statements. A single combined statement won't work because  conditions within a statement are ANDed, and we need OR logic: deny if the  caller isn't the blessed role, or deny if the caller isn't in our org.</p> <p>The second statement is important: without it, someone with a role named  <code>tagger</code> in an account outside your organisation could bypass the first  statement. <code>StringNotEquals</code> on a missing key evaluates to <code>true</code> for negated  operators, so this also blocks third-party accounts that aren't members of  any org.</p> <p>It might be interesting to note that OIDC and SAML IdPs are actually blocked by the first statement. They have a principal, it's just a federated principal (i.e. the one you see in your trust policy when you allow assumption by an IdP).</p>"},{"location":"blog/2026/02/21/locking-down-aws-principal-tags-with-rcps-and-scps.html#why-two-statements-in-the-rcp","title":"Why two statements in the RCP?","text":"<p>I mentioned this above, but it's worth elaborating. If we combined the two  conditions into a single statement:</p> <pre><code>Condition:\n  StringNotLike:\n    aws:PrincipalArn: \"arn:aws:iam::*:role/tagger\"\n  StringNotEquals:\n    aws:ResourceOrgID: \"${aws:PrincipalOrgID}\"\n</code></pre> <p>Both conditions would need to be true for the deny to fire. This would leave  two gaps:</p> <ul> <li>A <code>tagger</code> role outside your org \u2192 <code>StringNotLike</code> is false, deny    doesn't fire</li> <li>A non-tagger role inside your org \u2192 <code>StringNotEquals</code> is false, deny    doesn't fire</li> </ul> <p>Two separate statements give us OR logic: either mismatch triggers a deny.</p>"},{"location":"blog/2026/02/21/locking-down-aws-principal-tags-with-rcps-and-scps.html#protecting-the-tagger-role","title":"Protecting the tagger role","text":"<p>Keen readers will note that there's nothing in these examples protecting the <code>tagger</code> role. Indeed if we are giving our developers admin-level access, there's nothing stopping them from editing this role so they can assume it themselves (if it exists), or creating it in a way that suits them (if it doesn't). So we need to lock that down too. </p> <p>How exactly you lock this down depends on how you deploy the role in the first place. Despite their significant painpoints, I like using CloudFormation  service-managed stacksets for this. It's an easy way to say \"I want this role to exist in every AWS account in my organisation\" and leave it at that. The stack (and therefore role) will be deployed as soon as a new AWS account is created, which is well-suited for ensuring these security invariants. It also makes the SCP fairly straightforward:</p> <pre><code>- Sid: DenyModifyingPrivilegedRoles\n  Effect: Deny\n  Action:\n    - iam:AttachRolePolicy\n    - iam:CreateRole\n    - iam:DeleteRole\n    - iam:DeleteRolePermissionsBoundary\n    - iam:DeleteRolePolicy\n    - iam:DetachRolePolicy\n    - iam:PutRolePermissionsBoundary\n    - iam:PutRolePolicy\n    - iam:TagRole\n    - iam:UntagRole\n    - iam:UpdateAssumeRolePolicy\n    - iam:UpdateRole\n    - iam:UpdateRoleDescription\n  Resource: \n    - arn:aws:iam::*:role/tagger\n    - arn:aws:iam::*:role/stacksets-exec-*\n  Condition:\n    StringNotLike:\n      aws:PrincipalArn: arn:aws:iam::*:role/stacksets-exec-*\n</code></pre> <p>This prevents any principal other than the stackset execution role from creating or modifying the <code>tagger</code> role. It is also necessary to protect the stackset execution role itself from the same issue. Note that we don't need to exempt the  <code>AWSServiceRoleForCloudFormationStackSetsOrgMember</code> service-linked role, because service-linked roles are not subject to SCPs. </p>"},{"location":"blog/2026/02/21/locking-down-aws-principal-tags-with-rcps-and-scps.html#some-notes","title":"Some notes","text":"<ul> <li>SCPs and RCPs don't apply to service-linked roles. I think that's okay, because   I'm not aware of any service-linked roles that can assume (and provide session   tags for) a role in your account. Let me know if I've missed something!</li> <li>You might be tempted to structure tags in a format like <code>scp-exemption=access-keys</code>.   Technically this works, but I don't like it. I prefer to lock down a tag key   prefix and use a tag key per \"use case\". Example use cases: You might have    teams that are allowed to create access keys and internet gateways, but not    public buckets. Other teams might only be allowed to create public buckets. Each   of these three use cases should have their own tag.</li> <li>This is not a full ABAC strategy. How you might deal with matching tags on    principals and resources is a whole other blog post. Maybe I'll write it one   day (if I can ever figure out a good way to do it.)</li> <li>Thank you to Adam Cotenoff, Stephanie Shi and Santosh Ananthakrishnan for    motivating me to write this post and then helping to improve it.</li> </ul>"},{"location":"blog/2021/10/12/nested-express-step-functions.html","title":"Nested Express Step Functions","text":""},{"location":"blog/2021/10/12/nested-express-step-functions.html#history","title":"History","text":"<p>AWS Step Functions are cool. Express Step Functions have always felt like they had the potential to be cool, but were missing some key features when they launched. I think they're worth revisiting in light of recent releases. A timeline:</p> <ul> <li> <p>December 2019: Express workflows launched. </p> </li> <li> <p>August 2020: <code>ResultSelector</code> and instrinic functions launched.</p> </li> <li> <p>November 2020: Synchronous express workflows launched.</p> </li> <li> <p>September 2021: Arbitrary AWS SDK invocations launched.</p> </li> </ul> <p>These are the key launches that make it possible to have an express workflow invoke another express workflow.</p>"},{"location":"blog/2021/10/12/nested-express-step-functions.html#what-about-the-existing-feature","title":"What about the existing feature?","text":"<p>In May 2020, SFN released <code>arn:aws:states:::states:startExecution.sync:2</code>. This was a welcome improvement over the original (launched in August 2019) as the JSON output is parsed. But it only works when invoked by a standard workflow, because <code>.sync</code> isn't support for express workflows. And even from a standard workflow, it can't synchronously invoke an express step function because that is a different API: <code>StartSyncExecution</code> vs <code>StartExecution</code>. EDIT: See addendum.</p>"},{"location":"blog/2021/10/12/nested-express-step-functions.html#just-show-me","title":"Just show me","text":"<pre><code>Resources:\n  Parent:\n    Type: AWS::StepFunctions::StateMachine\n    Properties:\n      StateMachineType: EXPRESS\n      RoleArn: !GetAtt ParentRole.Arn\n      Definition:\n        StartAt: Example sync step\n        States:\n          Example sync step:\n            Type: Task\n            End: true\n            Parameters:\n              StateMachineArn: !Ref Child\n              Input.$: $\n            Resource: arn:aws:states:::aws-sdk:sfn:startSyncExecution\n            ResultPath: $.ChildOutput\n            OutputPath: $.ChildOutput\n            ResultSelector:\n              Output.$: States.StringToJson($.Output)\n\n  Child:\n    Type: AWS::StepFunctions::StateMachine\n    Properties:\n      StateMachineType: EXPRESS\n      RoleArn: !GetAtt ChildRole.Arn\n      Definition:\n        StartAt: Hello\n        States:\n          Hello:\n            Type: Pass\n            End: true\n            Result:\n              Hello: World\n\n  ParentRole:\n    Type: AWS::IAM::Role\n    Properties:\n      AssumeRolePolicyDocument:\n        Version: \"2012-10-17\"\n        Statement:\n          - Effect: Allow\n            Action: sts:AssumeRole\n            Principal:\n              Service: states.amazonaws.com\n      Policies:\n        - PolicyName: AllowStartChild\n          PolicyDocument:\n            Version: \"2012-10-17\"\n            Statement:\n              - Effect: Allow\n                Action: states:StartSyncExecution\n                Resource: !Ref Child\n\n  ChildRole:\n    Type: AWS::IAM::Role\n    Properties:\n      AssumeRolePolicyDocument:\n        Version: \"2012-10-17\"\n        Statement:\n          - Effect: Allow\n            Action: sts:AssumeRole\n            Principal:\n              Service: states.amazonaws.com\n      Policies: []\n</code></pre> <p>It's not a very useful couple of step functions, but it gets the point across. The parent state machine invokes the child state machine and then is able to process the child's output as normal.</p>"},{"location":"blog/2021/10/12/nested-express-step-functions.html#addendum","title":"Addendum","text":"<p>I made a mistake in the first version of this blog post. Calling an express  workflow synchronously from a standard workflow actually works just fine. In fact, there's even a sample of this in the AWS docs and they call it \"selective checkpointing\". But if you use the <code>...:sync:2</code> approach from a standard workflow, the parent IAM role instead needs to be:</p> <pre><code>  ParentRole:\n    Type: AWS::IAM::Role\n    Properties:\n      AssumeRolePolicyDocument:\n        Version: \"2012-10-17\"\n        Statement:\n          - Effect: Allow\n            Action: sts:AssumeRole\n            Principal:\n              Service: states.amazonaws.com\n      Policies:\n        - PolicyName: AllowStartChild\n          PolicyDocument:\n            Version: \"2012-10-17\"\n            Statement:\n              - Effect: Allow\n                Action: states:StartExecution\n                Resource: !Ref Child\n              - Effect: Allow\n                Action:\n                  - states:DescribeExecution\n                  - states:StopExecution\n                Resource: !Sub arn:aws:states:${AWS::Region}:${AWS::AccountId}:execution:${Child.Name}\n              - Effect: Allow\n                Action:\n                  - events:PutTargets\n                  - events:PutRule\n                  - events:DescribeRule\n                Resource: !Sub arn:aws:events:${AWS::Region}:${AWS::AccountId}:rule/StepFunctionsGetEventsForStepFunctionsExecutionRule\n</code></pre>"},{"location":"blog/2020/11/02/nitro-enclaves-first-impressions.html","title":"Nitro Enclaves - First Impressions","text":"<p>At the end of October, AWS released Nitro Enclaves. My mental model of these is essentially a secure virtual machine within a virtual machine - the outer VM being an EC2 instance. The secure qualifier is to distinguish that the inner VM has many restrictions: by default it has no network access, no persistent disk, no access to processes running on the host and crucially, vice-versa: the host likewise doesn't have access to resources inside the enclave. All communication instead happens over <code>vsock</code> sockets.</p> <p>This is all pretty cool, but what really piqued my interest is the integrations that enclaves have with AWS KMS cryptography and AWS-certified attestations of enclave identity and integrity. In other words, you can write AWS KMS key policies that ensure that only signed and unmodified code can decrypt or encrypt particular data. It also means that code running in an enclave can attest (prove) to third parties that it is running on a particular EC2 instance and that the code has not been tampered with.</p>"},{"location":"blog/2020/11/02/nitro-enclaves-first-impressions.html#shenanigans-for-self-education","title":"Shenanigans for self-education","text":"<p>The way I usually learn new things is by using them, changing them, breaking them, etc. In the case of Nitro Enclaves, the first thing I did was write an Enclave that allowed me to SSH into it over the <code>vsock</code> socket (instead of a regular TCP socket on port 22) as per a silly tweet:</p> <p></p> <p>This was fun, but pretty pointless. I was much more interested in the cryptographic attestations that the Nitro hypervisor would provide on behalf on enclaves. The attestation process is documented in great detail on GitHub. In fact, a good deal of the user-facing implementation details of Nitro Enclaves are open source and provided on GitHub. This is really great work by AWS.</p> <p>The attestation process works by sending some optional data (a public key, a nonce and arbitrary user data) to the hypervisor via the <code>/dev/nsm</code> special file and getting a CBOR-encoded document back. The hairy implementation details of this are abstracted by the helpful <code>nsm_get_attestation_doc</code> function in the NSM (Nitro Secure Module) library. The library is written in Rust, but consumable from almost any language thanks to it exporting a C ABI.</p> <p>This attestation document includes a handful of fields in one form or another. These are:</p> <ul> <li>The EC2 instance ID</li> <li>The enclave ID</li> <li>The SHA-384 digest of the Nitro enclave <code>.eif</code> file</li> <li>The optional user-provided public key, nonce and user data</li> <li>A chain of X.509 certificates from this enclave all the way to the global Nitro root</li> </ul> <p>The document itself is signed by the leaf certificate, which is in turn signed by the first intermediary, etc until you reach the global root Nitro certificate. That root certificate is documented by AWS. Hell, here it is:</p> <pre><code>-----BEGIN CERTIFICATE-----\nMIICETCCAZagAwIBAgIRAPkxdWgbkK/hHUbMtOTn+FYwCgYIKoZIzj0EAwMwSTEL\nMAkGA1UEBhMCVVMxDzANBgNVBAoMBkFtYXpvbjEMMAoGA1UECwwDQVdTMRswGQYD\nVQQDDBJhd3Mubml0cm8tZW5jbGF2ZXMwHhcNMTkxMDI4MTMyODA1WhcNNDkxMDI4\nMTQyODA1WjBJMQswCQYDVQQGEwJVUzEPMA0GA1UECgwGQW1hem9uMQwwCgYDVQQL\nDANBV1MxGzAZBgNVBAMMEmF3cy5uaXRyby1lbmNsYXZlczB2MBAGByqGSM49AgEG\nBSuBBAAiA2IABPwCVOumCMHzaHDimtqQvkY4MpJzbolL//Zy2YlES1BR5TSksfbb\n48C8WBoyt7F2Bw7eEtaaP+ohG2bnUs990d0JX28TcPQXCEPZ3BABIeTPYwEoCWZE\nh8l5YoQwTcU/9KNCMEAwDwYDVR0TAQH/BAUwAwEB/zAdBgNVHQ4EFgQUkCW1DdkF\nR+eWw5b6cp3PmanfS5YwDgYDVR0PAQH/BAQDAgGGMAoGCCqGSM49BAMDA2kAMGYC\nMQCjfy+Rocm9Xue4YnwWmNJVA44fA0P5W2OpYow9OYCVRaEevL8uO1XYru5xtMPW\nrfMCMQCi85sWBbJwKKXdS6BptQFuZbT73o/gBh1qUxl/nNr12UO8Yfwr6wPLb+6N\nIwLz3/Y=\n-----END CERTIFICATE-----\n</code></pre> <p>This means that code in an enclave can request a signed copy of any arbitrary data, share it with the outside world and prove that it came from an untampered app in a specific Enclave.</p>"},{"location":"blog/2020/11/02/nitro-enclaves-first-impressions.html#what-can-we-do-with-that-proof","title":"What can we do with that proof?","text":"<p>There are lots of interesting things you can do when you can prove who you are and that it was you who authored some data. I decided that I would create a service that would vend AWS IAM role session credentials to code running in enclaves. This might actually be useful, because for the time being AWS doesn't provide a native way to assign a role to an enclave. Here's a reasonably detailed sequence diagram:</p> <p></p> <p>For roles that you want to be assumable by enclaves, you would then have a trust policy that looks something like this:</p> <pre><code>{\n  \"Version\": \"2012-10-17\",\n  \"Statement\": [\n    {\n      \"Sid\": \"AllowIamUserAssumeRole\",\n      \"Effect\": \"Allow\",\n      \"Action\": \"sts:AssumeRole\",\n      \"Principal\": {\n        \"AWS\": \"arn:aws:iam::123456789012:role/idp-role\"\n      },\n      \"Condition\": {\n        \"StringLike\": {\n          \"aws:RequestTag/enclave:enclave-id\": \"*\",\n          \"aws:RequestTag/enclave:instance-id\": \"*\",\n          \"aws:RequestTag/enclave:account-id\": \"*\",\n          \"aws:RequestTag/enclave:instance-role-arn\": \"*\"\n        }\n      }\n    },\n    {\n      \"Sid\": \"AllowPassSessionTagsAndTransitive\",\n      \"Effect\": \"Allow\",\n      \"Action\": \"sts:TagSession\",\n      \"Principal\": {\n        \"AWS\": \"arn:aws:iam::123456789012:role/idp-role\"\n      },\n      \"Condition\": {\n        \"StringLike\": {\n          \"aws:RequestTag/enclave:enclave-id\": \"*\",\n          \"aws:RequestTag/enclave:instance-id\": \"*\"\n        },\n        \"StringEquals\": {\n          \"aws:RequestTag/enclave:account-id\": \"123456789012\",\n          \"aws:RequestTag/enclave:instance-role-arn\": \"arn:aws:iam::123456789012:role/instance-role\"\n        }\n      }\n    }\n  ]\n}\n</code></pre> <p>I've put a steaming pile of code on GitHub - please don't use it. It probably has a line-to-bug ratio of 1:1.</p>"},{"location":"blog/2021/11/06/no-need-for-aws-iam-users.html","title":"No need for AWS IAM users","text":"<p>It's long been considered \"best practice\" to avoid having IAM users in AWS. Where possible IAM roles are preferable as role session credentials are  short-lived. As far as I can tell, the only justification for AWS IAM users that  I hear nowadays is for usage on non-interactive systems outside of AWS (so AWS SSO won't work), e.g. a Raspberry Pi in your closet.</p> <p>I created a proof-of-concept project <code>cloudkey</code> to show that even that  scenario can avoid IAM users. It uses the little-known <code>iot:AssumeRoleWithCertificate</code> functionality to avoid that.</p> <p>Specifically, it uses the \"card authentication\" slot on a Yubikey to store a TLS certificate and private key. This slot can be used to sign requests without a PIN or touch - perfect for the Raspberry Pi use case. By making a <code>credential_process</code> app for it, it works with any AWS SDK or AWS CLI from the last few years.</p> <p>This could also be made to work with TPMs for deployments where having a  removeable Yubikey is undesirable.</p> <p>I'd love to hear from you if you can think of any remaining reasons why IAM users are still necessary.</p>"},{"location":"blog/2022/07/14/openrolesanywhere-roles-anywhere.html","title":"openrolesanywhere - an IAM Roles Anywhere client","text":"<p>Update: AWS now has an open source implementation of a Roles Anywhere <code>credential_process</code> provider - and it even supports PKCS#11. I'll  keep the following project online for historical purposes, but there's not much need for it.</p> <p>I just published a proof-of-concept CLI tool named <code>openrolesanywhere</code>. It lets you assume a role in AWS using IAM Roles Anywhere and a private key  stored in your SSH agent - rather than on-disk as required by the official client. It implements <code>AWS4-X509-RSA-SHA256</code>, <code>AWS4-X509-ECDSA-SHA256</code> via a forked copy of the SigV4 signer in the AWS SDK for Go.</p> <p>Check out the repo for more details.</p>"},{"location":"blog/2025/10/26/querying-terraform-state-with-aws-athena.html","title":"Querying Terraform state with AWS Athena","text":"<p>Athena is one of my favourite AWS services. Though it's marketed as a big data service, it is useful in many other scenarios. Sometimes I use it as a \"grep through unstructured logs in S3\" and other times I use it to query CloudTrail  logs - but this latter use case is likely better served by CloudTrail Lake  nowadays. Today, I'll show how it can be used for querying Terraform state stored in S3.</p> <p>S3 is probably the most common place for storing Terraform state (source: I made it up). A common pattern is to store all state for all stacks across an AWS organisation in a single bucket. This makes it easy for a central ops/security team to lock down and audit access to the bucket, ensure it is backed up  correctly, etc. Sometimes those central teams have questions like \"what providers are developers using?\" or \"how many instances of <code>aws_s3_bucket_versioning</code> are deployed across my org?\" Those questions can be easily answered via Athena queries against that central bucket.</p> <p>Step one is creating a table in Athena. Note that this doesn't actually write  any data to S3, it's just metadata that Athena uses to locate and parse the data it finds in an S3 bucket. </p> <p>The table can be created using this Athena query: <pre><code>CREATE EXTERNAL TABLE terraform_state(\n  version int, \n  terraform_version string, \n  serial int, \n  lineage string, \n  outputs map&lt;string,string&gt;, \n  resources array&lt;\n    struct&lt;\n      module:string,\n      mode:string,\n      type:string,\n      name:string,\n      provider:string,\n      instances:array&lt;\n        struct&lt;\n          schema_version:int,\n          attributes:string,\n          identity_schema_version:int,\n          private:string,\n          dependencies:array&lt;string&gt;\n        &gt;\n      &gt;\n    &gt;\n  &gt;\n)\nSTORED AS ION\nLOCATION 's3://your-s3-bucket-name-here/'\n</code></pre></p> <p>Note that we use the Amazon Ion serde. This is because Terraform state files are pretty-printed JSON - which is a subset of valid Ion files. The other JSON-specific serdes in Athena don't support pretty-printed (multi-line) JSON.</p> <p>Now the fun part: querying it. First, a basic query to demonstrate the kind of data we're working with:</p> <pre><code>SELECT \n\"$path\",\nsplit(\"$path\", '/')[4] as accountId,\nsplit(\"$path\", '/')[5] as stack,\nterraform_version,\noutputs,\nr.module,\nr.mode,\nr.type,\nr.name,\nr.provider,\ni.attributes,\ni.dependencies\nFROM terraform_state\nCROSS JOIN unnest(resources) AS _(r)\nCROSS JOIN unnest(r.instances) AS _(i)\n</code></pre> <p>Some points to note, in no particular order:</p> <ul> <li>The path to my Terraform state across my org is always    <code>${accountId}/${stack}/terraform.tfstate</code>. If yours isn't, you can delete the   second and third columns.</li> <li>The cross joins exist because Terraform state files exist as nested JSON arrays.   I mostly care about instances of Terraform resources, and I suspect you do too.</li> <li>There are some fields declared in the <code>CREATE TABLE</code> that I didn't reference   here. There's a small chance you might find them useful, but I didn't.</li> </ul> <p>Now what kind of useful queries can we write with this? That's where I'm hoping others can pitch in: please get in touch via Twitter, Bluesky, Slack, etc and let me know what you come up with.  </p> <p>Here's one to get started: find every instance where you're one refactor away from a bad time:</p> <pre><code>SELECT \nsplit(\"$path\", '/')[4] as accountId,\nsplit(\"$path\", '/')[5] as stack,\nr.module,\nr.name\nFROM terraform_state\ncross join unnest(resources) as _(r)\ncross join unnest(r.instances) as _(i)\nwhere r.type = 'aws_iam_policy_attachment'\n</code></pre>"},{"location":"blog/2023/11/19/reversing-aws-iam-unique-ids.html","title":"Reversing AWS IAM unique IDs","text":"<p>A few years ago, I wrote about determining AWS account IDs from AWS  access keys, i.e. those strings that begin with <code>AKIA</code> or <code>ASIA</code>. It's also  possible to determine information from other AWS IAM unique IDs, specifically  these two from the table in Amazon's docs.</p> <p></p> <p>These unique IDs can pop up in a few places, but the place I see them most often is in CloudTrail logs when a principal in a different AWS account accesses a  resource (like a KMS key or S3 bucket) in my account. In these situations, I often want to know the ARN of the user/role, because it's easier to understand my logs. As far as I know, the process for doing this is not documented and not well-known, so here's a blog post on how to do it:</p> <ol> <li>You have a unique ID. Let's say it's <code>AROAJMD24IEMKTX6BABJI</code> (that's a real ID    you can use to follow this blog post)</li> <li>Create or find an unused S3 bucket that you control. Add the following to its    bucket policy:    <pre><code>{\n  \"Version\": \"2012-10-17\",\n  \"Statement\": [\n    {\n      \"Principal\": {\n        \"AWS\": \"AROAJMD24IEMKTX6BABJI\"\n      },\n      \"Effect\": \"Deny\",\n      \"Action\": [\"s3:GetObject\"],\n      \"Resource\": \"arn:aws:s3:::YOUR_BUCKET_NAME_HERE/*\"\n    }\n  ]\n}\n</code></pre></li> <li>Save the bucket policy.</li> <li>Now view the bucket policy. You'll see that AWS has automatically resolved     the unique ID to the ARN of a role in my personal account:    <pre><code>{\n  \"Version\": \"2012-10-17\",\n  \"Statement\": [\n    {\n      \"Principal\": {\n        \"AWS\": \"arn:aws:iam::607481581596:role/service-role/abctestrole\"\n      },\n      \"Effect\": \"Deny\",\n      \"Action\": [\"s3:GetObject\"],\n      \"Resource\": \"arn:aws:s3:::YOUR_BUCKET_NAME_HERE/*\"\n    }\n  ]\n}\n</code></pre></li> </ol> <p>There are a few things to know about this:</p> <ul> <li>Unique IDs only get resolved if they're in a <code>Principal</code> part of an AWS IAM    policy statement. They won't get affected if they're elsewhere, e.g. a <code>Condition</code>.</li> <li>It doesn't work for principals that have been deleted. If you try use a unique ID   of a deleted principal, you get an <code>Invalid principal in policy</code> error when    you try to save the bucket policy.</li> <li>If you save this bucket policy on day 1 and I delete the role in my account on   day 2, then when you look at the bucket policy again on day 3 it will revert   to showing you the original unique ID. This is how you know I've deleted the   role. Even if I recreate a role with the same name, it won't reappear - because   the new role gets a new unique ID (this is documented).</li> </ul>"},{"location":"blog/2020/09/22/security-september-cataclysms-in-the-cloud-formations.html","title":"Security September: Cataclysms in the Cloud Formations","text":"<p>Security September: Cataclysms in the Cloud Formations was a guest post on Ian Mckay's blog.</p>"},{"location":"blog/2020/09/08/security-september-escaping-codebuild-the-compromise-that-wasnt.html","title":"Security September: Escaping CodeBuild - The compromise that wasn't","text":"<p>Security September: Escaping CodeBuild - The compromise that wasn't was a guest post on Ian Mckay's blog.</p>"},{"location":"blog/2022/01/02/shared-vpcs-are-underrated.html","title":"Shared VPCs are underrated","text":"<p>AWS launched VPC sharing in January 2019, two years ago. It feels to me that there hasn't been much chatter about it since then. Which is a shame, because I suspect they are quite useful. I'm going to focus on cost and security.</p> <p>Recommended practice nowadays is to have a \"multi-account strategy\" in AWS. You typically have an account per [app, environment]. So if you have three apps and two environments (e.g. dev and prod) that would be at least six AWS accounts. Lets also assume that you want to maintain high availability, so you have those apps deployed across three availability zones. And lets assume that the apps are deployed in private subnets (i.e. without access to an Internet Gateway) to appease auditors.</p>"},{"location":"blog/2022/01/02/shared-vpcs-are-underrated.html#cost","title":"Cost","text":"<p>With a VPC per AWS account, you have a fixed monthly cost of $250+ per month per app - see the boring details below for a rationale if you care. I've found this to have a real chilling effect on people's motivation to spin up new accounts -- and therefore new apps. You might say that's silly, but it's real and it has a real (negative, IMO) impact on system architecture.</p> <p>Or you can use VPC sharing and that $250+/month is fixed and doesn't scale with your number of applications. This means you free up your developers from the silly mental burden of \"does this warrant the cost?\" questions that are a waste of time.</p>"},{"location":"blog/2022/01/02/shared-vpcs-are-underrated.html#security","title":"Security","text":"<p>I think VPC sharing can have a significant positive impact on cloud security. Take the following Twitter exchange between Houston Hopkins and Nick Frichette. </p> <p></p> <p>They're both right. AWS gives us all these tools to lock things down but they're almost never used in practice. Think about it: in a VPC-per-account world, how  are you meant to use the <code>aws:SourceVpc</code> IAM policy condition? You can't apply it an an org or OU level via service control policies (SCPs) as every account  has different VPC IDs. Same with the <code>aws:SourceVpce</code> condition. You could  have a standard IAM permission boundary in every account, but then  it's on your developers to use that boundary on every role in those accounts -  likely to get pushback.</p> <p>But what if our org architecture looked like this?</p> <p></p> <p>Then we could apply an SCP to the prod OU that looks something like:</p> <pre><code>{\n    \"Version\": \"2012-10-17\",\n    \"Statement\": [\n        {\n            \"Sid\": \"DenyFromOutsideVpcUsingEndpoints\",\n            \"Effect\": \"Deny\",\n            \"Action\": \"*\",\n            \"Resource\": \"*\",\n            \"Condition\": {\n                \"Bool\": {\n                    \"aws:ViaAWSService\": \"false\"\n                },\n                \"Null\": {\n                    \"aws:SourceIp\": \"true\"\n                },\n                \"StringNotEquals\": {\n                    \"aws:SourceVpc\": \"vpc-08abc123\",\n                    \"aws:PrincipalTag/VpcLimited\": \"false\"\n                }\n            }\n        },\n        {\n            \"Sid\": \"DenyFromOutsideVpcNotUsingEndpoints\",\n            \"Effect\": \"Deny\",\n            \"Action\": \"*\",\n            \"Resource\": \"*\",\n            \"Condition\": {\n                \"Bool\": {\n                    \"aws:ViaAWSService\": \"false\"\n                },\n                \"Null\": {\n                    \"aws:SourceVpc\": \"true\"\n                },\n                \"NotIpAddress\": {\n                    \"aws:SourceIp\": [\"1.2.3.1/32\", \"1.2.3.2/32\", \"1.2.3.3/32\"]\n                },\n                \"StringNotEquals\": {\n                    \"aws:PrincipalTag/VpcLimited\": \"false\"\n                }\n            }\n        }\n    ]\n}\n</code></pre> <p>For roles and users within the prod OU, this would require AWS API calls to be  made either through a VPC endpoint (for services where those have been  configured) or via the elastic IP addresses associated with the NAT gateway. In  a sufficiently complex account,  there are likely going to need to be exceptions -  and for those IAM roles you  can add a <code>VpcLimited = false</code> tag. </p>"},{"location":"blog/2022/01/02/shared-vpcs-are-underrated.html#wrap-up","title":"Wrap up","text":"<p>So there you have it: VPC sharing can improve security posture and reduce costs at the same time. Or at least it feels that way to me. I feel like I could be  missing something as I'm yet to see anyone talk about using this pattern. </p> <p>I'd be keen to hear from folks who think this isn't feasible, please reach out  to  me on Twitter. EDIT: My gratitude to Sean McLaughlin  who did exactly that. I've amended the SCP to account for non-endpoint use and  reworded to (hopefully) clarify.</p>"},{"location":"blog/2022/01/02/shared-vpcs-are-underrated.html#the-boring-details","title":"The boring details","text":"<p>NAT Gateways are $0.045 per hour. They are also AZ-specific. So that is 3  availability zones * 730 hours in a month * \\(0.045/hr = **\\)99 per month** per VPC.</p> <p>Lets say you also use VPC interface endpoints for various AWS services. Those  are \\(0.01 per hour per availability zone = **\\)22 per month per service** per VPC.  That adds up quickly if your app uses a handful of services. SQS, SNS, KMS,  STS, X-Ray, ECR, ECS are a reasonable example for a modern containerised app.</p> <p>Maybe the above is all too much, so you instead decide to centralise things using AWS Transit Gateway. That is \\(0.05 per hour = **\\)37 per month** per VPC. And a few hundred thousand more per year for the network  engineers that can actually understand it.</p>"},{"location":"blog/2024/08/05/surprising-behaviour-in-aws-web-console-session-duration.html","title":"Surprising behaviour in AWS web console session duration","text":"<p>Credentials for AWS IAM role sessions are short-lived. By default, they last for one hour. When calling <code>AssumeRole</code> you can request a different duration by  passing a value between <code>900</code> (15 minutes) and <code>43200</code> (12 hours) in the  <code>DurationSeconds</code> parameter. Note that this API call will fail if you request a session duration longer than is configured on the role itself (in the \"max sesson duration\" property). These credentials can be used by the AWS CLI and AWS SDKs.</p> <p>You can also use these credentials to log into the AWS web console. You do this by calling the <code>GetSigninToken</code> and <code>Login</code> endpoints of the AWS federation API. AWS provides this documentation on how to do that. The first of these endpoints (<code>GetSigninToken</code>) allows you to pass an optional <code>SessionDuration</code> parameter. This acts as you might expect: it defines how long the web console session will remain valid. What surprised me: you can start a 12 hour web console session for a role that has a max session duration of 1 hour. The web console session will outlive the credentials that were used to create it. The closest I could find to documentation of this behaviour is this line:</p> <p>The ability to create a console session that is longer than one hour is  intrinsic to the <code>getSigninToken</code> operation of the federation endpoint.</p> <p>That doesn't feel explicit enough to me. It would be nice if the docs included a parenthetical like (even if the role's max session duration is only one hour)</p> <p>Other things that surprised me when I was digging into this:</p> <p>CloudTrail will log a call to this endpoint (the event name is  <code>GetSigninToken</code>) but it doesn't log the requested <code>SessionDuration</code>. That feels  like useful info to log: I'd like to know how often people in my organisation  are using this.</p> <p>Once you have a 12 hour console session, you can extract credentials that are usable in your terminal. Simply open CloudShell and run this command: </p> <pre><code>aws configure export-credentials --format env\n</code></pre> <p>This will print a string that can be pasted directly into your local terminal. The credentials are short-lived (about 10-15 minutes), but you can keep repeating this for the full 12 hours. I learned this command from this article on hackingthe.cloud. </p> <p>This probably doesn't count as a security issue per se (because no one has access to things they shouldn't have access to), but it might be concerning if your environment relies on an assumption that role sessions are extremely short-lived.</p>"},{"location":"blog/2021/10/29/two-approaches-to-cross-account-eventbridge.html","title":"Two approaches to cross-account EventBridge","text":"<p>Since November 2020, EventBridge has supported two methods of creating cross-account event subscriptions. Both require a level of indirection in the  form of an \"intermediary\" bus in the subscriber's account. Here are examples of how they work. We will refer to the account whose bus has interesting events as  account B. We will refer to the subscriber account as account S.</p>"},{"location":"blog/2021/10/29/two-approaches-to-cross-account-eventbridge.html#the-templates","title":"The templates","text":""},{"location":"blog/2021/10/29/two-approaches-to-cross-account-eventbridge.html#scenario-1-rule-created-by-account-b-bus-owner","title":"Scenario 1: Rule created by account B (bus owner)","text":"<p>This is the method that existed prior to the features released in November 2020. Account B has the following resources:</p> <pre><code>Resources:\n  Bus:\n    Type: AWS::Events::EventBus\n    Properties:\n      Name: publisher-bus\n\n  ForwardToSubscriberBus:\n    Type: AWS::Events::Rule\n    Properties:\n      EventBusName: !Ref Bus\n      EventPattern:\n        detail-type: [some-detail-type]\n      Targets:\n        - Id: subscriber-bus\n          Arn: !Sub arn:aws:events:${AWS::Region}:${Account-S-Id}:event-bus/subscriber-bus\n          RoleArn: !GetAtt ForwarderRole.Arn      \n\n  ForwarderRole:\n    Type: AWS::IAM::Role\n    Properties:\n      AssumeRolePolicyDocument:\n        Version: \"2012-10-17\"\n        Statement:\n          - Effect: Allow\n            Action: sts:AssumeRole\n            Principal:\n              Service: events.amazonaws.com\n      Policies:\n        - PolicyName: ForwardEventsToSubscriberBus\n          PolicyDocument:\n            Version: \"2012-10-17\"\n            Statement:\n              - Effect: Allow\n                Action: events:PutEvents\n                Resource: !Sub arn:aws:events:${AWS::Region}:${Account-S-Id}:event-bus/subscriber-bus\n</code></pre> <p>And account S has these resources: </p> <pre><code>Resources:\n  Bus:\n    Type: AWS::Events::EventBus\n    Properties:\n      Name: subscriber-bus\n\n  EnqueueRule:\n    Type: AWS::Events::Rule\n    Properties:\n      EventBusName: !Ref Bus\n      EventPattern:\n        detail-type: [some-detail-type]\n      Targets:\n        - Id: my-queue\n          Arn: !GetAtt Queue.Arn    \n\n  Queue:\n    Type: AWS::SQS::Queue\n\n  QueuePolicy:\n    Type: AWS::SQS::QueuePolicy\n    Properties:\n      Queues: [!Ref Queue]\n      PolicyDocument:\n        Version: \"2012-10-17\"\n        Statement:\n          - Effect: Allow\n            Action: sqs:SendMessage\n            Resource: !GetAtt Queue.Arn\n            Principal:\n              Service: events.amazonaws.com\n            Condition:\n              ArnLike:\n                aws:SourceArn: !GetAtt EnqueueRule.Arn    \n</code></pre>"},{"location":"blog/2021/10/29/two-approaches-to-cross-account-eventbridge.html#scenario-2-rule-created-by-account-s-target-owner","title":"Scenario 2: Rule created by account S (target owner)","text":"<p>This is the new method that exists as of November 2020. Account B has the following resources:</p> <pre><code>Resources:\n  Bus:\n    Type: AWS::Events::EventBus\n    Properties:\n      Name: publisher-bus\n\n  BusPolicy:\n    Type: AWS::Events::EventBusPolicy\n    Properties:\n      EventBusName: !Ref Bus\n      StatementId: AllowAnyAccountInOrgToCreateRules\n      Statement:\n        Effect: Allow\n        Action: \n          - events:PutRule\n          - events:DeleteRule\n          - events:DescribeRule\n          - events:DisableRule\n          - events:EnableRule\n          - events:PutTargets\n          - events:RemoveTargets\n        Resource: !Sub arn:aws:events:${AWS::Region}:${AWS::AccountId}:rule/${Bus}/*\n        Principal: \"*\" # this is ok because of the aws:PrincipalOrgID condition below\n        Condition:\n          StringEquals:\n            aws:PrincipalOrgID: o-yourorgid\n          StringEqualsIfExists:\n            events:creatorAccount: \"${aws:PrincipalAccount}\"\n</code></pre> <p>And account S has these resources: </p> <pre><code>Resources:\n  # only these first two new resources are different to the previous scenario\n  RemoteRule:\n    Type: AWS::Events::Rule\n    Properties:\n      EventBusName: !Sub arn:aws:events:${AWS::Region}:${Account-B-Id}:event-bus/publisher-bus\n      EventPattern:\n        detail-type: [some-detail-type]\n      Targets:\n        - Id: local-bus\n          Arn: !GetAtt Bus.Arn\n          RoleArn: !GetAtt CrossAccountEventBridgeRole.Arn\n\n  CrossAccountEventBridgeRole:\n    Type: AWS::IAM::Role\n    Properties:\n      AssumeRolePolicyDocument:\n        Version: \"2012-10-17\"\n        Statement:\n          - Effect: Allow\n            Action: sts:AssumeRole\n            Principal:\n              Service: events.amazonaws.com\n      Policies:\n        - PolicyName: PutEventsOnLocalBus\n          PolicyDocument:\n            Version: \"2012-10-17\"\n            Statement:\n              - Effect: Allow\n                Action: events:PutEvents\n                Resource: !GetAtt Bus.Arn\n\n  # everything below this line is the same as in the previous scenario\n  Bus:\n    Type: AWS::Events::EventBus\n    Properties:\n      Name: subscriber-bus\n\n  EnqueueRule:\n    Type: AWS::Events::Rule\n    Properties:\n      EventBusName: !Ref Bus\n      EventPattern:\n        detail-type: [some-detail-type]\n      Targets:\n        - Id: my-queue\n          Arn: !GetAtt Queue.Arn    \n\n  Queue:\n    Type: AWS::SQS::Queue\n\n  QueuePolicy:\n    Type: AWS::SQS::QueuePolicy\n    Properties:\n      Queues: [!Ref Queue]\n      PolicyDocument:\n        Version: \"2012-10-17\"\n        Statement:\n          - Effect: Allow\n            Action: sqs:SendMessage\n            Resource: !GetAtt Queue.Arn\n            Principal:\n              Service: events.amazonaws.com\n            Condition:\n              ArnLike:\n                aws:SourceArn: !GetAtt EnqueueRule.Arn    \n</code></pre>"},{"location":"blog/2021/10/29/two-approaches-to-cross-account-eventbridge.html#discussion","title":"Discussion","text":"<p>Both approaches work equally well. Which one is best for you depends on how your organisation works. </p> <p>In the first scenario, where the bus owner creates the rules, the event publisher  needs to know the ARNs of every subscriber's event bus - but the subscriber doesn't need to know where the events are coming from.</p> <p>In the second scenario, the bus owner doesn't need to know anything about its subscribers - it just sends events to its local bus. But the subscribers need to know the ARN of the bus that events are being published to.</p>"},{"location":"blog/2023/08/02/useful-flags-for-go-lambda-functions.html","title":"Useful flags for Go Lambda functions","text":"<p>Last week AWS published a blog post advising that the <code>go1.x</code> Lambda runtime will be deprecated and people should migrate to <code>provided.al2</code>. I was  already using the newer runtime, but I also learned from the blog post that AWS SAM can now build Go Lambda functions for the newer runtime - no more Makefiles required!</p> <p>I switched from <code>BuildMethod: makefile</code> to <code>BuildMethod: go1.x</code> and noticed that my Lambda packages were now twice the size. This means slower cold starts and  slower deployments - especially from my laptop in Australia. I also noticed that  my CI pipelines were slower because every commit was causing Lambda updates, even when no code had changed. </p> <p>The cause of all these issues was the set of build-time flags. My Makefile was setting them, but the AWS SAM builder defaults to no flags. The problem can be addressed by setting the following environment variable  while building (i.e. in your CI system):</p> <pre><code>export GOFLAGS=\"-buildvcs=false -trimpath '-ldflags=-s -w -buildid='\"\n</code></pre> <p>Setting this environment variable halved the size of my Lambda packages and caused update churn to go away, meaning faster CI. Breaking down the flags:</p> <p>Three of the flags relate to deterministic builds. <code>-buildvcs=false</code> causes Go  to not embed the Git commit hash and date in the built binaries. This can be  useful for distributed tools, but causes every commit to generate unique Lambda  package ZIPs - not what I wanted. The linker flag <code>-buildid=</code> is the same: it  tells Go not to embed a unique hash in the binary. <code>-trimpath</code> is similar: it  tells Go not to embed the absolute paths of packages in the binary. This means  deploying from my laptop (where the code lives under <code>/Users/aidan/dev/...</code>)  and from CI (where it could be anything!) will produce the same binaries.</p> <p>The other two flags relate to binary size. The <code>-s</code> linker flag causes the Go linker to omit the symbol table from the binary. Note that this does not affect stacktraces. If your function panics, you still get a full stack trace with function names and line numbers. The <code>-w</code> linker flag causes the Go linker to omit DWARF data from the binary. This means the binary can't (usefully) be controlled by a debugger, but that's not relevant in the Lambda environment.</p>"},{"location":"blog/2024/02/20/when-aws-invariants-are-not.html","title":"When AWS invariants aren't [invariant]","text":""},{"location":"blog/2024/02/20/when-aws-invariants-are-not.html#update-14-march-2024","title":"Update (14 March, 2024)","text":"<p>The AWS Security Outreach team contacted me to say that they've heard this feedback and the situation has improved. <code>explicitTrustGrant</code> is now  documented and appears on Google. I've also updated my quotes from  Arkadiy's article to reflect his changes. I continue to be impressed by Amazon's responsiveness to feedback published on some random personal blogs.</p>"},{"location":"blog/2024/02/20/when-aws-invariants-are-not.html#tldr","title":"tl;dr","text":"<p>Search CloudTrail for instances of <code>AssumeRole</code> with  <code>additionalEventData.explicitTrustGrant == false</code>. These will yield results for role assumptions that aren't permitted by the trust policy, i.e. the ones that are going to surprise you - and violate your invariants like \"role  session names will always be an employee's email address\".</p>"},{"location":"blog/2024/02/20/when-aws-invariants-are-not.html#not-quite-invariant","title":"Not quite invariant","text":"<p>Arkadiy Tetelman recently published a very interesting blog post on detecting manual actions undertaken by humans in his AWS environment. It's worth reading: if you haven't already, read it first and then come back here.</p> <p>I want to draw attention to two things he said, because they recently caught me out and I suspect they might be surprising to others. This is the first:</p> <ul> <li> <p>The only way that employees can access AWS is through Okta / AssumeRoleWithSAML.    There are no other mechanisms for an employee to get access to AWS (zero    IAM users, etc)</p> </li> <li> <p>When someone assumes an employee role, Okta is configured to set the AWS    role session name to be the employee email address</p> </li> </ul> <p>The above two conditions are an invariant for employee access to AWS.</p> <p>The second is this:</p> <p>Also until very recently roles implicitly trusted themselves from  a role trust policy perspective (so if they had identity-based permissions to  assume themselves, they could do so without an explicit grant in their trust  policy, or vice versa), which similarly allowed some roles to change their own  role session name.</p> <p>The AWS blog that Arkadiy links to actually briefly mentions that the implicit allowed self-assumption behaviour is still present for some roles - and it's not easy to find out which roles that applies to. This is what caught me out: I had some roles that still had the grandfathered-in implicit self-assumption allowed and I had no idea. </p> <p>The AWS blog shares an AWS Athena query that can be used to identify instances of role self-assumption, but this isn't helpful in a large enterprise: there will  be many false positives, because it will also return instances of self-assumption that is permitted by explicit statements in the role's trust policy. We only want to identify roles that are still relying on the implicit behaviour.</p> <p>It turns out that it is actually possible to identify the implicit behaviour, but I only stumbled across it by accident when reviewing CloudTrail logs. It wasn't documented on the Internet at the time I originally wrote this post, but AWS has since documented <code>explicitTrustGrant</code> and it appears in search results.</p> <p></p>"},{"location":"blog/2024/02/20/when-aws-invariants-are-not.html#email","title":"Email","text":"<p>In the interest of searchability, I've reproduced the email from Amazon here:</p> <p>We contacted you previously regarding an AWS Identity and Access Management (IAM) change delivered on September 21, 2022 that updated an aspect of how role trust policy evaluation behaves when a role assumes itself. With this change, role assumption always requires an explicit role trust policy grant. At that time, we identified one or more roles in this account relying on implicit trust when the role assumes itself. These roles were placed on a temporary allow list to prevent AssumeRole calls from being denied due to the new trust policy evaluation behavior. We advised you to make any necessary changes to your existing processes, code, or configurations to prepare for elimination of the implicit trust behavior. For more information about this behavior change in your account, please review additional details in the blog post \"Announcing an Update to IAM Role Trust Policy Behavior\" [1].</p> <p>On February 3, 2023, we announced that starting June 30, 2023, all roles, regardless of allow list status, that attempt to assume themselves will fail with an access denied error unless the role trust policy explicitly grants the permission and the conditions and actions are satisfied.</p> <p>We are contacting you again to announce that rather than enforcing an explicit trust grant for all roles regardless of allow list status starting June 30, 2023, we will instead automatically remove roles from the allow list based on observed role assumption behavior. Roles on the allow list that we observe either not performing role self-assumption or whose trust policy grants explicit trust with every role assumption over the previous 90 days or more are candidates for removal. A role that performs self-assumption without granting explicit trust at least once over the previous 90 days will be retained on the allow list to give you additional time to make the necessary code or configuration changes. As we announced on December 20, 2022, you can verify whether a specific role self-assumption call by an allow-listed role grants explicit trust by reviewing the corresponding CloudTrail entry and observing a value of \u201ctrue\u201d for the \u201cexplicitTrustGrant\u201d flag.</p> <p>Automatic removal of candidate roles from the allow list that match the criteria defined above begins on June 30, 2023. You may choose to remove a role from the allow list prior to its identification as a removal candidate if its role assumption behavior matches your use case expectations. For assistance with removing such roles from the allow list, please contact AWS Support [2].</p> <p>Once a role is removed from the allow list, its role assumption calls will always require an explicit trust grant.</p>"},{"location":"blog/2020/09/25/yet-another-blog.html","title":"Yet another blog","text":"<p>It's yet another blog. I'm not going to post super often. My Twitter feed is  probably more interesting - for a very generous definition of interesting.</p> <p>Do code blocks work?</p> <pre><code>this: is code\n</code></pre> <p>Nice. That's all I need.</p>"},{"location":"index.html","title":"Blog","text":""},{"location":"archive/2026.html","title":"2026","text":""},{"location":"archive/2025.html","title":"2025","text":""},{"location":"archive/2024.html","title":"2024","text":""},{"location":"archive/2023.html","title":"2023","text":""},{"location":"archive/2022.html","title":"2022","text":""},{"location":"archive/2021.html","title":"2021","text":""},{"location":"archive/2020.html","title":"2020","text":""},{"location":"blog/category/aws.html","title":"AWS","text":""},{"location":"blog/category/azure.html","title":"Azure","text":""},{"location":"blog/category/gcp.html","title":"GCP","text":""},{"location":"blog/category/oidc.html","title":"OIDC","text":""},{"location":"blog/category/github.html","title":"GitHub","text":""},{"location":"page/2/index.html","title":"Blog","text":""},{"location":"page/3/index.html","title":"Blog","text":""},{"location":"page/4/index.html","title":"Blog","text":""},{"location":"page/5/index.html","title":"Blog","text":""},{"location":"blog/category/aws/page/2.html","title":"AWS","text":""},{"location":"blog/category/aws/page/3.html","title":"AWS","text":""},{"location":"blog/category/aws/page/4.html","title":"AWS","text":""},{"location":"blog/category/aws/page/5.html","title":"AWS","text":""}]}